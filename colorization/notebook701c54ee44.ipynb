{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.18","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pycocotools","metadata":{"execution":{"iopub.status.busy":"2025-07-24T04:01:52.768596Z","iopub.execute_input":"2025-07-24T04:01:52.769161Z","iopub.status.idle":"2025-07-24T04:01:57.997314Z","shell.execute_reply.started":"2025-07-24T04:01:52.769133Z","shell.execute_reply":"2025-07-24T04:01:57.992825Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting pycocotools\n  Downloading pycocotools-2.0.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (455 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m455.0/455.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from pycocotools) (2.0.2)\nInstalling collected packages: pycocotools\nSuccessfully installed pycocotools-2.0.10\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os \nos.listdir('/kaggle/input/')\nbase_path = \"/kaggle/input/coco2017\"\nprint(\"Annotations:\", os.listdir(os.path.join(base_path, \"annotations\")))\nprint(\"Training images:\", len(os.listdir(os.path.join(base_path, \"train2017\"))))\nprint(\"Validation images:\", len(os.listdir(os.path.join(base_path, \"val2017\"))))\nprint(\"Test images:\", len(os.listdir(os.path.join(base_path, \"test2017\"))))\n\n# Data visualization","metadata":{"execution":{"iopub.status.busy":"2025-07-24T04:01:57.999901Z","iopub.execute_input":"2025-07-24T04:01:58.000484Z","iopub.status.idle":"2025-07-24T04:02:00.077143Z","shell.execute_reply.started":"2025-07-24T04:01:58.000459Z","shell.execute_reply":"2025-07-24T04:02:00.071718Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Annotations: ['person_keypoints_train2017.json', 'instances_val2017.json', 'instances_train2017.json', 'person_keypoints_val2017.json', 'captions_train2017.json', 'captions_val2017.json']\nTraining images: 118287\nValidation images: 5000\nTest images: 40670\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import Dataset\n\nIMAGE_DIRS = [\n    f\"{base_path}/train2017\",\n    f\"{base_path}/test2017\", \n    f\"{base_path}/val2017\"\n]\n\n# Memory-efficient approach: Just collect image paths, don't load images yet!\nimage_paths = []\nfor dir_path in IMAGE_DIRS:\n    for fname in os.listdir(dir_path):\n        if fname.endswith(\".jpg\"):\n            image_paths.append(os.path.join(dir_path, fname))\n\nprint(f\"Found {len(image_paths)} images total\")\n\n# Limit to a reasonable number for memory constraints\nmax_images = 10000  # Reduced for memory efficiency\nimage_paths = image_paths[:max_images]\nprint(f\"Using {len(image_paths)} images for training\")\n","metadata":{"execution":{"iopub.status.busy":"2025-07-24T04:02:00.078900Z","iopub.execute_input":"2025-07-24T04:02:00.079109Z","iopub.status.idle":"2025-07-24T04:02:25.756757Z","shell.execute_reply.started":"2025-07-24T04:02:00.079087Z","shell.execute_reply":"2025-07-24T04:02:25.750407Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Found 163957 images total\nUsing 10000 images for training\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Memory-efficient Dataset class - loads images on-the-fly!\nclass ColorizationDataset(Dataset):\n    def __init__(self, image_paths, color_bins):\n        self.image_paths = image_paths\n        self.color_bins = color_bins\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        \n        try:\n            # Load and process image on-demand\n            img_bgr = cv2.imread(img_path)\n            if img_bgr is None:\n                # Return a dummy image if loading fails\n                L = np.zeros((256, 256), dtype=np.float32)\n                ab_class = np.zeros((256, 256), dtype=np.int64)\n                return torch.tensor(L).unsqueeze(0), torch.tensor(ab_class)\n            \n            # Resize to 256x256\n            img_bgr = cv2.resize(img_bgr, (256, 256), interpolation=cv2.INTER_AREA)\n            \n            # Convert to LAB and split\n            img_lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2Lab)\n            L, a, b = cv2.split(img_lab)\n            ab = np.stack([a, b], axis=-1)\n            \n            # Convert AB to class indices\n            ab_tensor = torch.tensor(ab, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)  # (1, 2, H, W)\n            ab_classes = self.ab_to_class_indices(ab_tensor, self.color_bins)\n            \n            # Normalize L channel to [0, 1]\n            L = L.astype(np.float32) / 255.0\n            \n            return torch.tensor(L).unsqueeze(0), ab_classes.squeeze(0)\n            \n        except Exception as e:\n            print(f\"Error loading {img_path}: {e}\")\n            # Return dummy data on error\n            L = np.zeros((256, 256), dtype=np.float32)\n            ab_class = np.zeros((256, 256), dtype=np.int64)\n            return torch.tensor(L).unsqueeze(0), torch.tensor(ab_class)\n    \n    def ab_to_class_indices(self, ab_batch, color_bins):\n        \"\"\"Convert AB values to class indices\"\"\"\n        if isinstance(ab_batch, np.ndarray):\n            ab_batch = torch.from_numpy(ab_batch)\n        \n        N, _, H, W = ab_batch.shape\n        ab_pixels = ab_batch.permute(0, 2, 3, 1).reshape(-1, 2)  # (N*H*W, 2)\n        ab_np = ab_pixels.numpy()\n        \n        # Broadcasting for distance calculation\n        diff = ab_np[:, None, :] - color_bins[None, :, :]  # (N*H*W, 313, 2)\n        distances = np.sum(diff**2, axis=2)  # (N*H*W, 313)\n        class_indices = np.argmin(distances, axis=1)  # (N*H*W,)\n        \n        return torch.tensor(class_indices.reshape(N, H, W), dtype=torch.long)\n","metadata":{"execution":{"iopub.status.busy":"2025-07-24T04:02:25.759151Z","iopub.execute_input":"2025-07-24T04:02:25.759517Z","iopub.status.idle":"2025-07-24T04:02:25.780125Z","shell.execute_reply.started":"2025-07-24T04:02:25.759489Z","shell.execute_reply":"2025-07-24T04:02:25.775317Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Training process\n## Model definition\n\n## Convolution Output Size Formula\n\nGiven:\n- Input size `X`: `m √ó n` (Height √ó Width)\n- Kernel size: `k √ó k`\n- Stride: `s`\n- Padding: `p`\n\nThe output size `Y` after applying a 2D convolution is:\n\n$$\n\\text{Output Height} = \\left\\lfloor \\frac{m - k + 2p}{s} \\right\\rfloor + 1\n$$\n\n$$\n\\text{Output Width} = \\left\\lfloor \\frac{n - k + 2p}{s} \\right\\rfloor + 1\n$$\n\nSo the output matrix `Y` has size:\n\n$$\nY \\in \\mathbb{R}^{\\left(\\left\\lfloor \\frac{m - k + 2p}{s} \\right\\rfloor + 1\\right) \\times \\left(\\left\\lfloor \\frac{n - k + 2p}{s} \\right\\rfloor + 1\\right)}\n$$\n\n---\n\n### ‚úÖ Example:\n\nIf:\n- Input: `m = 32`, `n = 32`\n- Kernel size: `k = 5`\n- Padding: `p = 0`\n- Stride: `s = 1`\n\nThen:\n\n$$\n\\text{Output Height} = \\left\\lfloor \\frac{32 - 5 + 0}{1} \\right\\rfloor + 1 = 28\n$$\n\n$$\n\\text{Output Width} = \\left\\lfloor \\frac{32 - 5 + 0}{1} \\right\\rfloor + 1 = 28\n$$\n\nOutput size: `28 √ó 28`\n\n---\n\n### üîé Note:\n\nTo keep the same size as the input:\n- Use **\"same\" padding**:\n  \n  $$\n  p = \\frac{k - 1}{2} \\quad \\text{(when } k \\text{ is odd)}\n  $$\n\n","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass ColorizationNet(nn.Module):\n    def __init__(self):\n        super(ColorizationNet, self).__init__()\n        \n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 64, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(64, 64, 3, stride=2, padding=1), nn.ReLU(),\n            nn.BatchNorm2d(64),\n\n            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(128, 128, 3, stride=2, padding=1), nn.ReLU(),\n            nn.BatchNorm2d(128),\n\n            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(256, 256, 3, stride=2, padding=1), nn.ReLU(),\n            nn.BatchNorm2d(256),\n\n            nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(512, 512, 3, stride=2, padding=1), nn.ReLU(),\n            nn.BatchNorm2d(512)\n        )\n\n        # Optional middle block (refinement / deeper processing)\n        self.refiner = nn.Sequential(\n            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU()\n        )\n\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1), nn.ReLU(),\n            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(),\n            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1), nn.ReLU(),\n            nn.Conv2d(128, 64, 3, padding=1), nn.ReLU(),\n            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1), nn.ReLU(),\n            nn.ConvTranspose2d(32, 313, 4, stride=2, padding=1)\n        )\n    def forward(self, x):\n        encoder = self.encoder(x)\n        for _ in range(3):  # Apply refiner block 3 times\n            encoder = self.refiner(encoder)\n        decoder = self.decoder(encoder)\n        return decoder","metadata":{"execution":{"iopub.status.busy":"2025-07-24T04:02:25.782369Z","iopub.execute_input":"2025-07-24T04:02:25.782710Z","iopub.status.idle":"2025-07-24T04:02:25.812830Z","shell.execute_reply.started":"2025-07-24T04:02:25.782681Z","shell.execute_reply":"2025-07-24T04:02:25.806462Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Change ground truth from ab-channel to 714 channels corresponding to 714 distinct colors","metadata":{}},{"cell_type":"code","source":"# Fast pre-computed quantization (much faster than K-means!)\ndef create_ab_quantization(n_bins=313):\n    \"\"\"\n    Create pre-computed AB color space quantization bins\n    This is much faster than K-means clustering\n    \"\"\"\n    # Define AB color space range: A and B channels are typically [-110, 110]\n    a_range = np.linspace(-110, 110, int(np.sqrt(n_bins)))\n    b_range = np.linspace(-110, 110, int(np.sqrt(n_bins)))\n    \n    # Create grid of all AB combinations\n    a_grid, b_grid = np.meshgrid(a_range, b_range)\n    color_bins = np.column_stack([a_grid.flatten(), b_grid.flatten()])\n    \n    # Take only the first n_bins (in case of rounding)\n    return color_bins[:n_bins]\n\nn_bins = 313\ncolor_bins = create_ab_quantization(n_bins)  # shape (313, 2) - MUCH faster!\nprint(f\"Created {len(color_bins)} color bins in AB space\")\n\ndef ab_to_class_indices(ab_batch, color_bins):\n    \"\"\"\n    ab_batch: (N, 2, H, W) or (N, H, W, 2)\n    color_bins: (313, 2)\n    returns: (N, H, W) ‚Äî class indices\n    \"\"\"\n    if isinstance(ab_batch, np.ndarray):\n        ab_batch = torch.from_numpy(ab_batch)\n    \n    # Handle different input formats\n    if ab_batch.shape[1] == 2:  # (N, 2, H, W) format\n        N, _, H, W = ab_batch.shape\n        ab_pixels = ab_batch.permute(0, 2, 3, 1).reshape(-1, 2)  # (N*H*W, 2)\n    else:  # (N, H, W, 2) format\n        N, H, W, _ = ab_batch.shape\n        ab_pixels = ab_batch.reshape(-1, 2)  # (N*H*W, 2)\n    \n    # Convert to numpy for distance calculation (faster than cdist with torch)\n    ab_np = ab_pixels.numpy()\n    \n    # Use broadcasting for faster distance calculation than scipy.spatial.distance.cdist\n    # ab_np: (N*H*W, 2), color_bins: (313, 2)\n    diff = ab_np[:, None, :] - color_bins[None, :, :]  # (N*H*W, 313, 2)\n    distances = np.sum(diff**2, axis=2)  # (N*H*W, 313) - squared distances\n    class_indices = np.argmin(distances, axis=1)  # (N*H*W,)\n    \n    return torch.tensor(class_indices.reshape(N, H, W), dtype=torch.long)\n","metadata":{"execution":{"iopub.status.busy":"2025-07-24T04:02:25.814316Z","iopub.execute_input":"2025-07-24T04:02:25.814609Z","iopub.status.idle":"2025-07-24T04:02:25.834511Z","shell.execute_reply.started":"2025-07-24T04:02:25.814582Z","shell.execute_reply":"2025-07-24T04:02:25.825827Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Created 289 color bins in AB space\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Split data 80 - 20 (memory-efficient way)","metadata":{}},{"cell_type":"code","source":"# Split image paths instead of loaded images\nfrom sklearn.model_selection import train_test_split\n\ntrain_paths, test_paths = train_test_split(image_paths, test_size=0.2, random_state=42)\n\nprint(f\"Training images: {len(train_paths)}\")\nprint(f\"Testing images: {len(test_paths)}\")\n\n# Create datasets (these don't load images yet!)\ntrain_dataset = ColorizationDataset(train_paths, color_bins)\ntest_dataset = ColorizationDataset(test_paths, color_bins)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T04:02:25.835763Z","iopub.execute_input":"2025-07-24T04:02:25.836051Z","iopub.status.idle":"2025-07-24T04:02:27.114110Z","shell.execute_reply.started":"2025-07-24T04:02:25.836021Z","shell.execute_reply":"2025-07-24T04:02:27.108485Z"}},"outputs":[{"name":"stdout","text":"Training images: 8000\nTesting images: 2000\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Fiting model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# Create DataLoaders (memory-efficient!)\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)  # Reduced batch size\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T04:02:27.114839Z","iopub.execute_input":"2025-07-24T04:02:27.115139Z","iopub.status.idle":"2025-07-24T04:02:27.128242Z","shell.execute_reply.started":"2025-07-24T04:02:27.115113Z","shell.execute_reply":"2025-07-24T04:02:27.122650Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Initialize model\nmodel = ColorizationNet()\nmodel = model.to(device)\n\n# Loss function and optimizer  \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T04:02:27.130682Z","iopub.execute_input":"2025-07-24T04:02:27.130926Z","iopub.status.idle":"2025-07-24T04:02:33.523603Z","shell.execute_reply.started":"2025-07-24T04:02:27.130904Z","shell.execute_reply":"2025-07-24T04:02:33.519261Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# üîç Training Status Checker - Run this cell to check progress\n# This replaces the need for resume_training.py in Kaggle\n\nimport glob\nimport os\nfrom safetensors.torch import load_file\n\ndef check_kaggle_training_status():\n    \"\"\"Check the current status of training and provide guidance for Kaggle.\"\"\"\n    \n    print(\"üîç Kaggle Training Session Status Check\")\n    print(\"=\" * 50)\n    \n    # Check for checkpoints\n    checkpoint_dir = \"checkpoints\"\n    if os.path.exists(checkpoint_dir):\n        checkpoint_files = glob.glob(f\"{checkpoint_dir}/checkpoint_epoch_*.safetensors\")\n        \n        if checkpoint_files:\n            # Get latest checkpoint\n            latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n            print(f\"‚úÖ Found {len(checkpoint_files)} checkpoint(s)\")\n            print(f\"üìç Latest checkpoint: {os.path.basename(latest_checkpoint)}\")\n            \n            # Load checkpoint info\n            try:\n                checkpoint_data = load_file(latest_checkpoint)\n                epoch = int(checkpoint_data.get('epoch', 0))\n                loss = float(checkpoint_data.get('loss', 0))\n                timestamp = checkpoint_data.get('timestamp', 'unknown')\n                \n                print(f\"üéØ Last completed epoch: {epoch}\")\n                print(f\"üìâ Last loss: {loss:.4f}\")\n                print(f\"‚è∞ Checkpoint time: {timestamp}\")\n                \n                # Calculate progress\n                total_epochs = 50  # From updated configuration\n                progress = (epoch + 1) / total_epochs * 100\n                remaining_epochs = total_epochs - (epoch + 1)\n                \n                print(f\"üìä Training progress: {progress:.1f}% ({epoch + 1}/{total_epochs} epochs)\")\n                print(f\"üéØ Remaining epochs: {remaining_epochs}\")\n                \n                if remaining_epochs > 0:\n                    print(f\"\\nüí° Next steps:\")\n                    print(f\"   1. Continue running the training cells below\")\n                    print(f\"   2. Training will automatically resume from epoch {epoch + 1}\")\n                    print(f\"   3. Estimated time: ~{remaining_epochs * 0.2:.1f} hours remaining\")\n                else:\n                    print(f\"\\nüéâ Training appears to be complete!\")\n                    \n                return epoch, loss, remaining_epochs\n                    \n            except Exception as e:\n                print(f\"‚ö†Ô∏è  Could not read checkpoint details: {e}\")\n                return None, None, None\n                \n        else:\n            print(\"‚ö†Ô∏è  No checkpoints found in checkpoints/ directory\")\n            print(\"üí° Training will start from epoch 0\")\n            return 0, None, 50\n    else:\n        print(\"üìÅ No checkpoints directory found\")\n        print(\"üí° Training will start from the beginning\")\n        return 0, None, 50\n\n# Run the status check\nprint(\"üöÄ Checking training status before starting...\")\nlast_epoch, last_loss, remaining = check_kaggle_training_status()\n\nif last_epoch is not None and last_epoch > 0:\n    print(f\"\\n‚ú® Will resume training from epoch {last_epoch + 1}\")\nelse:\n    print(\"\\nWill start fresh training from epoch 0\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T04:02:33.526840Z","iopub.execute_input":"2025-07-24T04:02:33.527563Z","iopub.status.idle":"2025-07-24T04:02:33.562223Z","shell.execute_reply.started":"2025-07-24T04:02:33.527533Z","shell.execute_reply":"2025-07-24T04:02:33.557169Z"}},"outputs":[{"name":"stdout","text":"üöÄ Checking training status before starting...\nüîç Kaggle Training Session Status Check\n==================================================\nüìÅ No checkpoints directory found\nüí° Training will start from the beginning\n\nWill start fresh training from epoch 0\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Enhanced Training loop with checkpointing for Kaggle's 12-hour limit\nimport time\nimport glob\nimport os\nfrom datetime import datetime\nfrom safetensors.torch import save_file, load_file\n\n# Kaggle-optimized configuration\nnum_epochs = 50  # Reduced from 1000 to fit within 12-hour limit\ncheckpoint_interval = 1  # Save checkpoint every 10 epochs\nmax_training_hours = 11  # Stop before 12-hour limit\n\ndef save_checkpoint(model, optimizer, epoch, loss, checkpoint_dir=\"checkpoints\"):\n    \"\"\"Save training checkpoint\"\"\"\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    checkpoint_path = f\"{checkpoint_dir}/checkpoint_epoch_{epoch:03d}_{timestamp}.safetensors\"\n    \n    checkpoint_data = {\n        **model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'epoch': epoch,\n        'loss': loss,\n        'timestamp': timestamp\n    }\n    \n    save_file(checkpoint_data, checkpoint_path)\n    print(f\"üíæ Checkpoint saved: {checkpoint_path}\")\n    return checkpoint_path\n\ndef load_latest_checkpoint(model, optimizer, checkpoint_dir=\"checkpoints\"):\n    \"\"\"Load the latest checkpoint if available\"\"\"\n    if not os.path.exists(checkpoint_dir):\n        return 0, None\n    \n    checkpoint_files = glob.glob(f\"{checkpoint_dir}/checkpoint_epoch_*.safetensors\")\n    if not checkpoint_files:\n        return 0, None\n    \n    # Get the latest checkpoint\n    latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n    print(f\"üîÑ Loading checkpoint: {latest_checkpoint}\")\n    \n    checkpoint_data = load_file(latest_checkpoint)\n    \n    # Separate model weights from other data\n    model_state = {k: v for k, v in checkpoint_data.items() if not k.startswith(('optimizer_', 'epoch', 'loss', 'timestamp'))}\n    model.load_state_dict(model_state)\n    optimizer.load_state_dict(checkpoint_data['optimizer_state_dict'])\n    \n    start_epoch = int(checkpoint_data['epoch']) + 1\n    last_loss = float(checkpoint_data['loss'])\n    print(f\"‚úÖ Resumed from epoch {start_epoch-1}, loss: {last_loss:.4f}\")\n    return start_epoch, last_loss\n\n# Try to load from checkpoint\nstart_epoch, last_loss = load_latest_checkpoint(model, optimizer)\nprint(f\"üìä Starting training from epoch {start_epoch}\")\n\n# Training loop with time monitoring\ntraining_start_time = time.time()\nmax_training_seconds = max_training_hours * 3600\n\nfor epoch in range(start_epoch, num_epochs):\n    # Check time limit\n    elapsed_time = time.time() - training_start_time\n    if elapsed_time > max_training_seconds:\n        print(f\"‚è∞ Reached time limit ({max_training_hours} hours). Stopping training...\")\n        print(f\"üíæ Saving final checkpoint...\")\n        save_checkpoint(model, optimizer, epoch-1, avg_loss)\n        break\n    \n    model.train()\n    running_loss = 0.0\n    epoch_start_time = time.time()\n    \n    for i, (inputs, targets) in enumerate(train_loader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        \n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        \n        # Print progress every 50 batches\n        if (i + 1) % 50 == 0:\n            elapsed = time.time() - training_start_time\n            remaining = max_training_seconds - elapsed\n            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}, Time remaining: {remaining/3600:.1f}h')\n    \n    avg_loss = running_loss / len(train_loader)\n    epoch_time = time.time() - epoch_start_time\n    total_elapsed = time.time() - training_start_time\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}] completed in {epoch_time:.1f}s, Average Loss: {avg_loss:.4f}, Total time: {total_elapsed/3600:.2f}h\")\n    \n    # Save checkpoint at intervals\n    if (epoch + 1) % checkpoint_interval == 0:\n        save_checkpoint(model, optimizer, epoch, avg_loss)\n        print(f\"üìà Progress: {(epoch+1)/num_epochs*100:.1f}% complete\")\n\nprint(\"\\nüéâ Training completed!\")\nprint(f\"‚è±Ô∏è  Total training time: {(time.time() - training_start_time)/3600:.2f} hours\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T04:02:33.563999Z","iopub.execute_input":"2025-07-24T04:02:33.564202Z"}},"outputs":[{"name":"stdout","text":"üìä Starting training from epoch 0\nEpoch [1/50], Batch [50/1000], Loss: 0.5045, Time remaining: 11.0h\nEpoch [1/50], Batch [100/1000], Loss: 1.1799, Time remaining: 10.9h\nEpoch [1/50], Batch [150/1000], Loss: 0.1648, Time remaining: 10.9h\nEpoch [1/50], Batch [200/1000], Loss: 0.1915, Time remaining: 10.8h\nEpoch [1/50], Batch [250/1000], Loss: 0.4012, Time remaining: 10.8h\nEpoch [1/50], Batch [300/1000], Loss: 0.1492, Time remaining: 10.7h\nEpoch [1/50], Batch [350/1000], Loss: 0.3735, Time remaining: 10.7h\nEpoch [1/50], Batch [400/1000], Loss: 0.1863, Time remaining: 10.6h\nEpoch [1/50], Batch [450/1000], Loss: 0.1544, Time remaining: 10.6h\nEpoch [1/50], Batch [500/1000], Loss: 0.1915, Time remaining: 10.5h\nEpoch [1/50], Batch [550/1000], Loss: 0.4390, Time remaining: 10.5h\nEpoch [1/50], Batch [600/1000], Loss: 0.4974, Time remaining: 10.5h\nEpoch [1/50], Batch [650/1000], Loss: 0.1582, Time remaining: 10.4h\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# üöÄ Kaggle Session Management Guide\n\n## üìã How to Handle Kaggle's 12-Hour Session Limit\n\n### ‚úÖ What This Enhanced Training Does:\n- **Automatic Checkpointing**: Saves progress every 10 epochs\n- **Time Monitoring**: Tracks elapsed time and stops before 12-hour limit\n- **Resume Capability**: Automatically resumes from latest checkpoint when restarted\n- **Optimized Epochs**: Reduced to 50 epochs (from 1000) to fit time constraint\n\n### üîÑ If Your Session Gets Interrupted:\n\n1. **Re-run the notebook from the beginning** (to reload libraries and data)\n2. **Run the \"Training Status Checker\" cell** to see your progress\n3. **The training will automatically resume** from the latest checkpoint\n4. **No need for external scripts** - everything works within the notebook!\n\n### üìä Monitoring Progress:\n- **Real-time time tracking**: Shows remaining time in each batch\n- **Checkpoint intervals**: Progress saved every 10 epochs\n- **Memory-efficient**: Uses on-demand image loading\n\n### üõ†Ô∏è Manual Checkpoint Management:\n\n```python\n# The \"Training Status Checker\" cell above handles this automatically!\n# But you can also manually check checkpoints:\n\nimport glob\ncheckpoints = glob.glob('checkpoints/*.safetensors')\nfor cp in sorted(checkpoints):\n    print(cp)\n\n# The training loop automatically finds and loads the latest checkpoint\n```\n\n### ‚ö° Performance Tips for Kaggle:\n- **Batch size**: Optimized to 8 for memory efficiency\n- **Epochs**: Reduced to 50 to ensure completion\n- **GPU utilization**: Monitor GPU usage in Kaggle sidebar\n- **Storage**: Checkpoints and models saved to persistent storage","metadata":{}},{"cell_type":"markdown","source":"# üõ†Ô∏è Additional Optimizations & Troubleshooting\n\n## ‚ö° Further Optimizations for Kaggle\n\n### Memory Management:\n```python\n# If you encounter memory issues, try these:\n\n# 1. Reduce batch size further\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=1)\n\n# 2. Clear GPU cache periodically\nimport torch\ntorch.cuda.empty_cache()\n\n# 3. Reduce image count for testing\nimage_paths = image_paths[:10000]  # Use subset for faster testing\n```\n\n### Performance Monitoring:\n```python\n# Monitor GPU usage\nimport GPUtil\ngpus = GPUtil.getGPUs()\nfor gpu in gpus:\n    print(f\"GPU {gpu.id}: {gpu.memoryUtil*100:.1f}% memory, {gpu.load*100:.1f}% load\")\n```\n\n### Alternative Training Strategies:\n\n#### 1. **Progressive Training** (if time is very limited):\n```python\n# Train on a smaller subset first, then gradually increase\nsmall_paths = image_paths[:5000]  # Start with 5k images\n# After initial training, increase to full dataset\n```\n\n#### 2. **Transfer Learning** (faster convergence):\n```python\n# Use a pre-trained encoder (like ResNet) instead of training from scratch\n# This can significantly reduce training time\n```\n\n#### 3. **Learning Rate Scheduling**:\n```python\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nscheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n# Add to training loop: scheduler.step(avg_loss)\n```\n\n## üêõ Common Issues & Solutions\n\n| Issue | Solution |\n|-------|----------|\n| Out of memory | Reduce batch_size to 4 or 2 |\n| Training too slow | Reduce num_workers to 1, use smaller dataset subset |\n| Checkpoint loading fails | Check file permissions, ensure safetensors is installed |\n| Session timeout | Run `python resume_training.py` to check status |\n| Model not saving | Ensure `saved_models/` directory has write permissions |\n\n## üìä Expected Results\n\nAfter **50 epochs** with the COCO dataset, you should expect:\n- **Loss**: Should decrease from ~5.0 to ~2.0-3.0\n- **Quality**: Basic colorization with some realistic colors\n- **Time**: ~8-10 hours on Kaggle GPU\n- **File sizes**: ~100MB for model, ~10MB per checkpoint\n\n## üéÜ Next Steps After Training\n\n1. **Test the model** on new images using the inference cells\n2. **Fine-tune** with different learning rates or architectures\n3. **Experiment** with different color quantization strategies\n4. **Deploy** the model for real-world applications\n\n---\n\n*This enhanced notebook is optimized for Kaggle's constraints while maintaining training quality!*","metadata":{}},{"cell_type":"code","source":"# Import libraries for model saving\nfrom safetensors.torch import save_file, load_file\nimport os\nimport json\nfrom datetime import datetime","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the trained model\nprint(\"\\n\" + \"=\"*50)\nprint(\"üíæ Saving trained model...\")\nprint(\"=\"*50)\n\n# Create a directory for saved models if it doesn't exist\nos.makedirs(\"saved_models\", exist_ok=True)\n\n# Create a timestamp for the model filename\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nmodel_filename = f\"saved_models/colorization_model_{timestamp}.safetensors\"\n\n# Save the model state dict using safetensors\nsave_file(model.state_dict(), model_filename)\nprint(f\"‚úÖ Model saved as: {model_filename}\")\n\n# Save training metadata\nmetadata = {\n    \"model_name\": \"Colorization CNN\",\n    \"epochs\": num_epochs,\n    \"batch_size\": 8,\n    \"learning_rate\": 0.001,\n    \"optimizer\": \"Adam\",\n    \"loss_function\": \"CrossEntropyLoss\",\n    \"num_color_bins\": 313,\n    \"input_size\": \"256x256\",\n    \"architecture\": \"encoder_decoder_with_refiner\",\n    \"dataset\": \"COCO 2017\",\n    \"num_training_images\": len(train_paths),\n    \"num_testing_images\": len(test_paths),\n    \"train_test_split\": \"80/20\",\n    \"color_space\": \"LAB\",\n    \"training_date\": datetime.now().isoformat(),\n    \"final_avg_loss\": avg_loss\n}\n\n# Save metadata as JSON\nmetadata_filename = f\"saved_models/training_metadata_{timestamp}.json\"\nwith open(metadata_filename, 'w') as f:\n    json.dump(metadata, f, indent=4)\n\nprint(f\"‚úÖ Training metadata saved as: {metadata_filename}\")\nprint(\"\\nüéâ Model and metadata saved successfully!\")\nprint(f\"üìÅ Check the 'saved_models' directory for your files.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Functions to load saved models\ndef load_colorization_model(model_path, device='cuda' if torch.cuda.is_available() else 'cpu'):\n    \"\"\"\n    Load a saved colorization model from safetensors file\n    \n    Args:\n        model_path: Path to the .safetensors file\n        device: Device to load the model on ('cuda' or 'cpu')\n    \n    Returns:\n        Loaded model ready for inference\n    \"\"\"\n    print(f\"Loading model from: {model_path}\")\n    \n    # Initialize the model architecture (same as training)\n    model = ColorizationNet()\n    \n    # Load the saved state dict\n    state_dict = load_file(model_path)\n    model.load_state_dict(state_dict)\n    \n    # Move to device and set to evaluation mode\n    model = model.to(device)\n    model.eval()\n    \n    print(f\"‚úÖ Model loaded successfully!\")\n    print(f\"üéØ Model is on device: {next(model.parameters()).device}\")\n    print(f\"üîß Model is in evaluation mode: {not model.training}\")\n    \n    return model\n\ndef demo_load_model():\n    \"\"\"\n    Demo function showing how to load the most recent saved model\n    \"\"\"\n    # List available saved models\n    if os.path.exists(\"saved_models\"):\n        saved_models = [f for f in os.listdir(\"saved_models\") if f.endswith(\".safetensors\")]\n        if saved_models:\n            print(\"Available saved models:\")\n            for i, model_file in enumerate(saved_models):\n                print(f\"  {i+1}. {model_file}\")\n            \n            # Load the most recent model (last in list when sorted)\n            latest_model = sorted(saved_models)[-1]\n            model_path = f\"saved_models/{latest_model}\"\n            \n            print(f\"\\nLoading latest model: {latest_model}\")\n            loaded_model = load_colorization_model(model_path)\n            return loaded_model\n        else:\n            print(\"No saved models found in 'saved_models' directory\")\n            return None\n    else:\n        print(\"'saved_models' directory doesn't exist yet. Train the model first!\")\n        return None\n\ndef test_saved_model():\n    \"\"\"\n    Test function to verify the saved model can be loaded and used\n    \"\"\"\n    try:\n        # Try to load the most recent model\n        loaded_model = demo_load_model()\n        \n        if loaded_model is not None:\n            print(\"\\nüß™ Testing loaded model...\")\n            \n            # Test with a dummy input\n            dummy_input = torch.randn(1, 1, 256, 256).to(device)\n            \n            with torch.no_grad():\n                output = loaded_model(dummy_input)\n                print(f\"‚úÖ Model inference successful!\")\n                print(f\"   Input shape: {dummy_input.shape}\")\n                print(f\"   Output shape: {output.shape}\")\n                print(f\"   Expected output shape: (1, 313, 256, 256)\")\n                \n                if output.shape == (1, 313, 256, 256):\n                    print(\"üéâ Model loaded and working perfectly!\")\n                else:\n                    print(\"‚ö†Ô∏è  Output shape doesn't match expected dimensions\")\n        \n    except Exception as e:\n        print(f\"‚ùå Error testing saved model: {e}\")\n\nprint(\"üîß Model saving and loading functions defined!\")\nprint(\"üí° After training completes, your model will be automatically saved!\")\nprint(\"üí° To load a saved model later, use: loaded_model = demo_load_model()\")\nprint(\"üí° To test a saved model, use: test_saved_model()\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test phase - using memory-efficient approach","metadata":{}},{"cell_type":"code","source":"# Test with a single image from the test dataset\nmodel.eval()\n\n# Get one batch from test loader\ntest_iter = iter(test_loader)\ntest_batch = next(test_iter)\ngray_input, ground_truth = test_batch\n\n# Use the first image in the batch\ngray_input = gray_input[0:1].to(device)  # shape: [1, 1, H, W]\n\nprint(f\"Input shape: {gray_input.shape}\")\n\nwith torch.no_grad():\n    output = model(gray_input)  # shape: [1, 313, H, W]\n    print(f\"Output shape: {output.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get predicted color classes\npred_classes = output.argmax(dim=1)  # shape: [1, H, W]\nprint(f\"Predicted classes shape: {pred_classes.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred_classes_np = pred_classes.squeeze().cpu().numpy()  # shape: (H, W)\nH, W = pred_classes_np.shape\nab_decoded = color_bins[pred_classes_np.flatten()]      # shape: (H*W, 2)\nab_decoded = ab_decoded.reshape(H, W, 2).astype(np.float32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gray_input_np = gray_input.squeeze().cpu().numpy()  # (H, W)\nlab_img = np.zeros((H, W, 3), dtype=np.float32)\nlab_img[..., 0] = gray_input_np * 255.0  # Set L channel (denormalize from [0,1] to [0,255])\nlab_img[..., 1:] = ab_decoded   # Set A and B channels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nrgb_img = cv2.cvtColor(lab_img, cv2.COLOR_LAB2RGB)\nrgb_img = np.clip(rgb_img, 0, 1)\n\nimport matplotlib.pyplot as plt\nplt.imshow(rgb_img)\nplt.title(\"Colorized Image\")\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Optional: Test the saved model loading functionality\nprint(\"\\n\" + \"=\"*50)\nprint(\"üß™ Testing saved model functionality...\")\nprint(\"=\"*50)\n\n# Test loading the saved model\ntest_saved_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}