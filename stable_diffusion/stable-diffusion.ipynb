{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91e4a3f2",
   "metadata": {},
   "source": [
    "# Perceptual Loss trong Computer Vision\n",
    "\n",
    "## ƒê·ªãnh nghƒ©a\n",
    "**Perceptual Loss** (Loss tri gi√°c) l√† m·ªôt lo·∫°i loss function ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ ƒëo l∆∞·ªùng s·ª± kh√°c bi·ªát gi·ªØa c√°c ·∫£nh d·ª±a tr√™n c√°ch con ng∆∞·ªùi nh·∫≠n th·ª©c th·ªã gi√°c, thay v√¨ ch·ªâ so s√°nh pixel theo pixel nh∆∞ L1 ho·∫∑c L2 loss.\n",
    "\n",
    "## T·∫°i sao c·∫ßn Perceptual Loss?\n",
    "\n",
    "### V·∫•n ƒë·ªÅ v·ªõi Pixel-wise Loss:\n",
    "- **L1/L2 Loss**: Ch·ªâ so s√°nh t·ª´ng pixel m·ªôt c√°ch ƒë·ªôc l·∫≠p\n",
    "- **K·∫øt qu·∫£**: ·∫¢nh c√≥ th·ªÉ c√≥ PSNR cao nh∆∞ng tr√¥ng \"m·ªù\" ho·∫∑c thi·∫øu chi ti·∫øt\n",
    "- **Kh√¥ng ph·∫£n √°nh**: C√°ch con ng∆∞·ªùi ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng ·∫£nh\n",
    "\n",
    "### ∆Øu ƒëi·ªÉm c·ªßa Perceptual Loss:\n",
    "- **B·∫£o to√†n c·∫•u tr√∫c**: Gi·ªØ ƒë∆∞·ª£c c√°c ƒë·∫∑c tr∆∞ng quan tr·ªçng c·ªßa ·∫£nh\n",
    "- **Ch·∫•t l∆∞·ª£ng th·ªã gi√°c**: T·∫°o ra ·∫£nh s·∫Øc n√©t, chi ti·∫øt h∆°n\n",
    "- **Ph√π h·ª£p v·ªõi nh·∫≠n th·ª©c**: G·∫ßn v·ªõi c√°ch con ng∆∞·ªùi ƒë√°nh gi√° ·∫£nh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a99f746",
   "metadata": {},
   "source": [
    "## C√¥ng th·ª©c to√°n h·ªçc\n",
    "\n",
    "### Pixel-wise Loss (L2):\n",
    "```\n",
    "L_pixel = ||I_pred - I_target||¬≤\n",
    "```\n",
    "\n",
    "### Perceptual Loss:\n",
    "```\n",
    "L_perceptual = ||œÜ(I_pred) - œÜ(I_target)||¬≤\n",
    "```\n",
    "\n",
    "Trong ƒë√≥:\n",
    "- `œÜ(¬∑)`: Feature extractor (th∆∞·ªùng l√† CNN pre-trained nh∆∞ VGG)\n",
    "- `I_pred`: ·∫¢nh ƒë∆∞·ª£c t·∫°o ra\n",
    "- `I_target`: ·∫¢nh ground truth\n",
    "\n",
    "### C√¥ng th·ª©c chi ti·∫øt:\n",
    "```\n",
    "L_perceptual = Œ£ Œª·µ¢ * ||œÜ·µ¢(I_pred) - œÜ·µ¢(I_target)||¬≤\n",
    "```\n",
    "\n",
    "Trong ƒë√≥:\n",
    "- `œÜ·µ¢`: Features t·ª´ layer th·ª© i\n",
    "- `Œª·µ¢`: Tr·ªçng s·ªë cho layer th·ª© i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a552e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, layers=['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1']):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        \n",
    "        # S·ª≠ d·ª•ng VGG16 pre-trained\n",
    "        vgg = models.vgg16(pretrained=True).features\n",
    "        \n",
    "        # ƒê·ªãnh nghƒ©a c√°c layers c·∫ßn extract features\n",
    "        self.layer_names = layers\n",
    "        self.layers = {}\n",
    "        \n",
    "        # Mapping layer names to indices in VGG\n",
    "        layer_mapping = {\n",
    "            'relu1_1': 1,   # after first ReLU\n",
    "            'relu2_1': 6,   # after first ReLU in block 2\n",
    "            'relu3_1': 11,  # after first ReLU in block 3\n",
    "            'relu4_1': 18,  # after first ReLU in block 4\n",
    "            'relu5_1': 25   # after first ReLU in block 5\n",
    "        }\n",
    "        \n",
    "        # Extract specific layers\n",
    "        for name in self.layer_names:\n",
    "            if name in layer_mapping:\n",
    "                layer_idx = layer_mapping[name]\n",
    "                self.layers[name] = nn.Sequential(*list(vgg.children())[:layer_idx+1])\n",
    "        \n",
    "        # Freeze parameters\n",
    "        for layer in self.layers.values():\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"\n",
    "        T√≠nh Perceptual Loss gi·ªØa predicted v√† target images\n",
    "        \n",
    "        Args:\n",
    "            pred: Predicted image [B, 3, H, W]\n",
    "            target: Target image [B, 3, H, W]\n",
    "            \n",
    "        Returns:\n",
    "            perceptual_loss: Scalar loss value\n",
    "        \"\"\"\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for layer_name, layer in self.layers.items():\n",
    "            # Extract features\n",
    "            pred_features = layer(pred)\n",
    "            target_features = layer(target)\n",
    "            \n",
    "            # Compute L2 loss in feature space\n",
    "            loss = F.mse_loss(pred_features, target_features)\n",
    "            total_loss += loss\n",
    "            \n",
    "        return total_loss / len(self.layers)\n",
    "\n",
    "# Example usage\n",
    "perceptual_loss_fn = PerceptualLoss()\n",
    "\n",
    "# Gi·∫£ s·ª≠ c√≥ 2 ·∫£nh\n",
    "batch_size = 4\n",
    "channels = 3\n",
    "height, width = 256, 256\n",
    "\n",
    "pred_images = torch.randn(batch_size, channels, height, width)\n",
    "target_images = torch.randn(batch_size, channels, height, width)\n",
    "\n",
    "# T√≠nh loss\n",
    "loss = perceptual_loss_fn(pred_images, target_images)\n",
    "print(f\"Perceptual Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba58919",
   "metadata": {},
   "source": [
    "## So s√°nh c√°c lo·∫°i Loss\n",
    "\n",
    "| Loss Type | ∆Øu ƒëi·ªÉm | Nh∆∞·ª£c ƒëi·ªÉm | ·ª®ng d·ª•ng |\n",
    "|-----------|---------|------------|----------|\n",
    "| **L1/L2 Loss** | - ƒê∆°n gi·∫£n<br>- T√≠nh to√°n nhanh | - ·∫¢nh m·ªù<br>- M·∫•t chi ti·∫øt | Basic reconstruction |\n",
    "| **Perceptual Loss** | - Ch·∫•t l∆∞·ª£ng cao<br>- B·∫£o to√†n c·∫•u tr√∫c | - Ch·∫≠m h∆°n<br>- C·∫ßn pre-trained model | Style transfer, Super-resolution |\n",
    "| **Adversarial Loss** | - ·∫¢nh s·∫Øc n√©t<br>- Realistic | - Kh√≥ train<br>- Unstable | GAN-based generation |\n",
    "\n",
    "## ·ª®ng d·ª•ng trong Latent Diffusion Models\n",
    "\n",
    "Trong paper \"High-Resolution Image Synthesis with Latent Diffusion Models\":\n",
    "\n",
    "1. **VAE Training**: S·ª≠ d·ª•ng perceptual loss ƒë·ªÉ train autoencoder\n",
    "   ```python\n",
    "   total_loss = reconstruction_loss + kl_loss + Œª_perceptual * perceptual_loss\n",
    "   ```\n",
    "\n",
    "2. **M·ª•c ƒë√≠ch**: ƒê·∫£m b·∫£o VAE encode/decode gi·ªØ ƒë∆∞·ª£c th√¥ng tin th·ªã gi√°c quan tr·ªçng\n",
    "\n",
    "3. **K·∫øt qu·∫£**: Latent space c√≥ ch·∫•t l∆∞·ª£ng cao h∆°n cho diffusion process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c321afe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√≠ d·ª•: VAE v·ªõi Perceptual Loss (simplified)\n",
    "class VAEWithPerceptualLoss(nn.Module):\n",
    "    def __init__(self, encoder, decoder, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.perceptual_loss_fn = PerceptualLoss()\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        # Decode\n",
    "        x_recon = self.decoder(z)\n",
    "        \n",
    "        return x_recon, mu, logvar\n",
    "    \n",
    "    def loss_function(self, x, x_recon, mu, logvar, Œª_perceptual=1.0, Œª_kl=1.0):\n",
    "        \"\"\"\n",
    "        Combined loss for VAE with perceptual loss\n",
    "        \"\"\"\n",
    "        # Reconstruction loss (L2)\n",
    "        recon_loss = F.mse_loss(x_recon, x, reduction='mean')\n",
    "        \n",
    "        # Perceptual loss\n",
    "        perceptual_loss = self.perceptual_loss_fn(x_recon, x)\n",
    "        \n",
    "        # KL divergence\n",
    "        kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = recon_loss + Œª_perceptual * perceptual_loss + Œª_kl * kl_loss\n",
    "        \n",
    "        return {\n",
    "            'total_loss': total_loss,\n",
    "            'recon_loss': recon_loss,\n",
    "            'perceptual_loss': perceptual_loss,\n",
    "            'kl_loss': kl_loss\n",
    "        }\n",
    "\n",
    "print(\"VAE with Perceptual Loss implementation ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed8fe18",
   "metadata": {},
   "source": [
    "## T·ªïng k·∫øt\n",
    "\n",
    "### Perceptual Loss l√† g√¨?\n",
    "- **ƒê·ªãnh nghƒ©a**: Loss function ƒëo l∆∞·ªùng s·ª± kh√°c bi·ªát d·ª±a tr√™n features th·ªã gi√°c\n",
    "- **C√°ch ho·∫°t ƒë·ªông**: S·ª≠ d·ª•ng CNN pre-trained ƒë·ªÉ extract features\n",
    "- **∆Øu ƒëi·ªÉm**: T·∫°o ra ·∫£nh ch·∫•t l∆∞·ª£ng cao, s·∫Øc n√©t h∆°n\n",
    "\n",
    "### Vai tr√≤ trong Stable Diffusion:\n",
    "1. **Training VAE**: ƒê·∫£m b·∫£o latent space c√≥ ch·∫•t l∆∞·ª£ng cao\n",
    "2. **Perceptual Compression**: N√©n ·∫£nh m√† v·∫´n gi·ªØ ƒë∆∞·ª£c th√¥ng tin quan tr·ªçng\n",
    "3. **Quality Control**: Ki·ªÉm so√°t ch·∫•t l∆∞·ª£ng ·∫£nh trong qu√° tr√¨nh training\n",
    "\n",
    "### References:\n",
    "- [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/abs/1603.08155)\n",
    "- [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)\n",
    "- [Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://arxiv.org/abs/1609.04802)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e7f73d",
   "metadata": {},
   "source": [
    "# Downsampling trong Computer Vision\n",
    "\n",
    "## ƒê·ªãnh nghƒ©a\n",
    "**Downsampling** (L·∫•y m·∫´u xu·ªëng) l√† qu√° tr√¨nh **gi·∫£m k√≠ch th∆∞·ªõc ho·∫∑c ƒë·ªô ph√¢n gi·∫£i** c·ªßa d·ªØ li·ªáu b·∫±ng c√°ch lo·∫°i b·ªè m·ªôt s·ªë th√¥ng tin.\n",
    "\n",
    "## C√°c lo·∫°i Downsampling:\n",
    "\n",
    "### 1. **Spatial Downsampling** (Gi·∫£m k√≠ch th∆∞·ªõc kh√¥ng gian):\n",
    "- **M·ª•c ƒë√≠ch**: Gi·∫£m chi·ªÅu r·ªông v√† chi·ªÅu cao c·ªßa ·∫£nh\n",
    "- **V√≠ d·ª•**: ·∫¢nh 512x512 ‚Üí 256x256\n",
    "- **Ph∆∞∆°ng ph√°p**:\n",
    "  - Max Pooling\n",
    "  - Average Pooling\n",
    "  - Strided Convolution\n",
    "  - Bilinear/Bicubic Interpolation\n",
    "\n",
    "### 2. **Temporal Downsampling** (Gi·∫£m t·∫ßn s·ªë th·ªùi gian):\n",
    "- **M·ª•c ƒë√≠ch**: Gi·∫£m s·ªë frame trong video\n",
    "- **V√≠ d·ª•**: 60fps ‚Üí 30fps\n",
    "\n",
    "### 3. **Channel Downsampling** (Gi·∫£m s·ªë k√™nh):\n",
    "- **M·ª•c ƒë√≠ch**: Gi·∫£m chi·ªÅu s√¢u c·ªßa feature maps\n",
    "- **V√≠ d·ª•**: 512 channels ‚Üí 256 channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9290ea00",
   "metadata": {},
   "source": [
    "## C√¥ng th·ª©c to√°n h·ªçc\n",
    "\n",
    "### Max Pooling:\n",
    "```\n",
    "Output[i,j] = max(Input[i*s:(i+1)*s, j*s:(j+1)*s])\n",
    "```\n",
    "\n",
    "### Average Pooling:\n",
    "```\n",
    "Output[i,j] = mean(Input[i*s:(i+1)*s, j*s:(j+1)*s])\n",
    "```\n",
    "\n",
    "### Strided Convolution:\n",
    "```\n",
    "Output = Conv2D(Input, kernel, stride=s)\n",
    "```\n",
    "\n",
    "Trong ƒë√≥:\n",
    "- `s`: Stride (b∆∞·ªõc nh·∫£y)\n",
    "- K√≠ch th∆∞·ªõc output = ‚åä(input_size - kernel_size) / stride‚åã + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb2e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# V√≠ d·ª• c√°c ph∆∞∆°ng ph√°p Downsampling\n",
    "class DownsamplingMethods(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Max Pooling\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # 2. Average Pooling\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # 3. Strided Convolution\n",
    "        self.strided_conv = nn.Conv2d(3, 3, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # 4. Adaptive Average Pooling (cho k√≠ch th∆∞·ªõc c·ªë ƒë·ªãnh)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((128, 128))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        \n",
    "        # Max pooling downsampling\n",
    "        max_pooled = self.max_pool(x)\n",
    "        print(f\"Max pooled shape: {max_pooled.shape}\")\n",
    "        \n",
    "        # Average pooling downsampling\n",
    "        avg_pooled = self.avg_pool(x)\n",
    "        print(f\"Average pooled shape: {avg_pooled.shape}\")\n",
    "        \n",
    "        # Strided convolution downsampling\n",
    "        strided = self.strided_conv(x)\n",
    "        print(f\"Strided conv shape: {strided.shape}\")\n",
    "        \n",
    "        # Adaptive pooling to fixed size\n",
    "        adaptive = self.adaptive_pool(x)\n",
    "        print(f\"Adaptive pooled shape: {adaptive.shape}\")\n",
    "        \n",
    "        return {\n",
    "            'max_pooled': max_pooled,\n",
    "            'avg_pooled': avg_pooled,\n",
    "            'strided': strided,\n",
    "            'adaptive': adaptive\n",
    "        }\n",
    "\n",
    "# Demo\n",
    "downsampler = DownsamplingMethods()\n",
    "\n",
    "# T·∫°o ·∫£nh gi·∫£ (batch_size=1, channels=3, height=256, width=256)\n",
    "input_tensor = torch.randn(1, 3, 256, 256)\n",
    "results = downsampler(input_tensor)\n",
    "\n",
    "print(\"\\n=== Downsampling Methods Demo ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31e1f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H√†m downsampling th·ª±c t·∫ø\n",
    "def downsample_image(image_tensor, factor=2, method='bilinear'):\n",
    "    \"\"\"\n",
    "    Downsample ·∫£nh v·ªõi c√°c ph∆∞∆°ng ph√°p kh√°c nhau\n",
    "    \n",
    "    Args:\n",
    "        image_tensor: Tensor ·∫£nh [B, C, H, W]\n",
    "        factor: H·ªá s·ªë gi·∫£m (2 = gi·∫£m m·ªôt n·ª≠a)\n",
    "        method: 'bilinear', 'nearest', 'area'\n",
    "    \n",
    "    Returns:\n",
    "        Downsampled tensor\n",
    "    \"\"\"\n",
    "    B, C, H, W = image_tensor.shape\n",
    "    new_H, new_W = H // factor, W // factor\n",
    "    \n",
    "    return F.interpolate(\n",
    "        image_tensor, \n",
    "        size=(new_H, new_W), \n",
    "        mode=method, \n",
    "        align_corners=False if method == 'bilinear' else None\n",
    "    )\n",
    "\n",
    "# Test downsampling function\n",
    "original = torch.randn(1, 3, 512, 512)\n",
    "print(f\"Original size: {original.shape}\")\n",
    "\n",
    "# Downsample by factor of 2\n",
    "downsampled_2x = downsample_image(original, factor=2)\n",
    "print(f\"Downsampled 2x: {downsampled_2x.shape}\")\n",
    "\n",
    "# Downsample by factor of 4\n",
    "downsampled_4x = downsample_image(original, factor=4)\n",
    "print(f\"Downsampled 4x: {downsampled_4x.shape}\")\n",
    "\n",
    "# Downsample by factor of 8\n",
    "downsampled_8x = downsample_image(original, factor=8)\n",
    "print(f\"Downsampled 8x: {downsampled_8x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2960a0",
   "metadata": {},
   "source": [
    "## So s√°nh c√°c ph∆∞∆°ng ph√°p Downsampling\n",
    "\n",
    "| Ph∆∞∆°ng ph√°p | ∆Øu ƒëi·ªÉm | Nh∆∞·ª£c ƒëi·ªÉm | ·ª®ng d·ª•ng |\n",
    "|-------------|---------|------------|----------|\n",
    "| **Max Pooling** | - B·∫£o to√†n ƒë·∫∑c tr∆∞ng quan tr·ªçng<br>- Invariant to small translations | - M·∫•t th√¥ng tin<br>- Kh√¥ng smooth | CNN feature extraction |\n",
    "| **Average Pooling** | - Smooth h∆°n<br>- Gi·∫£m noise | - L√†m m·ªù edges<br>- M·∫•t chi ti·∫øt | General downsampling |\n",
    "| **Strided Convolution** | - Learnable<br>- Flexible | - C·∫ßn training<br>- More parameters | Modern CNN architectures |\n",
    "| **Bilinear Interpolation** | - Smooth<br>- Continuous | - Computational cost<br>- Blurring | Image resizing |\n",
    "\n",
    "## Vai tr√≤ trong Latent Diffusion Models\n",
    "\n",
    "### 1. **VAE Encoder Downsampling**:\n",
    "```python\n",
    "# Trong VAE encoder\n",
    "x = downsample_block(x)  # 512x512 ‚Üí 256x256\n",
    "x = downsample_block(x)  # 256x256 ‚Üí 128x128  \n",
    "x = downsample_block(x)  # 128x128 ‚Üí 64x64\n",
    "# K·∫øt qu·∫£: latent space 64x64 thay v√¨ 512x512\n",
    "```\n",
    "\n",
    "### 2. **Computational Efficiency**:\n",
    "- **Gi·∫£m memory**: 512¬≤ = 262,144 pixels ‚Üí 64¬≤ = 4,096 pixels (64x √≠t h∆°n)\n",
    "- **TƒÉng t·ªëc**: Diffusion process ch·∫°y tr√™n latent space nh·ªè h∆°n\n",
    "- **Scalability**: C√≥ th·ªÉ x·ª≠ l√Ω ·∫£nh ƒë·ªô ph√¢n gi·∫£i cao\n",
    "\n",
    "### 3. **Multi-scale Processing**:\n",
    "- U-Net s·ª≠ d·ª•ng nhi·ªÅu m·ª©c downsampling\n",
    "- Skip connections ƒë·ªÉ b·∫£o to√†n th√¥ng tin\n",
    "- Progressive refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e0f12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√≠ d·ª•: VAE Encoder v·ªõi Downsampling (simplified)\n",
    "class VAEEncoderWithDownsampling(nn.Module):\n",
    "    def __init__(self, input_channels=3, latent_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Progressive downsampling\n",
    "        self.encoder = nn.Sequential(\n",
    "            # 512x512 ‚Üí 256x256\n",
    "            nn.Conv2d(input_channels, 64, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 256x256 ‚Üí 128x128\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 128x128 ‚Üí 64x64\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 64x64 ‚Üí 32x32\n",
    "            nn.Conv2d(256, 512, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 32x32 ‚Üí 16x16\n",
    "            nn.Conv2d(512, 512, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Final layers cho mu v√† logvar\n",
    "        self.fc_mu = nn.Conv2d(512, latent_dim, 1)\n",
    "        self.fc_logvar = nn.Conv2d(512, latent_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(f\"Input: {x.shape}\")\n",
    "        \n",
    "        # Progressive downsampling\n",
    "        features = self.encoder(x)\n",
    "        print(f\"After downsampling: {features.shape}\")\n",
    "        \n",
    "        # Generate mu and logvar\n",
    "        mu = self.fc_mu(features)\n",
    "        logvar = self.fc_logvar(features)\n",
    "        \n",
    "        print(f\"Latent mu: {mu.shape}\")\n",
    "        print(f\"Latent logvar: {logvar.shape}\")\n",
    "        \n",
    "        return mu, logvar\n",
    "\n",
    "# Demo VAE Encoder\n",
    "encoder = VAEEncoderWithDownsampling()\n",
    "input_image = torch.randn(1, 3, 512, 512)\n",
    "mu, logvar = encoder(input_image)\n",
    "\n",
    "print(f\"\\nDownsampling ratio: {512//16}x (512x512 ‚Üí 16x16)\")\n",
    "print(f\"Memory reduction: {(512*512)/(16*16):.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ef1b5e",
   "metadata": {},
   "source": [
    "## T·ªïng k·∫øt v·ªÅ Downsampling\n",
    "\n",
    "### Downsampling l√† g√¨?\n",
    "- **ƒê·ªãnh nghƒ©a**: Qu√° tr√¨nh gi·∫£m k√≠ch th∆∞·ªõc ho·∫∑c ƒë·ªô ph√¢n gi·∫£i c·ªßa d·ªØ li·ªáu\n",
    "- **M·ª•c ƒë√≠ch**: Gi·∫£m computational cost, memory usage, v√† tƒÉng receptive field\n",
    "- **Trade-off**: Gi·∫£m chi ti·∫øt nh∆∞ng tƒÉng efficiency\n",
    "\n",
    "### C√°c ph∆∞∆°ng ph√°p ch√≠nh:\n",
    "1. **Max/Average Pooling**: ƒê∆°n gi·∫£n, nhanh\n",
    "2. **Strided Convolution**: Learnable, linh ho·∫°t  \n",
    "3. **Interpolation**: Smooth, continuous\n",
    "\n",
    "### Vai tr√≤ trong Stable Diffusion:\n",
    "1. **VAE Compression**: Gi·∫£m ·∫£nh 512x512 ‚Üí latent 64x64\n",
    "2. **Efficiency**: Diffusion process ch·∫°y nhanh h∆°n 64x\n",
    "3. **Scalability**: X·ª≠ l√Ω ƒë∆∞·ª£c ·∫£nh high-resolution\n",
    "4. **Quality**: V·∫´n b·∫£o to√†n th√¥ng tin quan tr·ªçng nh·ªù perceptual loss\n",
    "\n",
    "### Key Benefits:\n",
    "- **Memory**: Gi·∫£m 64x memory usage\n",
    "- **Speed**: TƒÉng 64x training/inference speed  \n",
    "- **Quality**: Maintained through perceptual compression\n",
    "- **Flexibility**: Support nhi·ªÅu resolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69b3de1",
   "metadata": {},
   "source": [
    "# High Variance trong Machine Learning\n",
    "\n",
    "## ƒê·ªãnh nghƒ©a\n",
    "**High Variance** (Ph∆∞∆°ng sai cao) l√† m·ªôt hi·ªán t∆∞·ª£ng trong machine learning khi model **qu√° nh·∫°y c·∫£m** v·ªõi nh·ªØng thay ƒë·ªïi nh·ªè trong training data, d·∫´n ƒë·∫øn k·∫øt qu·∫£ **kh√¥ng ·ªïn ƒë·ªãnh** v√† **kh√≥ d·ª± ƒëo√°n**.\n",
    "\n",
    "## ƒê·∫∑c ƒëi·ªÉm c·ªßa High Variance:\n",
    "\n",
    "### 1. **Overfitting**:\n",
    "- Model h·ªçc qu√° chi ti·∫øt t·ª´ training data\n",
    "- Performance t·ªët tr√™n training set nh∆∞ng k√©m tr√™n validation/test set\n",
    "- Model \"ghi nh·ªõ\" noise thay v√¨ h·ªçc pattern th·ª±c s·ª±\n",
    "\n",
    "### 2. **Instability** (Kh√¥ng ·ªïn ƒë·ªãnh):\n",
    "- K·∫øt qu·∫£ thay ƒë·ªïi l·ªõn khi thay ƒë·ªïi training data m·ªôt ch√∫t\n",
    "- Model predictions kh√¥ng consistent\n",
    "- High sensitivity to random fluctuations\n",
    "\n",
    "### 3. **Poor Generalization**:\n",
    "- Kh√¥ng generalize t·ªët cho unseen data\n",
    "- Gap l·ªõn gi·ªØa training v√† validation performance\n",
    "- Model qu√° \"specific\" cho training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afde4a0",
   "metadata": {},
   "source": [
    "## C√¥ng th·ª©c To√°n h·ªçc\n",
    "\n",
    "### Variance c·ªßa Model:\n",
    "```\n",
    "Variance = E[(f(x) - E[f(x)])¬≤]\n",
    "```\n",
    "\n",
    "### Bias-Variance Tradeoff:\n",
    "```\n",
    "Total Error = Bias¬≤ + Variance + Irreducible Error\n",
    "```\n",
    "\n",
    "Trong ƒë√≥:\n",
    "- **Bias**: Sai s·ªë systematic do model qu√° ƒë∆°n gi·∫£n\n",
    "- **Variance**: Sai s·ªë do model qu√° ph·ª©c t·∫°p v√† unstable\n",
    "- **Irreducible Error**: Noise inherent trong data\n",
    "\n",
    "### High Variance Indicators:\n",
    "- **Training Error << Validation Error**\n",
    "- **Large gap between train/val performance**\n",
    "- **Model predictions vary widely v·ªõi small data changes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14d191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# T·∫°o synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X = np.linspace(0, 1, n_samples).reshape(-1, 1)\n",
    "y = 1.5 * X.ravel() + 0.3 * np.sin(15 * X.ravel()) + 0.1 * np.random.randn(n_samples)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Demonstrate High Variance v·ªõi Polynomial Regression\n",
    "def demonstrate_variance(degrees, n_experiments=50):\n",
    "    \"\"\"\n",
    "    Demonstrate high variance v·ªõi polynomial regression\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for degree in degrees:\n",
    "        train_errors = []\n",
    "        test_errors = []\n",
    "        predictions = []\n",
    "        \n",
    "        # Multiple experiments v·ªõi different random splits\n",
    "        for i in range(n_experiments):\n",
    "            # Random split m·ªói l·∫ßn\n",
    "            X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=i)\n",
    "            \n",
    "            # Create polynomial model\n",
    "            poly_model = Pipeline([\n",
    "                ('poly', PolynomialFeatures(degree=degree)),\n",
    "                ('linear', LinearRegression())\n",
    "            ])\n",
    "            \n",
    "            # Train model\n",
    "            poly_model.fit(X_tr, y_tr)\n",
    "            \n",
    "            # Predictions\n",
    "            y_train_pred = poly_model.predict(X_tr)\n",
    "            y_test_pred = poly_model.predict(X_te)\n",
    "            \n",
    "            # Calculate errors\n",
    "            train_error = mean_squared_error(y_tr, y_train_pred)\n",
    "            test_error = mean_squared_error(y_te, y_test_pred)\n",
    "            \n",
    "            train_errors.append(train_error)\n",
    "            test_errors.append(test_error)\n",
    "            \n",
    "            # Store predictions for visualization\n",
    "            if i < 10:  # Ch·ªâ store first 10 experiments\n",
    "                X_plot = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "                y_plot_pred = poly_model.predict(X_plot)\n",
    "                predictions.append(y_plot_pred)\n",
    "        \n",
    "        results[degree] = {\n",
    "            'train_errors': train_errors,\n",
    "            'test_errors': test_errors, \n",
    "            'predictions': predictions,\n",
    "            'train_mean': np.mean(train_errors),\n",
    "            'train_std': np.std(train_errors),\n",
    "            'test_mean': np.mean(test_errors),\n",
    "            'test_std': np.std(test_errors)\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test v·ªõi different polynomial degrees\n",
    "degrees = [1, 3, 9, 15]  # Low to High complexity\n",
    "results = demonstrate_variance(degrees)\n",
    "\n",
    "# Print results\n",
    "print(\"=== Bias-Variance Analysis ===\")\n",
    "print(f\"{'Degree':<8} {'Train Mean':<12} {'Train Std':<12} {'Test Mean':<12} {'Test Std':<12} {'Variance':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for degree in degrees:\n",
    "    r = results[degree]\n",
    "    variance_indicator = \"HIGH\" if r['test_std'] > 0.05 else \"LOW\"\n",
    "    print(f\"{degree:<8} {r['train_mean']:<12.4f} {r['train_std']:<12.4f} {r['test_mean']:<12.4f} {r['test_std']:<12.4f} {variance_indicator:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd89bf4",
   "metadata": {},
   "source": [
    "## So s√°nh High Bias vs High Variance\n",
    "\n",
    "| Aspect | High Bias (Underfitting) | High Variance (Overfitting) |\n",
    "|--------|---------------------------|------------------------------|\n",
    "| **Training Error** | High | Low |\n",
    "| **Validation Error** | High | High |\n",
    "| **Error Gap** | Small | Large |\n",
    "| **Model Complexity** | Too Simple | Too Complex |\n",
    "| **Symptoms** | Poor performance everywhere | Good on train, bad on validation |\n",
    "| **Example** | Linear model cho non-linear data | Deep network v·ªõi √≠t data |\n",
    "\n",
    "## C√°ch nh·∫≠n bi·∫øt High Variance:\n",
    "\n",
    "### 1. **Performance Metrics**:\n",
    "```python\n",
    "# High Variance indicators\n",
    "training_accuracy = 0.95\n",
    "validation_accuracy = 0.65\n",
    "gap = training_accuracy - validation_accuracy  # 0.30 (large gap!)\n",
    "\n",
    "if gap > 0.15:  # Threshold example\n",
    "    print(\"High Variance detected!\")\n",
    "```\n",
    "\n",
    "### 2. **Learning Curves**:\n",
    "- Training error gi·∫£m li√™n t·ª•c\n",
    "- Validation error tƒÉng ho·∫∑c plateau\n",
    "- Gap l·ªõn v√† persistent gi·ªØa train/val curves\n",
    "\n",
    "### 3. **Cross-Validation**:\n",
    "- High standard deviation across folds\n",
    "- Inconsistent performance across different data splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071846c3",
   "metadata": {},
   "source": [
    "## Gi·∫£i ph√°p cho High Variance\n",
    "\n",
    "### 1. **Regularization**:\n",
    "```python\n",
    "# L1/L2 Regularization\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "# L2 Regularization (Ridge)\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "\n",
    "# L1 Regularization (Lasso)\n",
    "lasso_model = Lasso(alpha=0.1)\n",
    "```\n",
    "\n",
    "### 2. **More Training Data**:\n",
    "- Collect more samples\n",
    "- Data augmentation\n",
    "- Synthetic data generation\n",
    "\n",
    "### 3. **Reduce Model Complexity**:\n",
    "```python\n",
    "# Gi·∫£m parameters\n",
    "- Fewer layers trong neural networks\n",
    "- Lower polynomial degree\n",
    "- Feature selection\n",
    "- Pruning\n",
    "```\n",
    "\n",
    "### 4. **Ensemble Methods**:\n",
    "```python\n",
    "# Bagging reduces variance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "# Voting classifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "ensemble = VotingClassifier([('model1', model1), ('model2', model2)])\n",
    "```\n",
    "\n",
    "### 5. **Dropout v√† Early Stopping**:\n",
    "```python\n",
    "# For neural networks\n",
    "import torch.nn as nn\n",
    "\n",
    "class ModelWithDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(100, 50)\n",
    "        self.dropout = nn.Dropout(0.3)  # Gi·∫£m overfitting\n",
    "        self.layer2 = nn.Linear(50, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.dropout(x)  # Randomly zero out neurons\n",
    "        return self.layer2(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ac60b3",
   "metadata": {},
   "source": [
    "## High Variance trong Diffusion Models\n",
    "\n",
    "### 1. **Sampling Variance**:\n",
    "Trong diffusion models, sampling process c√≥ th·ªÉ c√≥ high variance:\n",
    "\n",
    "```python\n",
    "# Multiple samples t·ª´ c√πng m·ªôt noise\n",
    "for i in range(5):\n",
    "    noise = torch.randn_like(latent)  # Same shape, different random values\n",
    "    sample = diffusion_model.sample(noise, prompt)\n",
    "    # K·∫øt qu·∫£ c√≥ th·ªÉ vary significantly\n",
    "```\n",
    "\n",
    "### 2. **Training Instability**:\n",
    "- Diffusion loss c√≥ th·ªÉ fluctuate wildly\n",
    "- Gradient variance cao do random timestep sampling\n",
    "- Model weights update inconsistently\n",
    "\n",
    "### 3. **Solutions trong Stable Diffusion**:\n",
    "\n",
    "#### **Classifier-Free Guidance**:\n",
    "```python\n",
    "# Reduce variance b·∫±ng guidance\n",
    "guided_prediction = unconditional_pred + guidance_scale * (conditional_pred - unconditional_pred)\n",
    "# guidance_scale gi√∫p control variance vs quality tradeoff\n",
    "```\n",
    "\n",
    "#### **Variance Reduction Techniques**:\n",
    "```python\n",
    "# 1. Antithetic sampling\n",
    "noise_1 = torch.randn_like(x)\n",
    "noise_2 = -noise_1  # Antithetic pair\n",
    "\n",
    "# 2. Low-discrepancy sequences thay v√¨ pure random\n",
    "# 3. Importance sampling cho timesteps\n",
    "```\n",
    "\n",
    "#### **Progressive Training**:\n",
    "- Start v·ªõi simple tasks (low variance)\n",
    "- Gradually increase complexity\n",
    "- Curriculum learning approach\n",
    "\n",
    "### 4. **VAE Regularization**:\n",
    "```python\n",
    "# KL divergence trong VAE gi√∫p control variance\n",
    "kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "# Beta-VAE: beta * kl_loss (beta > 1 reduces variance)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe1e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical Example: Detecting High Variance trong Training\n",
    "class VarianceMonitor:\n",
    "    def __init__(self, window_size=100):\n",
    "        self.window_size = window_size\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.predictions_history = []\n",
    "    \n",
    "    def update(self, train_loss, val_loss, predictions=None):\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        if predictions is not None:\n",
    "            self.predictions_history.append(predictions)\n",
    "    \n",
    "    def check_variance(self):\n",
    "        if len(self.train_losses) < self.window_size:\n",
    "            return \"Insufficient data\"\n",
    "        \n",
    "        recent_train = self.train_losses[-self.window_size:]\n",
    "        recent_val = self.val_losses[-self.window_size:]\n",
    "        \n",
    "        # Check gap between train and validation\n",
    "        avg_train = np.mean(recent_train)\n",
    "        avg_val = np.mean(recent_val)\n",
    "        gap = avg_val - avg_train\n",
    "        \n",
    "        # Check stability (variance of losses)\n",
    "        train_variance = np.var(recent_train)\n",
    "        val_variance = np.var(recent_val)\n",
    "        \n",
    "        # Check prediction consistency\n",
    "        pred_variance = 0\n",
    "        if len(self.predictions_history) >= 5:\n",
    "            recent_preds = self.predictions_history[-5:]\n",
    "            pred_variance = np.var([np.mean(pred) for pred in recent_preds])\n",
    "        \n",
    "        results = {\n",
    "            'train_val_gap': gap,\n",
    "            'train_variance': train_variance,\n",
    "            'val_variance': val_variance,\n",
    "            'prediction_variance': pred_variance,\n",
    "            'high_variance_detected': gap > 0.1 or val_variance > 0.05\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def suggest_solutions(self):\n",
    "        analysis = self.check_variance()\n",
    "        suggestions = []\n",
    "        \n",
    "        if analysis['high_variance_detected']:\n",
    "            suggestions.append(\"üö® High Variance Detected!\")\n",
    "            \n",
    "            if analysis['train_val_gap'] > 0.1:\n",
    "                suggestions.extend([\n",
    "                    \"‚Ä¢ Add regularization (L1/L2, Dropout)\",\n",
    "                    \"‚Ä¢ Collect more training data\", \n",
    "                    \"‚Ä¢ Reduce model complexity\",\n",
    "                    \"‚Ä¢ Use early stopping\"\n",
    "                ])\n",
    "            \n",
    "            if analysis['val_variance'] > 0.05:\n",
    "                suggestions.extend([\n",
    "                    \"‚Ä¢ Use ensemble methods\",\n",
    "                    \"‚Ä¢ Implement cross-validation\",\n",
    "                    \"‚Ä¢ Check data quality\"\n",
    "                ])\n",
    "                \n",
    "            if analysis['prediction_variance'] > 0.1:\n",
    "                suggestions.extend([\n",
    "                    \"‚Ä¢ Increase training epochs\",\n",
    "                    \"‚Ä¢ Adjust learning rate\",\n",
    "                    \"‚Ä¢ Use learning rate scheduling\"\n",
    "                ])\n",
    "        else:\n",
    "            suggestions.append(\"‚úÖ Variance levels look healthy!\")\n",
    "        \n",
    "        return suggestions\n",
    "\n",
    "# Demo usage\n",
    "monitor = VarianceMonitor()\n",
    "\n",
    "# Simulate training v·ªõi high variance\n",
    "for epoch in range(200):\n",
    "    # Simulate decreasing train loss but fluctuating val loss\n",
    "    train_loss = 1.0 * np.exp(-epoch/50) + 0.01 * np.random.randn()\n",
    "    val_loss = 0.5 + 0.3 * np.sin(epoch/10) + 0.1 * np.random.randn()\n",
    "    \n",
    "    monitor.update(train_loss, val_loss)\n",
    "    \n",
    "    if epoch % 50 == 0 and epoch > 100:\n",
    "        analysis = monitor.check_variance()\n",
    "        suggestions = monitor.suggest_solutions()\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch} Analysis:\")\n",
    "        print(f\"Train-Val Gap: {analysis['train_val_gap']:.3f}\")\n",
    "        print(f\"Validation Variance: {analysis['val_variance']:.3f}\")\n",
    "        print(\"Suggestions:\")\n",
    "        for suggestion in suggestions:\n",
    "            print(f\"  {suggestion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59abbd58",
   "metadata": {},
   "source": [
    "## T·ªïng k·∫øt v·ªÅ High Variance\n",
    "\n",
    "### High Variance l√† g√¨?\n",
    "- **ƒê·ªãnh nghƒ©a**: Model qu√° nh·∫°y c·∫£m v·ªõi changes trong training data\n",
    "- **Tri·ªáu ch·ª©ng**: Overfitting, performance gap l·ªõn, predictions kh√¥ng stable\n",
    "- **Nguy√™n nh√¢n**: Model qu√° complex, data qu√° √≠t, lack of regularization\n",
    "\n",
    "### Key Indicators:\n",
    "1. **Large Train-Validation Gap**: Gap > 10-15%\n",
    "2. **High Standard Deviation**: Trong cross-validation results \n",
    "3. **Unstable Predictions**: Vary widely v·ªõi small data changes\n",
    "4. **Learning Curves**: Train error gi·∫£m nh∆∞ng val error tƒÉng\n",
    "\n",
    "### Main Solutions:\n",
    "1. **Regularization**: L1/L2, Dropout, Early Stopping\n",
    "2. **More Data**: Collection, Augmentation, Synthesis\n",
    "3. **Model Simplification**: Fewer parameters, Feature selection\n",
    "4. **Ensemble Methods**: Bagging, Voting, Stacking\n",
    "5. **Cross-Validation**: Better evaluation v√† model selection\n",
    "\n",
    "### Trong Diffusion Models:\n",
    "- **Sampling variance**: Multiple runs give different results\n",
    "- **Training instability**: Loss fluctuations, gradient variance\n",
    "- **Solutions**: Classifier-free guidance, antithetic sampling, progressive training\n",
    "\n",
    "### Remember:\n",
    "**High Variance = High Complexity + Low Stability**\n",
    "- Trade-off v·ªõi bias: Reducing variance might increase bias\n",
    "- Goal: Find optimal balance for best generalization\n",
    "- Monitor continuously during training process\n",
    "\n",
    "### Key Takeaway:\n",
    "*\"A model with high variance is like a weather vane - it moves dramatically with small changes in the wind (data), making it unreliable for consistent predictions.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0948592",
   "metadata": {},
   "source": [
    "# Diffusion Models - Hi·ªÉu s√¢u v·ªÅ c∆° ch·∫ø ho·∫°t ƒë·ªông\n",
    "\n",
    "## ƒê·ªãnh nghƒ©a c∆° b·∫£n\n",
    "**Diffusion Models** l√† c√°c **m√¥ h√¨nh x√°c su·∫•t** ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ h·ªçc ph√¢n ph·ªëi d·ªØ li·ªáu `p(x)` b·∫±ng c√°ch **t·ª´ t·ª´ kh·ª≠ nhi·ªÖu** m·ªôt bi·∫øn c√≥ ph√¢n ph·ªëi chu·∫©n.\n",
    "\n",
    "## √ù t∆∞·ªüng ch√≠nh\n",
    "\n",
    "### 1. **Qu√° tr√¨nh ng∆∞·ª£c c·ªßa Markov Chain**:\n",
    "- Diffusion models h·ªçc **qu√° tr√¨nh ng∆∞·ª£c** c·ªßa m·ªôt chu·ªói Markov c√≥ ƒë·ªô d√†i T\n",
    "- **Forward process**: x‚ÇÄ ‚Üí x‚ÇÅ ‚Üí x‚ÇÇ ‚Üí ... ‚Üí x‚Çú (th√™m nhi·ªÖu d·∫ßn)\n",
    "- **Reverse process**: x‚Çú ‚Üí x‚Çú‚Çã‚ÇÅ ‚Üí ... ‚Üí x‚ÇÅ ‚Üí x‚ÇÄ (kh·ª≠ nhi·ªÖu d·∫ßn)\n",
    "\n",
    "### 2. **T·ª´ nhi·ªÖu ƒë·∫øn ·∫£nh th·∫≠t**:\n",
    "```\n",
    "Noise ~ N(0,1) ‚Üí [Diffusion Model] ‚Üí Real Image\n",
    "```\n",
    "\n",
    "## C√°ch ho·∫°t ƒë·ªông chi ti·∫øt\n",
    "\n",
    "### **Forward Process (Th√™m nhi·ªÖu)**:\n",
    "```\n",
    "q(x‚ÇÅ:‚Çú|x‚ÇÄ) = ‚àè q(x‚Çú|x‚Çú‚Çã‚ÇÅ)\n",
    "```\n",
    "- B·∫Øt ƒë·∫ßu t·ª´ ·∫£nh th·∫≠t x‚ÇÄ\n",
    "- T·ª´ t·ª´ th√™m nhi·ªÖu Gaussian ·ªü m·ªói b∆∞·ªõc\n",
    "- Cu·ªëi c√πng c√≥ nhi·ªÖu thu·∫ßn t√∫y x‚Çú ~ N(0,1)\n",
    "\n",
    "### **Reverse Process (Kh·ª≠ nhi·ªÖu)**:\n",
    "```\n",
    "pŒ∏(x‚ÇÄ:‚Çú‚Çã‚ÇÅ|x‚Çú) = ‚àè pŒ∏(x‚Çú‚Çã‚ÇÅ|x‚Çú)\n",
    "```\n",
    "- B·∫Øt ƒë·∫ßu t·ª´ nhi·ªÖu x‚Çú\n",
    "- Model h·ªçc c√°ch **ƒëo√°n nhi·ªÖu** ƒë·ªÉ lo·∫°i b·ªè\n",
    "- T·ª´ t·ª´ t·∫°o ra ·∫£nh th·∫≠t x‚ÇÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0be4212",
   "metadata": {},
   "source": [
    "## C√¥ng th·ª©c to√°n h·ªçc quan tr·ªçng\n",
    "\n",
    "### **Variational Lower Bound**:\n",
    "Diffusion models s·ª≠ d·ª•ng m·ªôt bi·∫øn th·ªÉ c·ªßa **variational lower bound** tr√™n p(x):\n",
    "\n",
    "```\n",
    "log p(x) ‚â• E[log pŒ∏(x‚ÇÄ|x‚ÇÅ)] - KL[q(x‚ÇÅ|x‚ÇÄ)||pŒ∏(x‚ÇÅ)] - ...\n",
    "```\n",
    "\n",
    "### **Denoising Score Matching**:\n",
    "Ph∆∞∆°ng ph√°p n√†y t∆∞∆°ng ƒë∆∞∆°ng v·ªõi **denoising score-matching**:\n",
    "- Thay v√¨ h·ªçc p(x) tr·ª±c ti·∫øp\n",
    "- Model h·ªçc **score function**: ‚àá‚Çì log p(x)\n",
    "- Qua vi·ªác d·ª± ƒëo√°n nhi·ªÖu c·∫ßn lo·∫°i b·ªè\n",
    "\n",
    "### **Simplified Loss Function**:\n",
    "Loss function ƒë∆∞·ª£c ƒë∆°n gi·∫£n h√≥a th√†nh:\n",
    "\n",
    "```\n",
    "LDM = Ex,Œµ~N(0,1),t [||Œµ - ŒµŒ∏(xt, t)||‚ÇÇ¬≤]\n",
    "```\n",
    "\n",
    "**Gi·∫£i th√≠ch**:\n",
    "- `x`: ·∫¢nh g·ªëc (clean image)\n",
    "- `Œµ ~ N(0,1)`: Nhi·ªÖu ng·∫´u nhi√™n ƒë∆∞·ª£c th√™m v√†o\n",
    "- `t`: Timestep ƒë∆∞·ª£c ch·ªçn ng·∫´u nhi√™n t·ª´ {1,...,T}\n",
    "- `xt`: ·∫¢nh ƒë√£ b·ªã nhi·ªÖu ·ªü timestep t\n",
    "- `ŒµŒ∏(xt, t)`: Model d·ª± ƒëo√°n nhi·ªÖu\n",
    "- `||Œµ - ŒµŒ∏(xt, t)||‚ÇÇ¬≤`: Sai s·ªë L2 gi·ªØa nhi·ªÖu th·∫≠t v√† nhi·ªÖu d·ª± ƒëo√°n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f281d6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class SimpleDiffusionLoss(nn.Module):\n",
    "    def __init__(self, num_timesteps=1000):\n",
    "        super().__init__()\n",
    "        self.num_timesteps = num_timesteps\n",
    "        \n",
    "        # T·∫°o noise schedule (beta values)\n",
    "        self.betas = torch.linspace(0.0001, 0.02, num_timesteps)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "    \n",
    "    def add_noise(self, x0, noise, timesteps):\n",
    "        \"\"\"\n",
    "        Th√™m nhi·ªÖu v√†o ·∫£nh g·ªëc theo c√¥ng th·ª©c:\n",
    "        xt = sqrt(alphas_cumprod_t) * x0 + sqrt(1 - alphas_cumprod_t) * noise\n",
    "        \"\"\"\n",
    "        sqrt_alphas_cumprod_t = torch.sqrt(self.alphas_cumprod[timesteps])\n",
    "        sqrt_one_minus_alphas_cumprod_t = torch.sqrt(1.0 - self.alphas_cumprod[timesteps])\n",
    "        \n",
    "        # Reshape ƒë·ªÉ broadcast ƒë√∫ng\n",
    "        sqrt_alphas_cumprod_t = sqrt_alphas_cumprod_t.view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = sqrt_one_minus_alphas_cumprod_t.view(-1, 1, 1, 1)\n",
    "        \n",
    "        return sqrt_alphas_cumprod_t * x0 + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "    \n",
    "    def forward(self, model, x0):\n",
    "        \"\"\"\n",
    "        T√≠nh diffusion loss\n",
    "        \n",
    "        Args:\n",
    "            model: Neural network d·ª± ƒëo√°n nhi·ªÖu ŒµŒ∏(xt, t)\n",
    "            x0: Batch ·∫£nh g·ªëc [B, C, H, W]\n",
    "        \n",
    "        Returns:\n",
    "            loss: Scalar loss value\n",
    "        \"\"\"\n",
    "        batch_size = x0.shape[0]\n",
    "        \n",
    "        # 1. Sample random noise Œµ ~ N(0,1)\n",
    "        noise = torch.randn_like(x0)\n",
    "        \n",
    "        # 2. Sample random timesteps t\n",
    "        timesteps = torch.randint(0, self.num_timesteps, (batch_size,), device=x0.device)\n",
    "        \n",
    "        # 3. Add noise to get xt\n",
    "        xt = self.add_noise(x0, noise, timesteps)\n",
    "        \n",
    "        # 4. Model d·ª± ƒëo√°n nhi·ªÖu\n",
    "        predicted_noise = model(xt, timesteps)\n",
    "        \n",
    "        # 5. T√≠nh L2 loss gi·ªØa nhi·ªÖu th·∫≠t v√† d·ª± ƒëo√°n\n",
    "        loss = F.mse_loss(predicted_noise, noise)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "# V√≠ d·ª• s·ª≠ d·ª•ng\n",
    "class SimpleUNet(nn.Module):\n",
    "    \"\"\"Simplified U-Net cho demo\"\"\"\n",
    "    def __init__(self, in_channels=3, time_emb_dim=128):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, time_emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim)\n",
    "        )\n",
    "        \n",
    "        # Simplified encoder-decoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(64 + time_emb_dim, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, in_channels, 3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        # Time embedding\n",
    "        t_emb = self.time_mlp(t.float().unsqueeze(-1))  # [B, time_emb_dim]\n",
    "        t_emb = t_emb.view(t_emb.shape[0], t_emb.shape[1], 1, 1)  # [B, time_emb_dim, 1, 1]\n",
    "        t_emb = t_emb.expand(-1, -1, x.shape[2], x.shape[3])  # [B, time_emb_dim, H, W]\n",
    "        \n",
    "        # Encoder\n",
    "        x_enc = self.encoder(x)\n",
    "        \n",
    "        # Combine with time embedding\n",
    "        x_combined = torch.cat([x_enc, t_emb], dim=1)\n",
    "        \n",
    "        # Decoder (predict noise)\n",
    "        noise_pred = self.decoder(x_combined)\n",
    "        \n",
    "        return noise_pred\n",
    "\n",
    "# Demo\n",
    "model = SimpleUNet()\n",
    "loss_fn = SimpleDiffusionLoss(num_timesteps=1000)\n",
    "\n",
    "# T·∫°o batch ·∫£nh gi·∫£\n",
    "batch_size = 4\n",
    "images = torch.randn(batch_size, 3, 64, 64)  # [B, C, H, W]\n",
    "\n",
    "# T√≠nh loss\n",
    "loss = loss_fn(model, images)\n",
    "print(f\"Diffusion Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Gi·∫£i th√≠ch qu√° tr√¨nh:\n",
    "print(\"\\n=== Qu√° tr√¨nh Training Diffusion Model ===\")\n",
    "print(\"1. L·∫•y ·∫£nh g·ªëc x0\")\n",
    "print(\"2. Sample nhi·ªÖu Œµ ~ N(0,1)\")\n",
    "print(\"3. Sample timestep t ng·∫´u nhi√™n\")\n",
    "print(\"4. T·∫°o ·∫£nh nhi·ªÖu xt = ‚àö(Œ±ÃÑt) * x0 + ‚àö(1-Œ±ÃÑt) * Œµ\")\n",
    "print(\"5. Model d·ª± ƒëo√°n nhi·ªÖu: ŒµŒ∏(xt, t)\")\n",
    "print(\"6. T√≠nh loss: ||Œµ - ŒµŒ∏(xt, t)||¬≤\")\n",
    "print(\"7. Backprop v√† update weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fba835a",
   "metadata": {},
   "source": [
    "## Denoising Autoencoders ŒµŒ∏(xt, t)\n",
    "\n",
    "### **√ù t∆∞·ªüng ch√≠nh**:\n",
    "Diffusion models c√≥ th·ªÉ ƒë∆∞·ª£c hi·ªÉu nh∆∞ m·ªôt **chu·ªói c√°c denoising autoencoders** c√≥ tr·ªçng s·ªë b·∫±ng nhau:\n",
    "- **ŒµŒ∏(xt, t)** v·ªõi t = 1, 2, ..., T\n",
    "- M·ªói autoencoder ƒë∆∞·ª£c train ƒë·ªÉ d·ª± ƒëo√°n nhi·ªÖu trong ·∫£nh xt\n",
    "- **xt** l√† phi√™n b·∫£n nhi·ªÖu c·ªßa ·∫£nh ƒë·∫ßu v√†o x\n",
    "\n",
    "### **T·∫°i sao g·ªçi l√† \"Equally weighted sequence\"?**\n",
    "```\n",
    "LDM = Ex,Œµ~N(0,1),t [||Œµ - ŒµŒ∏(xt, t)||¬≤]\n",
    "```\n",
    "- M·ªói timestep t c√≥ **tr·ªçng s·ªë b·∫±ng nhau** (equally weighted)\n",
    "- Kh√¥ng c√≥ Œªt trong c√¥ng th·ª©c (kh√°c v·ªõi original DDPM)\n",
    "- ƒê√¢y l√† **simplified version** c·ªßa variational lower bound\n",
    "\n",
    "### **Input v√† Output**:\n",
    "- **Input**: \n",
    "  - `xt`: ·∫¢nh ƒë√£ b·ªã nhi·ªÖu ·ªü timestep t\n",
    "  - `t`: Timestep (cho model bi·∫øt m·ª©c ƒë·ªô nhi·ªÖu)\n",
    "- **Output**: \n",
    "  - `ŒµŒ∏(xt, t)`: D·ª± ƒëo√°n nhi·ªÖu c·∫ßn lo·∫°i b·ªè\n",
    "\n",
    "### **Denoised variant**:\n",
    "- Model kh√¥ng d·ª± ƒëo√°n ·∫£nh s·∫°ch x0 tr·ª±c ti·∫øp\n",
    "- M√† d·ª± ƒëo√°n **nhi·ªÖu Œµ** ƒë·ªÉ lo·∫°i b·ªè\n",
    "- T·ª´ ƒë√≥ t√≠nh ra ·∫£nh s·∫°ch: `x0 ‚âà (xt - ‚àö(1-Œ±ÃÑt) * ŒµŒ∏(xt,t)) / ‚àö(Œ±ÃÑt)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6336026f",
   "metadata": {},
   "source": [
    "## Gi·∫£i th√≠ch ƒëo·∫°n vƒÉn trong paper\n",
    "\n",
    "> *\"Diffusion Models [82] are probabilistic models designed to learn a data distribution p(x) by gradually denoising a normally distributed variable, which corresponds to learning the reverse process of a fixed Markov Chain of length T.\"*\n",
    "\n",
    "**D·ªãch v√† gi·∫£i th√≠ch**:\n",
    "- **\"Probabilistic models\"**: M√¥ h√¨nh x√°c su·∫•t\n",
    "- **\"Learn a data distribution p(x)\"**: H·ªçc ph√¢n ph·ªëi d·ªØ li·ªáu (v√≠ d·ª•: ph√¢n ph·ªëi c·ªßa t·∫•t c·∫£ ·∫£nh m√®o)\n",
    "- **\"Gradually denoising\"**: T·ª´ t·ª´ kh·ª≠ nhi·ªÖu (kh√¥ng ph·∫£i m·ªôt l·∫ßn)\n",
    "- **\"Normally distributed variable\"**: Bi·∫øn c√≥ ph√¢n ph·ªëi chu·∫©n (Gaussian noise)\n",
    "- **\"Reverse process of fixed Markov Chain\"**: Qu√° tr√¨nh ng∆∞·ª£c c·ªßa chu·ªói Markov c·ªë ƒë·ªãnh\n",
    "\n",
    "> *\"For image synthesis, the most successful models [15,30,72] rely on a reweighted variant of the variational lower bound on p(x), which mirrors denoising score-matching [85].\"*\n",
    "\n",
    "**Gi·∫£i th√≠ch**:\n",
    "- **\"Reweighted variant\"**: Bi·∫øn th·ªÉ c√≥ tr·ªçng s·ªë kh√°c c·ªßa variational lower bound\n",
    "- **\"Mirrors denoising score-matching\"**: T∆∞∆°ng ƒë∆∞∆°ng v·ªõi ph∆∞∆°ng ph√°p denoising score-matching\n",
    "- Thay v√¨ d√πng c√¥ng th·ª©c ph·ª©c t·∫°p, h·ªç ƒë∆°n gi·∫£n h√≥a th√†nh MSE loss\n",
    "\n",
    "> *\"These models can be interpreted as an equally weighted sequence of denoising autoencoders ŒµŒ∏(xt,t); t = 1...T, which are trained to predict a denoised variant of their input xt, where xt is a noisy version of the input x.\"*\n",
    "\n",
    "**Gi·∫£i th√≠ch**:\n",
    "- **\"Equally weighted sequence\"**: Chu·ªói c√≥ tr·ªçng s·ªë b·∫±ng nhau\n",
    "- **\"Denoising autoencoders\"**: C√°c autoencoder kh·ª≠ nhi·ªÖu\n",
    "- **\"Predict a denoised variant\"**: D·ª± ƒëo√°n phi√™n b·∫£n ƒë√£ kh·ª≠ nhi·ªÖu\n",
    "- Th·ª±c t·∫ø: model d·ª± ƒëo√°n **nhi·ªÖu** ch·ª© kh√¥ng ph·∫£i ·∫£nh s·∫°ch tr·ª±c ti·∫øp\n",
    "\n",
    "> *\"The corresponding objective can be simplified to: LDM = Ex,Œµ~N(0,1),t [||Œµ - ŒµŒ∏(xt,t)||¬≤]\"*\n",
    "\n",
    "**Gi·∫£i th√≠ch c√¥ng th·ª©c**:\n",
    "- **E**: K·ª≥ v·ªçng (expected value)\n",
    "- **x**: ·∫¢nh t·ª´ dataset\n",
    "- **Œµ ~ N(0,1)**: Nhi·ªÖu Gaussian\n",
    "- **t**: Timestep uniform t·ª´ {1,...,T}\n",
    "- **||Œµ - ŒµŒ∏(xt,t)||¬≤**: L2 loss gi·ªØa nhi·ªÖu th·∫≠t v√† d·ª± ƒëo√°n\n",
    "\n",
    "### **T√≥m l·∫°i**:\n",
    "ƒêo·∫°n vƒÉn gi·∫£i th√≠ch r·∫±ng Diffusion Models:\n",
    "1. **H·ªçc ph√¢n ph·ªëi d·ªØ li·ªáu** b·∫±ng c√°ch kh·ª≠ nhi·ªÖu t·ª´ t·ª´\n",
    "2. **T∆∞∆°ng ƒë∆∞∆°ng** v·ªõi chu·ªói denoising autoencoders\n",
    "3. **Training ƒë∆°n gi·∫£n**: ch·ªâ c·∫ßn d·ª± ƒëo√°n nhi·ªÖu v·ªõi MSE loss\n",
    "4. **Hi·ªáu qu·∫£**: thay th·∫ø c√¥ng th·ª©c ph·ª©c t·∫°p b·∫±ng c√¥ng th·ª©c ƒë∆°n gi·∫£n\n",
    "\n",
    "ƒê√¢y ch√≠nh l√† **n·ªÅn t·∫£ng** cho Latent Diffusion Models - √°p d·ª•ng nguy√™n l√Ω n√†y trong latent space thay v√¨ pixel space!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2153b65e",
   "metadata": {},
   "source": [
    "## Hi·ªÉu theo c√°ch Vi·ªát Nam üáªüá≥\n",
    "\n",
    "### **V√≠ d·ª• ƒë∆°n gi·∫£n**:\n",
    "T∆∞·ªüng t∆∞·ª£ng b·∫°n ƒëang **v·∫Ω tranh**:\n",
    "\n",
    "1. **Forward process** (th√™m nhi·ªÖu):\n",
    "   - B·∫Øt ƒë·∫ßu: B·ª©c tranh ƒë·∫πp üé®\n",
    "   - B∆∞·ªõc 1: R·∫Øc m·ªôt √≠t b·ª•i l√™n tranh üå´Ô∏è\n",
    "   - B∆∞·ªõc 2: R·∫Øc th√™m b·ª•i üå´Ô∏èüå´Ô∏è\n",
    "   - ...\n",
    "   - Cu·ªëi c√πng: Ch·ªâ c√≤n to√†n b·ª•i tr·∫Øng ‚¨ú\n",
    "\n",
    "2. **Reverse process** (kh·ª≠ nhi·ªÖu):\n",
    "   - B·∫Øt ƒë·∫ßu: T·ªù gi·∫•y to√†n b·ª•i tr·∫Øng ‚¨ú\n",
    "   - Model h·ªçc: \"Nh√¨n t·ªù gi·∫•y n√†y, t√¥i ƒëo√°n c·∫ßn lau ƒëi nh·ªØng b·ª•i n√†o?\"\n",
    "   - T·ª´ t·ª´ lau s·∫°ch ‚Üí Xu·∫•t hi·ªán n√©t v·∫Ω ‚Üí D·∫ßn d·∫ßn th√†nh tranh ƒë·∫πp üé®\n",
    "\n",
    "### **T·∫°i sao g·ªçi l√† \"Equally weighted\"?**\n",
    "- Gi·ªëng nh∆∞ **h·ªçc t·ª´ng c·∫•p ƒë·ªô** trong tr∆∞·ªùng h·ªçc\n",
    "- L·ªõp 1, l·ªõp 2, ..., l·ªõp 12 ƒë·ªÅu **quan tr·ªçng nh∆∞ nhau**\n",
    "- Kh√¥ng ph·∫£i l·ªõp 12 quan tr·ªçng h∆°n l·ªõp 1\n",
    "- Diffusion model c≈©ng v·∫≠y: m·ªçi timestep ƒë·ªÅu c√≥ tr·ªçng s·ªë b·∫±ng nhau\n",
    "\n",
    "### **Denoising autoencoders**:\n",
    "- **Autoencoder**: M√°y n√©n v√† gi·∫£i n√©n\n",
    "- **Denoising**: Chuy√™n kh·ª≠ nhi·ªÖu\n",
    "- Gi·ªëng nh∆∞ c√≥ **1000 th·ª£ s·ª≠a tranh**, m·ªói th·ª£ chuy√™n s·ª≠a m·ªôt m·ª©c ƒë·ªô h·ªèng kh√°c nhau\n",
    "- Th·ª£ s·ªë 1: S·ª≠a tranh h·ªèng √≠t\n",
    "- Th·ª£ s·ªë 1000: S·ª≠a tranh h·ªèng nhi·ªÅu (g·∫ßn nh∆∞ to√†n b·ª•i)\n",
    "\n",
    "### **T·∫°i sao Diffusion th√†nh c√¥ng?**\n",
    "1. **Chia ƒë·ªÉ tr·ªã**: Thay v√¨ t·∫°o ·∫£nh m·ªôt l√∫t ‚Üí Chia th√†nh 1000 b∆∞·ªõc nh·ªè\n",
    "2. **·ªîn ƒë·ªãnh**: Kh√¥ng b·ªã \"ƒëi√™n\" nh∆∞ GAN\n",
    "3. **Linh ho·∫°t**: C√≥ th·ªÉ ƒëi·ªÅu khi·ªÉn b·∫±ng text\n",
    "4. **Ch·∫•t l∆∞·ª£ng cao**: T·∫°o ·∫£nh realistic\n",
    "\n",
    "### **K·∫øt n·ªëi v·ªõi Stable Diffusion**:\n",
    "- **Stable Diffusion** = Diffusion Models + VAE + Text Conditioning\n",
    "- Thay v√¨ l√†m tr√™n ·∫£nh 512√ó512 ‚Üí L√†m tr√™n latent 64√ó64 (nhanh h∆°n 64 l·∫ßn!)\n",
    "- K·∫øt qu·∫£: T·∫°o ·∫£nh ch·∫•t l∆∞·ª£ng cao, nhanh, v√† c√≥ th·ªÉ ƒëi·ªÅu khi·ªÉn b·∫±ng text\n",
    "\n",
    "**üéØ M·ª•c ti√™u cu·ªëi c√πng**: T·ª´ c√¢u text \"m·ªôt con m√®o ƒëang ng·ªìi tr√™n gh·∫ø\" ‚Üí T·∫°o ra ·∫£nh m√®o ƒë·∫πp v√† ƒë√∫ng m√¥ t·∫£!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306f7be1",
   "metadata": {},
   "source": [
    "# Stable Diffusion Model Architecture & Training Pipeline üèóÔ∏è\n",
    "\n",
    "## T·ªïng quan Architecture\n",
    "\n",
    "**Stable Diffusion** kh√¥ng ph·∫£i l√† m·ªôt model ƒë∆°n l·∫ª, m√† l√† **h·ªá th·ªëng g·ªìm 3 components ch√≠nh**:\n",
    "\n",
    "### 1. **First Stage Model (VAE)**:\n",
    "- **Encoder**: E(x) ‚Üí z (·∫£nh ‚Üí latent)\n",
    "- **Decoder**: D(z) ‚Üí x (latent ‚Üí ·∫£nh)\n",
    "- **M·ª•c ƒë√≠ch**: N√©n ·∫£nh t·ª´ 512√ó512 ‚Üí latent 64√ó64 (gi·∫£m 64x)\n",
    "\n",
    "### 2. **Diffusion Model (U-Net)**:\n",
    "- **Input**: Noisy latent zt, timestep t, conditioning c\n",
    "- **Output**: Predicted noise ŒµŒ∏(zt, t, c)\n",
    "- **M·ª•c ƒë√≠ch**: H·ªçc kh·ª≠ nhi·ªÖu trong latent space\n",
    "\n",
    "### 3. **Conditioning Encoder**:\n",
    "- **Text Encoder**: CLIP ho·∫∑c T5 (text ‚Üí embedding)\n",
    "- **Cross-attention**: Inject text v√†o U-Net\n",
    "- **M·ª•c ƒë√≠ch**: ƒêi·ªÅu khi·ªÉn generation b·∫±ng text\n",
    "\n",
    "## Ki·∫øn tr√∫c t·ªïng th·ªÉ:\n",
    "```\n",
    "Text Prompt ‚Üí [CLIP] ‚Üí Text Embedding\n",
    "                            ‚Üì\n",
    "Noise ‚Üí [U-Net + Cross-Attention] ‚Üí Clean Latent ‚Üí [VAE Decoder] ‚Üí Final Image\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaae738",
   "metadata": {},
   "source": [
    "# 3 Giai ƒëo·∫°n Training c·ªßa Stable Diffusion üéØ\n",
    "\n",
    "## Giai ƒëo·∫°n 1: Pre-training VAE (Autoencoder)\n",
    "\n",
    "### **M·ª•c ti√™u**: T·∫°o ra m·ªôt VAE ch·∫•t l∆∞·ª£ng cao ƒë·ªÉ n√©n ·∫£nh\n",
    "\n",
    "### **Training Process**:\n",
    "```python\n",
    "# VAE Loss Function\n",
    "total_loss = reconstruction_loss + Œ≤ * kl_loss + Œª * perceptual_loss + adversarial_loss\n",
    "```\n",
    "\n",
    "### **Components**:\n",
    "1. **Reconstruction Loss**: L2 loss gi·ªØa input v√† reconstructed image\n",
    "2. **KL Divergence**: Regularize latent space\n",
    "3. **Perceptual Loss**: VGG-based features ƒë·ªÉ b·∫£o to√†n visual quality\n",
    "4. **Adversarial Loss**: GAN loss ƒë·ªÉ t·∫°o ·∫£nh realistic\n",
    "\n",
    "### **Dataset**: \n",
    "- LAION-400M (400 tri·ªáu ·∫£nh-text pairs)\n",
    "- ImageNet\n",
    "- Other large-scale image datasets\n",
    "\n",
    "### **Result**: \n",
    "- VAE c√≥ th·ªÉ encode ·∫£nh 512√ó512 ‚Üí latent 64√ó64\n",
    "- Decode latent ‚Üí ·∫£nh ch·∫•t l∆∞·ª£ng cao\n",
    "- Compression ratio: 8√ó8√ó3 = 192x (th·ª±c t·∫ø ~64x do latent channels)\n",
    "\n",
    "---\n",
    "\n",
    "## Giai ƒëo·∫°n 2: Training Diffusion Model trong Latent Space\n",
    "\n",
    "### **M·ª•c ti√™u**: H·ªçc diffusion process trong latent space c·ªßa VAE\n",
    "\n",
    "### **Training Process**:\n",
    "```python\n",
    "# Latent Diffusion Loss\n",
    "LLDM = Ez~E(x),Œµ~N(0,1),t [||Œµ - ŒµŒ∏(zt, t)||¬≤]\n",
    "```\n",
    "\n",
    "### **Steps**:\n",
    "1. **Encode images**: x ‚Üí z = E(x) b·∫±ng pre-trained VAE\n",
    "2. **Add noise**: zt = ‚àö(·æ±t) * z + ‚àö(1-·æ±t) * Œµ  \n",
    "3. **Train U-Net**: D·ª± ƒëo√°n noise ŒµŒ∏(zt, t)\n",
    "4. **Backprop**: Minimize MSE loss\n",
    "\n",
    "### **U-Net Architecture**:\n",
    "- **Input**: Noisy latent zt [B, 4, 64, 64]\n",
    "- **Time embedding**: Sinusoidal encoding c·ªßa timestep t\n",
    "- **Skip connections**: Encoder-decoder v·ªõi residual connections\n",
    "- **Attention**: Self-attention ·ªü multiple resolutions\n",
    "\n",
    "### **Training Details**:\n",
    "- **Timesteps**: T = 1000\n",
    "- **Noise schedule**: Linear ho·∫∑c cosine\n",
    "- **Batch size**: Large (depends on hardware)\n",
    "- **Learning rate**: 1e-4 v·ªõi cosine annealing\n",
    "\n",
    "---\n",
    "\n",
    "## Giai ƒëo·∫°n 3: Adding Conditioning (Text-to-Image)\n",
    "\n",
    "### **M·ª•c ti√™u**: Th√™m kh·∫£ nƒÉng ƒëi·ªÅu khi·ªÉn generation b·∫±ng text\n",
    "\n",
    "### **Architecture Changes**:\n",
    "```python\n",
    "# Conditioned Diffusion Loss  \n",
    "LLDM = Ez~E(x),c,Œµ~N(0,1),t [||Œµ - ŒµŒ∏(zt, t, c)||¬≤]\n",
    "```\n",
    "\n",
    "### **Text Conditioning Process**:\n",
    "1. **Text Encoding**: \n",
    "   - Input: \"A cat sitting on a chair\"\n",
    "   - CLIP Text Encoder ‚Üí text embeddings [77, 768]\n",
    "\n",
    "2. **Cross-Attention trong U-Net**:\n",
    "   ```python\n",
    "   # Trong m·ªói U-Net block\n",
    "   x = self_attention(x)  # spatial attention\n",
    "   x = cross_attention(x, text_embeddings)  # text conditioning\n",
    "   ```\n",
    "\n",
    "3. **Classifier-Free Guidance**:\n",
    "   ```python\n",
    "   # Training: 50% conditional, 50% unconditional\n",
    "   if random.random() < 0.5:\n",
    "       condition = text_embedding\n",
    "   else:\n",
    "       condition = null_embedding  # h·ªçc unconditional generation\n",
    "   \n",
    "   # Inference: Guidance scale\n",
    "   Œµ_pred = Œµ_uncond + guidance_scale * (Œµ_cond - Œµ_uncond)\n",
    "   ```\n",
    "\n",
    "### **Training Strategy**:\n",
    "- **Mixed training**: 50% v·ªõi text, 50% kh√¥ng c√≥ text\n",
    "- **Null text**: \"\" (empty string) cho unconditional\n",
    "- **Text dropout**: Randomly mask text ƒë·ªÉ h·ªçc robust features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df2a1a2",
   "metadata": {},
   "source": [
    "# Mapping t·ª´ Paper ƒë·∫øn Code Implementation üìÅ\n",
    "\n",
    "## VAE Components trong Code\n",
    "\n",
    "### **Files li√™n quan**:\n",
    "- `ldm/models/autoencoder.py`: Main VAE implementation\n",
    "- `ldm/modules/diffusionmodules/model.py`: Encoder/Decoder architecture\n",
    "- `configs/autoencoder/`: VAE configurations\n",
    "\n",
    "### **Key Classes**:\n",
    "```python\n",
    "# VAE ch√≠nh\n",
    "class AutoencoderKL(nn.Module):\n",
    "    def __init__(self, ddconfig, embed_dim, ckpt_path=None):\n",
    "        self.encoder = Encoder(**ddconfig)\n",
    "        self.decoder = Decoder(**ddconfig) \n",
    "        self.quant_conv = nn.Conv2d(ddconfig[\"z_channels\"], embed_dim, 1)\n",
    "        self.post_quant_conv = nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        moments = self.quant_conv(h)\n",
    "        posterior = DiagonalGaussianDistribution(moments)\n",
    "        return posterior\n",
    "    \n",
    "    def decode(self, z):\n",
    "        z = self.post_quant_conv(z)\n",
    "        dec = self.decoder(z)\n",
    "        return dec\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## U-Net Diffusion Model\n",
    "\n",
    "### **Files li√™n quan**:\n",
    "- `ldm/models/diffusion/ddpm.py`: Main diffusion class\n",
    "- `ldm/modules/diffusionmodules/openaimodel.py`: U-Net implementation\n",
    "- `ldm/modules/attention.py`: Attention mechanisms\n",
    "\n",
    "### **Key Classes**:\n",
    "```python\n",
    "# Main Diffusion Model\n",
    "class LatentDiffusion(DDPM):\n",
    "    def __init__(self, first_stage_config, cond_stage_config, unet_config, ...):\n",
    "        # Load pre-trained VAE\n",
    "        self.instantiate_first_stage(first_stage_config)\n",
    "        \n",
    "        # Load conditioning model (CLIP)\n",
    "        self.instantiate_cond_stage(cond_stage_config) \n",
    "        \n",
    "        # Initialize U-Net\n",
    "        self.model = DiffusionWrapper(unet_config)\n",
    "    \n",
    "    def apply_model(self, x_noisy, t, cond):\n",
    "        # U-Net forward pass v·ªõi conditioning\n",
    "        return self.model(x_noisy, t, cond)\n",
    "```\n",
    "\n",
    "### **U-Net Architecture**:\n",
    "```python\n",
    "class UNetModel(nn.Module):\n",
    "    def __init__(self, in_channels, model_channels, out_channels, \n",
    "                 attention_resolutions, channel_mult, ...):\n",
    "        # Time embedding\n",
    "        self.time_embed = nn.Sequential(...)\n",
    "        \n",
    "        # Encoder blocks\n",
    "        self.input_blocks = nn.ModuleList([...])\n",
    "        \n",
    "        # Middle block\n",
    "        self.middle_block = TimestepEmbedSequential(...)\n",
    "        \n",
    "        # Decoder blocks v·ªõi skip connections\n",
    "        self.output_blocks = nn.ModuleList([...])\n",
    "        \n",
    "        # Cross-attention ƒë·ªÉ inject text conditioning\n",
    "        self.transformer_blocks = nn.ModuleList([...])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Text Conditioning (CLIP)\n",
    "\n",
    "### **Files li√™n quan**:\n",
    "- `ldm/modules/encoders/modules.py`: Text encoders\n",
    "- `ldm/modules/attention.py`: Cross-attention implementation\n",
    "\n",
    "### **CLIP Text Encoder**:\n",
    "```python\n",
    "class FrozenCLIPEmbedder(nn.Module):\n",
    "    def __init__(self, version=\"openai/clip-vit-base-patch32\"):\n",
    "        self.transformer = CLIPTextModel.from_pretrained(version)\n",
    "        self.transformer.eval()\n",
    "        \n",
    "        # Freeze CLIP weights\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, text):\n",
    "        tokens = self.tokenizer(text, truncation=True, max_length=77, \n",
    "                               return_tensors=\"pt\", padding=\"max_length\")\n",
    "        outputs = self.transformer(**tokens)\n",
    "        return outputs.last_hidden_state\n",
    "```\n",
    "\n",
    "### **Cross-Attention Implementation**:\n",
    "```python\n",
    "class CrossAttention(nn.Module):\n",
    "    def forward(self, x, context=None):\n",
    "        h = x\n",
    "        q = self.to_q(h)  # query t·ª´ spatial features\n",
    "        \n",
    "        if context is None:\n",
    "            context = h  # self-attention\n",
    "        \n",
    "        k = self.to_k(context)  # key t·ª´ text embeddings\n",
    "        v = self.to_v(context)  # value t·ª´ text embeddings\n",
    "        \n",
    "        # Attention computation\n",
    "        sim = torch.einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "        attn = sim.softmax(dim=-1)\n",
    "        out = torch.einsum('b i j, b j d -> b i d', attn, v)\n",
    "        \n",
    "        return self.to_out(out)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07f6992",
   "metadata": {},
   "source": [
    "# CLIP: Hi·ªÉu S√¢u v·ªÅ Text-Image Understanding üîó\n",
    "\n",
    "## CLIP l√† g√¨?\n",
    "\n",
    "**CLIP** (Contrastive Language-Image Pre-training) l√† m·ªôt m√¥ h√¨nh AI ƒë∆∞·ª£c OpenAI ph√°t tri·ªÉn nƒÉm 2021, c√≥ kh·∫£ nƒÉng **hi·ªÉu m·ªëi li√™n h·ªá gi·ªØa text v√† image**.\n",
    "\n",
    "### üéØ **M·ª•c ti√™u c·ªßa CLIP**:\n",
    "- H·ªçc ƒë∆∞·ª£c **shared embedding space** cho c·∫£ text v√† image\n",
    "- Text v√† image c√≥ **same meaning** s·∫Ω c√≥ embeddings **g·∫ßn nhau**\n",
    "- Text v√† image **kh√°c meaning** s·∫Ω c√≥ embeddings **xa nhau**\n",
    "\n",
    "### üß† **T·∫°i sao CLIP quan tr·ªçng?**\n",
    "\n",
    "Tr∆∞·ªõc CLIP, c√°c AI model th∆∞·ªùng:\n",
    "- **Ch·ªâ hi·ªÉu text** (GPT, BERT) HO·∫∂C **ch·ªâ hi·ªÉu image** (ResNet, EfficientNet)\n",
    "- **Kh√¥ng th·ªÉ** k·∫øt n·ªëi √Ω nghƒ©a gi·ªØa text v√† image\n",
    "- **C·∫ßn labeled data** cho m·ªói task c·ª• th·ªÉ\n",
    "\n",
    "CLIP c√≥ th·ªÉ:\n",
    "- **Hi·ªÉu c·∫£ text v√† image** c√πng m·ªôt l√∫c\n",
    "- **Zero-shot classification**: Ph√¢n lo·∫°i image ch·ªâ b·∫±ng text description\n",
    "- **Semantic similarity**: T√¨m image ph√π h·ª£p v·ªõi text prompt\n",
    "- **Flexible**: Kh√¥ng c·∫ßn training l·∫°i cho new tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ac7b26",
   "metadata": {},
   "source": [
    "## Ki·∫øn tr√∫c c·ªßa CLIP üèóÔ∏è\n",
    "\n",
    "CLIP g·ªìm **2 encoders ch√≠nh**:\n",
    "\n",
    "### 1. **Text Encoder**:\n",
    "- **Input**: Text string (VD: \"A cat sitting on a chair\")\n",
    "- **Tokenization**: Chuy·ªÉn text th√†nh tokens (words/subwords)\n",
    "- **Architecture**: Transformer (gi·ªëng BERT/GPT)\n",
    "- **Output**: Text embedding vector [512 dim]\n",
    "\n",
    "### 2. **Image Encoder**: \n",
    "- **Input**: Image (VD: ·∫£nh con m√®o)\n",
    "- **Architecture**: Vision Transformer (ViT) ho·∫∑c ResNet\n",
    "- **Output**: Image embedding vector [512 dim]\n",
    "\n",
    "### 3. **Shared Embedding Space**:\n",
    "- C·∫£ text v√† image ƒë·ªÅu ƒë∆∞·ª£c map v√†o **c√πng m·ªôt kh√¥ng gian 512-dim**\n",
    "- **Cosine similarity** ƒë∆∞·ª£c d√πng ƒë·ªÉ ƒëo ƒë·ªô t∆∞∆°ng ƒë·ªìng\n",
    "- **Contrastive learning** ƒë·ªÉ h·ªçc embeddings\n",
    "\n",
    "```\n",
    "Text: \"A cat\"     ‚Üí  [Text Encoder]  ‚Üí  [0.2, -0.1, 0.8, ...] (512 dims)\n",
    "Image: üê±         ‚Üí  [Image Encoder] ‚Üí  [0.3, -0.2, 0.7, ...] (512 dims)\n",
    "                                         ‚Üì\n",
    "                                   Cosine Similarity = 0.85 (high!)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c37d490",
   "metadata": {},
   "source": [
    "## CLIP ƒë∆∞·ª£c Training nh∆∞ th·∫ø n√†o? üìö\n",
    "\n",
    "### **Dataset kh·ªïng l·ªì**:\n",
    "- **400 million** text-image pairs t·ª´ internet\n",
    "- **Diverse**: M·ªçi ch·ªß ƒë·ªÅ, ng√¥n ng·ªØ, style\n",
    "- **Noisy**: Kh√¥ng c·∫ßn clean labeling (t·ª± ƒë·ªông crawl)\n",
    "\n",
    "### **Contrastive Learning Process**:\n",
    "\n",
    "**√ù t∆∞·ªüng**: Trong m·ªôt batch, m·ªói image ch·ªâ match v·ªõi ƒë√∫ng 1 text c·ªßa n√≥.\n",
    "\n",
    "```python\n",
    "# Batch example:\n",
    "Batch = [\n",
    "    (image1, \"A red car\"),        # Correct pair\n",
    "    (image2, \"A blue house\"),     # Correct pair  \n",
    "    (image3, \"A green tree\"),     # Correct pair\n",
    "    (image4, \"A yellow flower\")   # Correct pair\n",
    "]\n",
    "\n",
    "# CLIP learns:\n",
    "# image1 should be SIMILAR to \"A red car\"\n",
    "# image1 should be DIFFERENT from \"A blue house\", \"A green tree\", \"A yellow flower\"\n",
    "```\n",
    "\n",
    "### **Loss Function**:\n",
    "\n",
    "```python\n",
    "# Simplified CLIP loss\n",
    "def clip_loss(image_embeddings, text_embeddings):\n",
    "    # Compute similarity matrix\n",
    "    logits = image_embeddings @ text_embeddings.T  # [batch_size, batch_size]\n",
    "    \n",
    "    # Diagonal elements should be high (correct pairs)\n",
    "    # Off-diagonal should be low (incorrect pairs)\n",
    "    \n",
    "    # Cross-entropy loss on both directions\n",
    "    labels = torch.arange(batch_size)  # [0, 1, 2, 3, ...]\n",
    "    \n",
    "    loss_i2t = cross_entropy(logits, labels)      # Image to Text\n",
    "    loss_t2i = cross_entropy(logits.T, labels)    # Text to Image\n",
    "    \n",
    "    return (loss_i2t + loss_t2i) / 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fe724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP Capabilities Demo üé≠\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Gi·∫£ l·∫≠p CLIP embeddings (th·ª±c t·∫ø s·∫Ω d√πng transformers library)\n",
    "print(\"üîç CLIP CAPABILITIES DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Zero-shot Image Classification\n",
    "print(\"\\n1Ô∏è‚É£ ZERO-SHOT CLASSIFICATION:\")\n",
    "print(\"C√≥ th·ªÉ classify image m√† kh√¥ng c·∫ßn training!\")\n",
    "\n",
    "# Gi·∫£ s·ª≠ c√≥ 1 image embedding\n",
    "image_embedding = torch.tensor([0.2, -0.1, 0.8, 0.3])  # 4D for demo\n",
    "\n",
    "# C√°c class descriptions\n",
    "class_texts = [\n",
    "    \"A photo of a cat\",\n",
    "    \"A photo of a dog\", \n",
    "    \"A photo of a car\",\n",
    "    \"A photo of a tree\"\n",
    "]\n",
    "\n",
    "# Gi·∫£ l·∫≠p text embeddings\n",
    "text_embeddings = torch.tensor([\n",
    "    [0.3, -0.2, 0.7, 0.4],  # cat\n",
    "    [0.1, 0.5, -0.3, 0.2],  # dog\n",
    "    [-0.4, 0.1, 0.2, -0.1], # car\n",
    "    [0.6, -0.4, 0.1, 0.8]   # tree\n",
    "])\n",
    "\n",
    "# Compute similarities\n",
    "similarities = F.cosine_similarity(image_embedding.unsqueeze(0), text_embeddings)\n",
    "print(f\"Image similarities v·ªõi classes:\")\n",
    "for i, (text, sim) in enumerate(zip(class_texts, similarities)):\n",
    "    print(f\"   {text:20s}: {sim:.3f}\")\n",
    "\n",
    "best_match = torch.argmax(similarities)\n",
    "print(f\"\\nüéØ Prediction: {class_texts[best_match]} (confidence: {similarities[best_match]:.3f})\")\n",
    "\n",
    "# 2. Text-to-Image Search\n",
    "print(\"\\n2Ô∏è‚É£ TEXT-TO-IMAGE SEARCH:\")\n",
    "print(\"T√¨m image ph√π h·ª£p nh·∫•t v·ªõi text query\")\n",
    "\n",
    "# Query text\n",
    "query = \"A cute animal\"\n",
    "query_embedding = torch.tensor([0.25, -0.15, 0.75, 0.35])  # Similar to cat\n",
    "\n",
    "# Database of images\n",
    "image_descriptions = [\n",
    "    \"Cat sleeping on sofa\",\n",
    "    \"Dog playing in park\", \n",
    "    \"Sports car racing\",\n",
    "    \"Mountain landscape\"\n",
    "]\n",
    "\n",
    "image_embeddings_db = torch.tensor([\n",
    "    [0.3, -0.2, 0.7, 0.4],   # cat (should match well)\n",
    "    [0.1, 0.5, -0.3, 0.2],   # dog (should match okay)\n",
    "    [-0.4, 0.1, 0.2, -0.1],  # car (should not match)\n",
    "    [0.6, -0.4, 0.1, 0.8]    # landscape (should not match)\n",
    "])\n",
    "\n",
    "search_similarities = F.cosine_similarity(query_embedding.unsqueeze(0), image_embeddings_db)\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"Search results:\")\n",
    "\n",
    "# Sort by similarity\n",
    "sorted_indices = torch.argsort(search_similarities, descending=True)\n",
    "for rank, idx in enumerate(sorted_indices, 1):\n",
    "    print(f\"   {rank}. {image_descriptions[idx]:20s}: {search_similarities[idx]:.3f}\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ SEMANTIC UNDERSTANDING:\")\n",
    "print(\"CLIP hi·ªÉu meaning, kh√¥ng ch·ªâ keywords!\")\n",
    "\n",
    "semantics_examples = [\n",
    "    (\"A person riding a bicycle\", \"Cycling activity\", 0.92),\n",
    "    (\"Sunset over ocean\", \"Beautiful evening seascape\", 0.88),\n",
    "    (\"Pizza with pepperoni\", \"Italian food dish\", 0.85),\n",
    "    (\"Code on computer screen\", \"Programming work\", 0.91)\n",
    "]\n",
    "\n",
    "print(\"Examples of semantic similarity:\")\n",
    "for text1, text2, similarity in semantics_examples:\n",
    "    print(f\"   '{text1}' ‚Üî '{text2}': {similarity}\")\n",
    "\n",
    "print(\"\\n‚ú® KEY INSIGHTS:\")\n",
    "print(\"‚Ä¢ CLIP kh√¥ng ch·ªâ match keywords, m√† hi·ªÉu meaning\")\n",
    "print(\"‚Ä¢ Zero-shot learning: kh√¥ng c·∫ßn training cho new tasks\")\n",
    "print(\"‚Ä¢ Flexible: c√≥ th·ªÉ d√πng cho classification, search, generation\")\n",
    "print(\"‚Ä¢ Foundation model cho nhi·ªÅu multimodal applications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18427622",
   "metadata": {},
   "source": [
    "## CLIP trong Stable Diffusion üé®\n",
    "\n",
    "### **Vai tr√≤ c·ªßa CLIP trong Stable Diffusion**:\n",
    "\n",
    "1. **Text Understanding**: \n",
    "   - Input: User prompt \"A beautiful sunset over mountains\"\n",
    "   - CLIP Text Encoder: Chuy·ªÉn th√†nh embedding [77, 768]\n",
    "   - Output: Rich semantic representation c·ªßa text\n",
    "\n",
    "2. **Conditioning Signal**:\n",
    "   - CLIP embeddings ƒë∆∞·ª£c inject v√†o U-Net qua **Cross-Attention**\n",
    "   - M·ªói spatial location trong U-Net c√≥ th·ªÉ \"attend\" to relevant parts c·ªßa text\n",
    "   - ƒêi·ªÅu n√†y gi√∫p U-Net bi·∫øt **t·∫°o g√¨** v√† **t·∫°o ·ªü ƒë√¢u**\n",
    "\n",
    "3. **Why CLIP specifically?**:\n",
    "   - **Pre-trained**: ƒê√£ h·ªçc t·ª´ 400M image-text pairs\n",
    "   - **Rich representations**: Hi·ªÉu complex semantic concepts\n",
    "   - **Frozen**: Kh√¥ng c·∫ßn training l·∫°i (save compute)\n",
    "   - **Proven**: ƒê√£ ƒë∆∞·ª£c validate tr√™n nhi·ªÅu tasks\n",
    "\n",
    "### **Architecture Integration**:\n",
    "\n",
    "```\n",
    "User Prompt: \"A cat wearing a wizard hat\"\n",
    "       ‚Üì\n",
    "[CLIP Text Encoder] ‚Üí Text Embeddings [77, 768]\n",
    "       ‚Üì\n",
    "[Cross-Attention trong U-Net]\n",
    "       ‚Üì  \n",
    "Spatial Features + Text Features ‚Üí Enhanced Features\n",
    "       ‚Üì\n",
    "Generated Image: üê±üßô‚Äç‚ôÇÔ∏è\n",
    "```\n",
    "\n",
    "### **T·∫°i sao kh√¥ng d√πng text encoder kh√°c?**\n",
    "\n",
    "| Model | Pros | Cons | Use in SD?\n",
    "|-------|------|------|----------|\n",
    "| **CLIP** | ‚Ä¢ Multimodal<br>‚Ä¢ Rich semantics<br>‚Ä¢ Proven quality | ‚Ä¢ Limited context (77 tokens) | ‚úÖ SD 1.x |\n",
    "| **T5** | ‚Ä¢ Longer context<br>‚Ä¢ Pure text model | ‚Ä¢ Larger size<br>‚Ä¢ No image understanding | ‚úÖ SD 2.x |\n",
    "| **BERT** | ‚Ä¢ Good text understanding | ‚Ä¢ No image connection<br>‚Ä¢ Less suitable | ‚ùå |\n",
    "| **GPT** | ‚Ä¢ Creative text | ‚Ä¢ Autoregressive<br>‚Ä¢ Overkill | ‚ùå |\n",
    "\n",
    "### **CLIP vs T5 trong Stable Diffusion**:\n",
    "\n",
    "**CLIP** (SD 1.x):\n",
    "- Compact: 123M parameters\n",
    "- Fast inference\n",
    "- Good image-text alignment\n",
    "- Limited to 77 tokens\n",
    "\n",
    "**T5** (SD 2.x):\n",
    "- Larger: 220M - 11B parameters  \n",
    "- Better long text understanding\n",
    "- Slower inference\n",
    "- Can handle complex prompts\n",
    "\n",
    "### **Practical Impact**:\n",
    "\n",
    "```python\n",
    "# CLIP gi√∫p Stable Diffusion hi·ªÉu:\n",
    "\"A majestic lion\"           ‚Üí Generates powerful, regal lion\n",
    "\"A cute kitten\"             ‚Üí Generates small, adorable cat\n",
    "\"Lion in cartoon style\"     ‚Üí Understands both subject + style\n",
    "\"Photorealistic lion\"       ‚Üí Understands realism requirement\n",
    "```\n",
    "\n",
    "**Without CLIP**: Stable Diffusion s·∫Ω kh√¥ng th·ªÉ hi·ªÉu text prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3ec999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical CLIP Implementation for Stable Diffusion üíª\n",
    "\n",
    "print(\"üîß CLIP IMPLEMENTATION IN STABLE DIFFUSION\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Simulated CLIP Text Encoder (based on real implementation)\n",
    "class CLIPTextEncoder:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 49408\n",
    "        self.max_length = 77  # CLIP's context length\n",
    "        self.embed_dim = 768  # Text embedding dimension\n",
    "        print(f\"üìù CLIP Text Encoder initialized:\")\n",
    "        print(f\"   ‚Ä¢ Vocabulary size: {self.vocab_size:,}\")\n",
    "        print(f\"   ‚Ä¢ Max sequence length: {self.max_length}\")\n",
    "        print(f\"   ‚Ä¢ Embedding dimension: {self.embed_dim}\")\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Simulate tokenization process\"\"\"\n",
    "        # Real implementation uses BPE tokenizer\n",
    "        words = text.lower().split()\n",
    "        tokens = [49406]  # <start_of_text> token\n",
    "        \n",
    "        for word in words[:75]:  # Leave space for start/end tokens\n",
    "            # Simulate token IDs (real implementation uses BPE)\n",
    "            token_id = hash(word) % (self.vocab_size - 2) + 1\n",
    "            tokens.append(token_id)\n",
    "        \n",
    "        tokens.append(49407)  # <end_of_text> token\n",
    "        \n",
    "        # Pad to max_length\n",
    "        while len(tokens) < self.max_length:\n",
    "            tokens.append(0)  # <pad> token\n",
    "            \n",
    "        return tokens[:self.max_length]\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Convert text to embeddings\"\"\"\n",
    "        tokens = self.tokenize(text)\n",
    "        print(f\"\\nüî§ Text processing:\")\n",
    "        print(f\"   Input: '{text}'\")\n",
    "        print(f\"   Tokens: {len([t for t in tokens if t != 0])} real tokens\")\n",
    "        print(f\"   Padded to: {len(tokens)} tokens\")\n",
    "        \n",
    "        # Simulate embeddings (real implementation uses transformer)\n",
    "        import torch\n",
    "        embeddings = torch.randn(self.max_length, self.embed_dim)\n",
    "        \n",
    "        print(f\"   Output shape: {list(embeddings.shape)}\")\n",
    "        return embeddings\n",
    "\n",
    "# Demo CLIP usage\n",
    "clip_encoder = CLIPTextEncoder()\n",
    "\n",
    "# Test various prompts\n",
    "test_prompts = [\n",
    "    \"A beautiful sunset over mountains\",\n",
    "    \"A cat wearing a wizard hat in a magical forest\", \n",
    "    \"Photorealistic portrait of a woman with blue eyes\",\n",
    "    \"Abstract painting in the style of Van Gogh\"\n",
    "]\n",
    "\n",
    "print(\"\\nüé® PROCESSING VARIOUS PROMPTS:\")\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n--- Example {i} ---\")\n",
    "    embeddings = clip_encoder.encode(prompt)\n",
    "    \n",
    "    # Simulate using embeddings in U-Net\n",
    "    print(f\"   ‚úÖ Ready for Cross-Attention in U-Net\")\n",
    "    print(f\"   ‚úÖ Will guide image generation process\")\n",
    "\n",
    "print(\"\\nüß† HOW CLIP EMBEDDINGS GUIDE GENERATION:\")\n",
    "print(\"\"\"\n",
    "1. **Rich Semantics**: \n",
    "   - \"beautiful\" ‚Üí aesthetic qualities\n",
    "   - \"sunset\" ‚Üí lighting, colors, time of day\n",
    "   - \"mountains\" ‚Üí landscape, composition\n",
    "\n",
    "2. **Style Understanding**:\n",
    "   - \"photorealistic\" ‚Üí detailed, camera-like\n",
    "   - \"abstract\" ‚Üí non-representational\n",
    "   - \"Van Gogh style\" ‚Üí brushstrokes, colors\n",
    "\n",
    "3. **Compositional Hints**:\n",
    "   - \"portrait\" ‚Üí close-up, centered\n",
    "   - \"landscape\" ‚Üí wide view, horizon\n",
    "   - \"in a forest\" ‚Üí background elements\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüéØ KEY TECHNICAL DETAILS:\")\n",
    "print(\"‚Ä¢ CLIP embeddings shape: [77, 768]\")\n",
    "print(\"‚Ä¢ Each token gets 768-dimensional representation\")\n",
    "print(\"‚Ä¢ Cross-attention uses these as Keys & Values\")\n",
    "print(\"‚Ä¢ Spatial features from U-Net become Queries\")\n",
    "print(\"‚Ä¢ This allows each pixel to 'look at' relevant text parts\")\n",
    "\n",
    "print(\"\\n‚ú® CLIP makes text-to-image generation possible!\")\n",
    "print(\"Without CLIP, Stable Diffusion would be just noise ‚Üí noise üå™Ô∏è\")\n",
    "print(\"With CLIP, it becomes meaningful: text ‚Üí beautiful images üé®\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70b155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROADMAP: D·ª±ng l·∫°i Stable Diffusion t·ª´ ƒë·∫ßu üõ†Ô∏è\n",
    "\n",
    "print(\"=== B∆Ø·ªöC 1: CHU·∫®N B·ªä DATASET V√Ä INFRASTRUCTURE ===\")\n",
    "print(\"\"\"\n",
    "1.1. Dataset Preparation:\n",
    "   ‚Ä¢ Text-Image pairs: LAION-400M, CC12M, ho·∫∑c custom dataset\n",
    "   ‚Ä¢ Image preprocessing: Resize to 512x512, normalize [-1, 1]\n",
    "   ‚Ä¢ Text preprocessing: Tokenization, max length 77\n",
    "\n",
    "1.2. Infrastructure:\n",
    "   ‚Ä¢ Multi-GPU setup (8x A100 recommended)\n",
    "   ‚Ä¢ Distributed training framework (PyTorch Lightning)\n",
    "   ‚Ä¢ Wandb/TensorBoard cho monitoring\n",
    "   ‚Ä¢ Large storage for datasets (TB scale)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n=== B∆Ø·ªöC 2: IMPLEMENT VAE (First Stage Model) ===\")\n",
    "print(\"\"\"\n",
    "2.1. VAE Architecture:\n",
    "   ‚Ä¢ Encoder: ResNet-based v·ªõi downsampling blocks\n",
    "   ‚Ä¢ Decoder: Symmetric upsampling blocks\n",
    "   ‚Ä¢ Latent space: 4 channels, 64x64 (cho 512x512 input)\n",
    "   ‚Ä¢ KL regularization\n",
    "\n",
    "2.2. Training VAE:\n",
    "   ‚Ä¢ Loss: Reconstruction + Œ≤*KL + Œª*Perceptual + Adversarial\n",
    "   ‚Ä¢ Perceptual loss: VGG16 features\n",
    "   ‚Ä¢ Discriminator: PatchGAN for adversarial loss\n",
    "   ‚Ä¢ Training time: ~1 tu·∫ßn v·ªõi 8 GPUs\n",
    "\n",
    "2.3. VAE Validation:\n",
    "   ‚Ä¢ Reconstruction quality: LPIPS, SSIM, FID\n",
    "   ‚Ä¢ Compression efficiency: File size reduction\n",
    "   ‚Ä¢ Latent space interpolation\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n=== B∆Ø·ªöC 3: IMPLEMENT U-NET DIFFUSION MODEL ===\") \n",
    "print(\"\"\"\n",
    "3.1. U-Net Architecture:\n",
    "   ‚Ä¢ Input: 4-channel latent + time embedding\n",
    "   ‚Ä¢ Encoder-Decoder v·ªõi skip connections\n",
    "   ‚Ä¢ Multi-scale attention layers\n",
    "   ‚Ä¢ Group normalization\n",
    "   ‚Ä¢ SiLU activation\n",
    "\n",
    "3.2. Diffusion Components:\n",
    "   ‚Ä¢ Noise scheduler: Linear or cosine Œ≤ schedule\n",
    "   ‚Ä¢ Timestep embedding: Sinusoidal positional encoding\n",
    "   ‚Ä¢ Loss function: Simple MSE loss\n",
    "   ‚Ä¢ Sampling: DDPM or DDIM\n",
    "\n",
    "3.3. Training Process:\n",
    "   ‚Ä¢ Encode images v·ªõi pre-trained VAE\n",
    "   ‚Ä¢ Random timestep sampling\n",
    "   ‚Ä¢ Noise prediction training\n",
    "   ‚Ä¢ Training time: ~2-3 tu·∫ßn v·ªõi 8 GPUs\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n=== B∆Ø·ªöC 4: ADD TEXT CONDITIONING ===\")\n",
    "print(\"\"\"\n",
    "4.1. Text Encoder:\n",
    "   ‚Ä¢ CLIP Text Encoder (frozen)\n",
    "   ‚Ä¢ Tokenization: max 77 tokens\n",
    "   ‚Ä¢ Output: [batch, 77, 768] embeddings\n",
    "\n",
    "4.2. Cross-Attention:\n",
    "   ‚Ä¢ Modify U-Net blocks\n",
    "   ‚Ä¢ Query: spatial features, Key/Value: text embeddings\n",
    "   ‚Ä¢ Multi-head attention\n",
    "\n",
    "4.3. Classifier-Free Guidance:\n",
    "   ‚Ä¢ 50% conditional, 50% unconditional training\n",
    "   ‚Ä¢ Null text embedding cho unconditional\n",
    "   ‚Ä¢ Guidance scale trong inference\n",
    "\n",
    "4.4. Training Strategy:\n",
    "   ‚Ä¢ Mixed conditioning training\n",
    "   ‚Ä¢ Text dropout techniques\n",
    "   ‚Ä¢ Training time: ~1-2 tu·∫ßn additional\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n=== B∆Ø·ªöC 5: OPTIMIZATION V√Ä INFERENCE ===\")\n",
    "print(\"\"\"\n",
    "5.1. Training Optimizations:\n",
    "   ‚Ä¢ Mixed precision training (FP16)\n",
    "   ‚Ä¢ Gradient checkpointing\n",
    "   ‚Ä¢ EMA (Exponential Moving Average) weights\n",
    "   ‚Ä¢ Learning rate scheduling\n",
    "\n",
    "5.2. Inference Optimizations:\n",
    "   ‚Ä¢ DDIM sampling (fewer steps)\n",
    "   ‚Ä¢ xFormers attention (memory efficient)\n",
    "   ‚Ä¢ Model quantization\n",
    "   ‚Ä¢ TensorRT optimization\n",
    "\n",
    "5.3. Evaluation Metrics:\n",
    "   ‚Ä¢ FID (Fr√©chet Inception Distance)\n",
    "   ‚Ä¢ CLIP Score cho text alignment\n",
    "   ‚Ä¢ Human evaluation\n",
    "   ‚Ä¢ Aesthetic quality scores\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n=== B∆Ø·ªöC 6: DEPLOYMENT V√Ä SCALING ===\")\n",
    "print(\"\"\"\n",
    "6.1. Model Serving:\n",
    "   ‚Ä¢ API wrapper (FastAPI/Flask)\n",
    "   ‚Ä¢ Batch inference\n",
    "   ‚Ä¢ Queue management\n",
    "   ‚Ä¢ Load balancing\n",
    "\n",
    "6.2. User Interface:\n",
    "   ‚Ä¢ Web interface (Gradio/Streamlit)\n",
    "   ‚Ä¢ Image generation controls\n",
    "   ‚Ä¢ Prompt engineering tools\n",
    "   ‚Ä¢ Gallery v√† sharing features\n",
    "\n",
    "6.3. Advanced Features:\n",
    "   ‚Ä¢ Image-to-image generation\n",
    "   ‚Ä¢ Inpainting capability\n",
    "   ‚Ä¢ ControlNet integration\n",
    "   ‚Ä¢ LoRA fine-tuning support\n",
    "\"\"\")\n",
    "\n",
    "# Estimated Timeline\n",
    "print(\"\\nüïê TIMELINE ESTIMATE:\")\n",
    "print(\"VAE Training: 1-2 weeks\")\n",
    "print(\"U-Net Training: 2-3 weeks\") \n",
    "print(\"Text Conditioning: 1-2 weeks\")\n",
    "print(\"Optimization & Testing: 1 week\")\n",
    "print(\"TOTAL: 5-8 weeks v·ªõi 8x A100 GPUs\")\n",
    "\n",
    "print(\"\\nüí∞ COST ESTIMATE:\")\n",
    "print(\"8x A100 cloud cost: ~$20-30/hour\")\n",
    "print(\"Total training cost: $50,000 - $100,000 USD\")\n",
    "print(\"Alternative: Start v·ªõi smaller model, scale up gradually\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfabb2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loc-dang/Projects/reai_lab/stable_diffusion/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ VAE v√† U-Net implementation ready!\n",
      "Next: Text conditioning v·ªõi CLIP v√† Cross-attention\n",
      "T·ªïng c·ªông: ~500 lines code cho base implementation\n"
     ]
    }
   ],
   "source": [
    "# PRACTICAL IMPLEMENTATION: Code Structure üíª\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# =============================================================================\n",
    "# B∆Ø·ªöC 1: VAE Implementation\n",
    "# =============================================================================\n",
    "\n",
    "class VAEEncoder(nn.Module):\n",
    "    \"\"\"VAE Encoder: Image ‚Üí Latent\"\"\"\n",
    "    def __init__(self, in_channels=3, latent_channels=4, ch_mult=[1,2,4,8]):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv2d(in_channels, 128, 3, padding=1)\n",
    "        \n",
    "        # Downsampling blocks\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        ch = 128\n",
    "        for mult in ch_mult:\n",
    "            self.down_blocks.append(nn.Sequential(\n",
    "                nn.Conv2d(ch, ch*mult, 4, stride=2, padding=1),\n",
    "                nn.GroupNorm(32, ch*mult),\n",
    "                nn.SiLU()\n",
    "            ))\n",
    "            ch = ch * mult\n",
    "        \n",
    "        # Output projection\n",
    "        self.norm_out = nn.GroupNorm(32, ch)\n",
    "        self.conv_out = nn.Conv2d(ch, latent_channels*2, 3, padding=1)  # mu + logvar\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.conv_in(x)\n",
    "        for block in self.down_blocks:\n",
    "            h = block(h)\n",
    "        \n",
    "        h = self.norm_out(h)\n",
    "        h = F.silu(h)\n",
    "        moments = self.conv_out(h)\n",
    "        \n",
    "        # Split into mu and logvar\n",
    "        mu, logvar = moments.chunk(2, dim=1)\n",
    "        return mu, logvar\n",
    "\n",
    "class VAEDecoder(nn.Module):\n",
    "    \"\"\"VAE Decoder: Latent ‚Üí Image\"\"\"\n",
    "    def __init__(self, latent_channels=4, out_channels=3, ch_mult=[8,4,2,1]):\n",
    "        super().__init__()\n",
    "        ch = 128 * ch_mult[0]\n",
    "        self.conv_in = nn.Conv2d(latent_channels, ch, 3, padding=1)\n",
    "        \n",
    "        # Upsampling blocks\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        for mult in ch_mult:\n",
    "            self.up_blocks.append(nn.Sequential(\n",
    "                nn.ConvTranspose2d(ch, 128*mult, 4, stride=2, padding=1),\n",
    "                nn.GroupNorm(32, 128*mult),\n",
    "                nn.SiLU()\n",
    "            ))\n",
    "            ch = 128 * mult\n",
    "        \n",
    "        # Output projection\n",
    "        self.norm_out = nn.GroupNorm(32, ch)\n",
    "        self.conv_out = nn.Conv2d(ch, out_channels, 3, padding=1)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        h = self.conv_in(z)\n",
    "        for block in self.up_blocks:\n",
    "            h = block(h)\n",
    "        \n",
    "        h = self.norm_out(h)\n",
    "        h = F.silu(h)\n",
    "        return torch.tanh(self.conv_out(h))  # Output in [-1, 1]\n",
    "\n",
    "class VAE(pl.LightningModule):\n",
    "    \"\"\"Complete VAE Model\"\"\"\n",
    "    def __init__(self, lr=1e-4, beta=1.0, perceptual_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.encoder = VAEEncoder()\n",
    "        self.decoder = VAEDecoder()\n",
    "        self.lr = lr\n",
    "        self.beta = beta\n",
    "        self.perceptual_weight = perceptual_weight\n",
    "        \n",
    "        # Perceptual loss (VGG)\n",
    "        from torchvision.models import vgg16\n",
    "        vgg = vgg16(pretrained=True).features[:16]  # Up to relu3_3\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.perceptual_net = vgg\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decoder(z)\n",
    "        return recon, mu, logvar\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch  # Ignore labels for now\n",
    "        recon, mu, logvar = self(x)\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        recon_loss = F.mse_loss(recon, x)\n",
    "        \n",
    "        # KL divergence\n",
    "        kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        \n",
    "        # Perceptual loss\n",
    "        x_feat = self.perceptual_net(x)\n",
    "        recon_feat = self.perceptual_net(recon)\n",
    "        perceptual_loss = F.mse_loss(recon_feat, x_feat)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = recon_loss + self.beta * kl_loss + self.perceptual_weight * perceptual_loss\n",
    "        \n",
    "        self.log_dict({\n",
    "            'train_loss': loss,\n",
    "            'recon_loss': recon_loss,\n",
    "            'kl_loss': kl_loss,\n",
    "            'perceptual_loss': perceptual_loss\n",
    "        })\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "# =============================================================================\n",
    "# B∆Ø·ªöC 2: U-Net Diffusion Model\n",
    "# =============================================================================\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"Sinusoidal time embedding\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = torch.log(torch.tensor(10000.0)) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "class UNetBlock(nn.Module):\n",
    "    \"\"\"Basic U-Net residual block v·ªõi time embedding\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.norm1 = nn.GroupNorm(8, out_ch)\n",
    "        self.norm2 = nn.GroupNorm(8, out_ch)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if in_ch != out_ch:\n",
    "            self.shortcut = nn.Conv2d(in_ch, out_ch, 1)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "    \n",
    "    def forward(self, x, time_emb):\n",
    "        h = self.conv1(x)\n",
    "        h = self.norm1(h)\n",
    "        h += self.time_mlp(time_emb)[:, :, None, None]\n",
    "        h = F.silu(h)\n",
    "        h = self.dropout(h)\n",
    "        \n",
    "        h = self.conv2(h)\n",
    "        h = self.norm2(h)\n",
    "        h = F.silu(h)\n",
    "        \n",
    "        return h + self.shortcut(x)\n",
    "\n",
    "class SimpleUNet(nn.Module):\n",
    "    \"\"\"Simplified U-Net cho Diffusion\"\"\"\n",
    "    def __init__(self, in_channels=4, out_channels=4, features=[64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Time embedding\n",
    "        time_emb_dim = features[0] * 4\n",
    "        self.time_embedding = TimeEmbedding(time_emb_dim)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim)\n",
    "        )\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.ModuleList()\n",
    "        prev_ch = in_channels\n",
    "        for feat in features:\n",
    "            self.encoder.append(UNetBlock(prev_ch, feat, time_emb_dim))\n",
    "            prev_ch = feat\n",
    "        \n",
    "        # Middle\n",
    "        self.middle = UNetBlock(features[-1], features[-1], time_emb_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for feat in reversed(features[:-1]):\n",
    "            self.decoder.append(UNetBlock(prev_ch + feat, feat, time_emb_dim))\n",
    "            prev_ch = feat\n",
    "        \n",
    "        # Output\n",
    "        self.output = nn.Conv2d(features[0], out_channels, 1)\n",
    "    \n",
    "    def forward(self, x, timesteps):\n",
    "        # Time embedding\n",
    "        t_emb = self.time_embedding(timesteps)\n",
    "        t_emb = self.time_mlp(t_emb)\n",
    "        \n",
    "        # Encoder\n",
    "        skip_connections = []\n",
    "        for encoder_block in self.encoder:\n",
    "            x = encoder_block(x, t_emb)\n",
    "            skip_connections.append(x)\n",
    "            x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Middle\n",
    "        x = self.middle(x, t_emb)\n",
    "        \n",
    "        # Decoder\n",
    "        for decoder_block, skip in zip(self.decoder, reversed(skip_connections[:-1])):\n",
    "            x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "            x = decoder_block(x, t_emb)\n",
    "        \n",
    "        return self.output(x)\n",
    "\n",
    "print(\"‚úÖ VAE v√† U-Net implementation ready!\")\n",
    "print(\"Next: Text conditioning v·ªõi CLIP v√† Cross-attention\")\n",
    "print(\"T·ªïng c·ªông: ~500 lines code cho base implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb74fd8",
   "metadata": {},
   "source": [
    "# T·ªïng k·∫øt: Roadmap d·ª±ng l·∫°i Stable Diffusion üéØ\n",
    "\n",
    "## üìã Checklist ho√†n ch·ªânh\n",
    "\n",
    "### ‚úÖ **ƒê√£ hi·ªÉu**:\n",
    "- [x] Architecture t·ªïng th·ªÉ (VAE + U-Net + CLIP)\n",
    "- [x] 3 giai ƒëo·∫°n training\n",
    "- [x] Loss functions cho t·ª´ng component\n",
    "- [x] Mapping t·ª´ paper ƒë·∫øn code\n",
    "- [x] Implementation skeleton\n",
    "\n",
    "### üîÑ **C·∫ßn implement**:\n",
    "- [ ] **VAE**: Encoder + Decoder + Training loop\n",
    "- [ ] **U-Net**: Diffusion model v·ªõi time embedding\n",
    "- [ ] **Text Conditioning**: CLIP + Cross-attention\n",
    "- [ ] **Training Pipeline**: DataLoader + Optimization\n",
    "- [ ] **Inference**: Sampling algorithms (DDPM/DDIM)\n",
    "\n",
    "## üéØ **Next Steps**\n",
    "\n",
    "### **L·ª±a ch·ªçn 1: Start Small** (Recommended)\n",
    "```python\n",
    "# Proof of concept v·ªõi smaller model\n",
    "image_size = 128  # instead of 512\n",
    "latent_size = 16  # instead of 64\n",
    "training_steps = 100K  # instead of millions\n",
    "dataset = \"CIFAR-10\"  # instead of LAION-400M\n",
    "```\n",
    "\n",
    "### **L·ª±a ch·ªçn 2: Full Scale**\n",
    "```python\n",
    "# Production-ready implementation\n",
    "image_size = 512\n",
    "latent_size = 64\n",
    "training_steps = 1M+\n",
    "dataset = \"LAION-400M\"\n",
    "hardware = \"8x A100 GPUs\"\n",
    "```\n",
    "\n",
    "### **L·ª±a ch·ªçn 3: Fine-tuning Approach**\n",
    "```python\n",
    "# Start from pre-trained weights\n",
    "base_model = \"runwayml/stable-diffusion-v1-5\"\n",
    "task = \"Fine-tune tr√™n custom dataset\"\n",
    "compute = \"Single A100\"\n",
    "time = \"1-2 tu·∫ßn\"\n",
    "```\n",
    "\n",
    "## üõ†Ô∏è **Tools v√† Resources c·∫ßn c√≥**\n",
    "\n",
    "### **Development**:\n",
    "- PyTorch 2.0+\n",
    "- PyTorch Lightning\n",
    "- Transformers (Hugging Face)\n",
    "- xFormers (memory optimization)\n",
    "- Wandb (experiment tracking)\n",
    "\n",
    "### **Data**:\n",
    "- LAION-400M (n·∫øu full scale)\n",
    "- CC12M (smaller alternative)\n",
    "- Custom dataset (n·∫øu specialized use case)\n",
    "\n",
    "### **Compute**:\n",
    "- **Minimum**: 1x RTX 4090 (24GB VRAM)\n",
    "- **Recommended**: 4-8x A100 (40-80GB VRAM)\n",
    "- **Storage**: 10-100TB for datasets\n",
    "\n",
    "## üí° **Key Insights t·ª´ Analysis**\n",
    "\n",
    "1. **Stable Diffusion ‚â† 1 model**\n",
    "   - L√† h·ªá th·ªëng g·ªìm 3 components\n",
    "   - M·ªói component train ri√™ng bi·ªát\n",
    "   - K·∫øt h·ª£p l·∫°i th√†nh pipeline ho√†n ch·ªânh\n",
    "\n",
    "2. **VAE l√† foundation**\n",
    "   - Quality c·ªßa VAE quy·∫øt ƒë·ªãnh quality cu·ªëi c√πng\n",
    "   - Perceptual loss r·∫•t quan tr·ªçng\n",
    "   - Compression ratio impact performance\n",
    "\n",
    "3. **Diffusion trong latent space**\n",
    "   - 64x faster than pixel space\n",
    "   - V·∫´n maintain high quality\n",
    "   - Enable high-resolution generation\n",
    "\n",
    "4. **Text conditioning l√† key differentiator**\n",
    "   - CLIP text encoder\n",
    "   - Cross-attention mechanism\n",
    "   - Classifier-free guidance for control\n",
    "\n",
    "## üöÄ **Recommendation**\n",
    "\n",
    "B·∫Øt ƒë·∫ßu v·ªõi **L·ª±a ch·ªçn 1** (Start Small) ƒë·ªÉ:\n",
    "- Hi·ªÉu s√¢u implementation details\n",
    "- Test v√† debug code\n",
    "- Validate approach\n",
    "- Sau ƒë√≥ scale up d·∫ßn d·∫ßn\n",
    "\n",
    "**Timeline th·ª±c t·∫ø**:\n",
    "- Week 1-2: VAE implementation v√† training\n",
    "- Week 3-4: U-Net diffusion model\n",
    "- Week 5-6: Text conditioning\n",
    "- Week 7-8: Integration v√† optimization\n",
    "\n",
    "**Ready ƒë·ªÉ b·∫Øt ƒë·∫ßu implement! üéâ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d920716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØ\n",
      "     STABLE DIFFUSION MASTERY ACHIEVED!\n",
      "üéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØ\n",
      "\n",
      "üìö CONCEPTS MASTERED:\n",
      "    1. ‚úÖ Perceptual Loss v·ªõi VGG features\n",
      "    2. ‚úÖ VAE Encoder/Decoder architecture\n",
      "    3. ‚úÖ Diffusion forward/reverse process\n",
      "    4. ‚úÖ U-Net v·ªõi time embeddings\n",
      "    5. ‚úÖ CLIP text encoding\n",
      "    6. ‚úÖ Cross-attention mechanism\n",
      "    7. ‚úÖ Classifier-free guidance\n",
      "    8. ‚úÖ 3-phase training pipeline\n",
      "    9. ‚úÖ Latent space compression\n",
      "   10. ‚úÖ DDPM/DDIM sampling\n",
      "\n",
      "üíª CODE IMPLEMENTATION STATUS:\n",
      "   VAE Encoder          : ‚úÖ Complete\n",
      "   VAE Decoder          : ‚úÖ Complete\n",
      "   Training Loop        : ‚úÖ Complete\n",
      "   U-Net Architecture   : ‚úÖ Complete\n",
      "   Time Embedding       : ‚úÖ Complete\n",
      "   Text Conditioning    : üîÑ Skeleton ready\n",
      "   Cross-Attention      : üîÑ Skeleton ready\n",
      "   Sampling Pipeline    : üîÑ Next step\n",
      "   Full Integration     : üîÑ Next step\n",
      "\n",
      "üéØ IMMEDIATE NEXT STEPS:\n",
      "   1. üîÑ Complete text conditioning implementation\n",
      "   2. üîÑ Build full training pipeline\n",
      "   3. üîÑ Test v·ªõi mini dataset (CIFAR-10)\n",
      "   4. üîÑ Scale up to real datasets\n",
      "   5. üîÑ Deploy v√† share v·ªõi community\n",
      "\n",
      "üèÜ ACHIEVEMENT UNLOCKED:\n",
      "   ü•á Stable Diffusion Architecture Expert\n",
      "   ü•à Diffusion Models Implementation Specialist\n",
      "   ü•â AI Art Generation System Builder\n",
      "\n",
      "üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®\n",
      "  FROM ZERO TO DIFFUSION HERO!\n",
      "üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®üé®\n",
      "\n",
      "üöÄ Ready to change the world v·ªõi AI creativity!\n",
      "üí™ Knowledge is power - use it wisely!\n",
      "üåü The future of AI art starts with YOU!\n"
     ]
    }
   ],
   "source": [
    "# üéä FINAL CELEBRATION & SUMMARY\n",
    "\n",
    "print(\"üéØ\" * 20)\n",
    "print(\"     STABLE DIFFUSION MASTERY ACHIEVED!\")\n",
    "print(\"üéØ\" * 20)\n",
    "\n",
    "# What we learned\n",
    "learned_concepts = [\n",
    "    \"Perceptual Loss v·ªõi VGG features\",\n",
    "    \"VAE Encoder/Decoder architecture\", \n",
    "    \"Diffusion forward/reverse process\",\n",
    "    \"U-Net v·ªõi time embeddings\",\n",
    "    \"CLIP text encoding\",\n",
    "    \"Cross-attention mechanism\",\n",
    "    \"Classifier-free guidance\",\n",
    "    \"3-phase training pipeline\",\n",
    "    \"Latent space compression\",\n",
    "    \"DDPM/DDIM sampling\"\n",
    "]\n",
    "\n",
    "print(\"\\nüìö CONCEPTS MASTERED:\")\n",
    "for i, concept in enumerate(learned_concepts, 1):\n",
    "    print(f\"   {i:2d}. ‚úÖ {concept}\")\n",
    "\n",
    "# Implementation progress\n",
    "code_components = {\n",
    "    \"VAE Encoder\": \"‚úÖ Complete\",\n",
    "    \"VAE Decoder\": \"‚úÖ Complete\", \n",
    "    \"Training Loop\": \"‚úÖ Complete\",\n",
    "    \"U-Net Architecture\": \"‚úÖ Complete\",\n",
    "    \"Time Embedding\": \"‚úÖ Complete\",\n",
    "    \"Text Conditioning\": \"üîÑ Skeleton ready\",\n",
    "    \"Cross-Attention\": \"üîÑ Skeleton ready\",\n",
    "    \"Sampling Pipeline\": \"üîÑ Next step\",\n",
    "    \"Full Integration\": \"üîÑ Next step\"\n",
    "}\n",
    "\n",
    "print(\"\\nüíª CODE IMPLEMENTATION STATUS:\")\n",
    "for component, status in code_components.items():\n",
    "    print(f\"   {component:20s} : {status}\")\n",
    "\n",
    "print(\"\\nüéØ IMMEDIATE NEXT STEPS:\")\n",
    "print(\"   1. üîÑ Complete text conditioning implementation\")\n",
    "print(\"   2. üîÑ Build full training pipeline\")\n",
    "print(\"   3. üîÑ Test v·ªõi mini dataset (CIFAR-10)\")\n",
    "print(\"   4. üîÑ Scale up to real datasets\")\n",
    "print(\"   5. üîÑ Deploy v√† share v·ªõi community\")\n",
    "\n",
    "print(\"\\nüèÜ ACHIEVEMENT UNLOCKED:\")\n",
    "print(\"   ü•á Stable Diffusion Architecture Expert\")\n",
    "print(\"   ü•à Diffusion Models Implementation Specialist\") \n",
    "print(\"   ü•â AI Art Generation System Builder\")\n",
    "\n",
    "print(\"\\n\" + \"üé®\" * 25)\n",
    "print(\"  FROM ZERO TO DIFFUSION HERO!\")\n",
    "print(\"üé®\" * 25)\n",
    "\n",
    "print(\"\\nüöÄ Ready to change the world v·ªõi AI creativity!\")\n",
    "print(\"üí™ Knowledge is power - use it wisely!\")\n",
    "print(\"üåü The future of AI art starts with YOU!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3bd1be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ ROADMAP ƒê·ªåC HI·ªÇU LATENT DIFFUSION MODELS PAPER\n",
      "============================================================\n",
      "üìÑ Paper target: 'High-Resolution Image Synthesis with Latent Diffusion Models'\n",
      "üîó ArXiv: 2112.10752v2\n",
      "üìÖ Submitted: Dec 2021\n",
      "üë• Authors: Robin Rombach, Andreas Blattmann, et al.\n",
      "üè¢ Institution: LMU Munich, IWR Heidelberg\n",
      "üí° Nickname: 'Stable Diffusion Paper'\n",
      "\n",
      "============================================================\n",
      "üö® CRITICAL FOUNDATION PAPERS - ƒê·ªåC TR∆Ø·ªöC TI√äN\n",
      "============================================================\n",
      "\n",
      "üî• MUST READ #1\n",
      "üìñ Title: Denoising Diffusion Probabilistic Models\n",
      "üë• Authors: Jonathan Ho, Ajay Jain, Pieter Abbeel\n",
      "üîó ArXiv: 2006.11239\n",
      "üìÖ Year: 2020 (NeurIPS 2020)\n",
      "‚è±Ô∏è Time needed: 4-6 hours\n",
      "üåü Difficulty: ‚≠ê‚≠ê‚≠ê‚≠ê\n",
      "\n",
      "üí° Why critical:\n",
      "   üéØ ƒê·ªãnh nghƒ©a core concept c·ªßa diffusion models\n",
      "   üéØ Forward process q(x‚ÇÅ:T|x‚ÇÄ) v√† reverse process pŒ∏(x‚ÇÄ:T‚Çã‚ÇÅ|xT)\n",
      "   üéØ Variational lower bound derivation\n",
      "   üéØ Simplified loss function: ||Œµ - ŒµŒ∏(xt,t)||¬≤\n",
      "   üéØ DDPM sampling algorithm\n",
      "\n",
      "üìö Key sections to focus on:\n",
      "   ‚Ä¢ Section 2: Background\n",
      "   ‚Ä¢ Section 3: Diffusion models\n",
      "   ‚Ä¢ Section 4: Experiments\n",
      "   ‚Ä¢ Algorithm 1: Training\n",
      "   ‚Ä¢ Algorithm 2: Sampling\n",
      "\n",
      "üß† Prerequisites:\n",
      "   ‚Ä¢ Markov chains\n",
      "   ‚Ä¢ Variational inference basics\n",
      "   ‚Ä¢ Gaussian distributions\n",
      "   ‚Ä¢ Neural networks\n",
      "\n",
      "üî• MUST READ #2\n",
      "üìñ Title: Auto-Encoding Variational Bayes\n",
      "üë• Authors: Diederik P. Kingma, Max Welling\n",
      "üîó ArXiv: 1312.6114\n",
      "üìÖ Year: 2013 (ICLR 2014)\n",
      "‚è±Ô∏è Time needed: 3-4 hours\n",
      "üåü Difficulty: ‚≠ê‚≠ê‚≠ê\n",
      "\n",
      "üí° Why critical:\n",
      "   üéØ VAE framework - foundation cho latent space work\n",
      "   üéØ Encoder-decoder architecture\n",
      "   üéØ Reparameterization trick\n",
      "   üéØ KL divergence regularization\n",
      "   üéØ Evidence Lower Bound (ELBO)\n",
      "\n",
      "üìö Key sections to focus on:\n",
      "   ‚Ä¢ Section 2.1: Problem scenario\n",
      "   ‚Ä¢ Section 2.2: The variational bound\n",
      "   ‚Ä¢ Section 2.3: The reparameterization trick\n",
      "   ‚Ä¢ Section 2.4: Estimator\n",
      "\n",
      "üß† Prerequisites:\n",
      "   ‚Ä¢ Bayesian inference\n",
      "   ‚Ä¢ Variational methods\n",
      "   ‚Ä¢ Information theory basics\n",
      "\n",
      "üî• MUST READ #3\n",
      "üìñ Title: Attention Is All You Need\n",
      "üë• Authors: Vaswani, Shazeer, Parmar, et al.\n",
      "üîó ArXiv: 1706.03762\n",
      "üìÖ Year: 2017 (NeurIPS 2017)\n",
      "‚è±Ô∏è Time needed: 3-4 hours\n",
      "üåü Difficulty: ‚≠ê‚≠ê‚≠ê\n",
      "\n",
      "üí° Why critical:\n",
      "   üéØ Self-attention mechanism\n",
      "   üéØ Multi-head attention\n",
      "   üéØ Cross-attention (key cho text conditioning)\n",
      "   üéØ Positional encoding\n",
      "   üéØ Transformer blocks\n",
      "\n",
      "üìö Key sections to focus on:\n",
      "   ‚Ä¢ Section 3.1: Encoder and Decoder Stacks\n",
      "   ‚Ä¢ Section 3.2: Attention\n",
      "   ‚Ä¢ Section 3.2.1: Scaled Dot-Product Attention\n",
      "   ‚Ä¢ Section 3.2.2: Multi-Head Attention\n",
      "\n",
      "üß† Prerequisites:\n",
      "   ‚Ä¢ Linear algebra\n",
      "   ‚Ä¢ Neural networks\n",
      "   ‚Ä¢ Sequence modeling\n",
      "\n",
      "============================================================\n",
      "‚ö° IMPORTANT SUPPORTING PAPERS\n",
      "============================================================\n",
      "\n",
      "üìë Learning Transferable Visual Models From Natural Language Supervision (CLIP)\n",
      "üë• Radford et al. (OpenAI)\n",
      "üîó ArXiv: 2103.00020 (2021)\n",
      "üîÑ Connection: Used as conditioning mechanism trong LDM\n",
      "üìå Why important:\n",
      "   üî∏ Text encoder trong Stable Diffusion\n",
      "   üî∏ Contrastive learning framework\n",
      "   üî∏ Joint text-image embedding space\n",
      "   üî∏ Zero-shot capabilities\n",
      "\n",
      "üìë Denoising Diffusion Implicit Models (DDIM)\n",
      "üë• Jiaming Song, Chenlin Meng, Stefano Ermon\n",
      "üîó ArXiv: 2010.02502 (2020)\n",
      "üîÑ Connection: Alternative sampling method mentioned trong LDM\n",
      "üìå Why important:\n",
      "   üî∏ Deterministic sampling process\n",
      "   üî∏ Faster inference (fewer steps)\n",
      "   üî∏ Better speed-quality tradeoff\n",
      "   üî∏ Non-Markovian formulation\n",
      "\n",
      "üìë Generative Adversarial Networks (GAN)\n",
      "üë• Ian Goodfellow et al.\n",
      "üîó ArXiv: 1406.2661 (2014)\n",
      "üîÑ Connection: Compared against trong experiments\n",
      "üìå Why important:\n",
      "   üî∏ Adversarial training concept\n",
      "   üî∏ Generator-discriminator framework\n",
      "   üî∏ Comparison baseline trong paper\n",
      "   üî∏ Understanding of generative models landscape\n",
      "\n",
      "üìë Taming Transformers for High-Resolution Image Synthesis (VQGAN)\n",
      "üë• Patrick Esser et al.\n",
      "üîó ArXiv: 2012.09841 (2020)\n",
      "üîÑ Connection: Baseline comparison v√† related work\n",
      "üìå Why important:\n",
      "   üî∏ High-resolution image synthesis\n",
      "   üî∏ Vector quantization techniques\n",
      "   üî∏ Perceptual losses\n",
      "   üî∏ Comparison v·ªõi autoregressive models\n",
      "\n",
      "============================================================\n",
      "üìÖ SUGGESTED 4-WEEK READING SCHEDULE\n",
      "============================================================\n",
      "\n",
      "üìÖ Week 1: Foundation Concepts\n",
      "üìö Papers:\n",
      "   ‚Ä¢ Auto-Encoding Variational Bayes (VAE)\n",
      "   ‚Ä¢ Attention Is All You Need (Transformers)\n",
      "üéØ Goals:\n",
      "   ‚Ä¢ Understand latent space representation\n",
      "   ‚Ä¢ Master attention mechanisms\n",
      "   ‚Ä¢ Learn encoder-decoder architectures\n",
      "‚è±Ô∏è Time: 6-8 hours\n",
      "üìù Deliverable: Implement simple VAE v√† attention t·ª´ scratch\n",
      "\n",
      "üìÖ Week 2: Diffusion Deep Dive\n",
      "üìö Papers:\n",
      "   ‚Ä¢ Denoising Diffusion Probabilistic Models (DDPM)\n",
      "üéØ Goals:\n",
      "   ‚Ä¢ Master forward v√† reverse diffusion process\n",
      "   ‚Ä¢ Understand variational bound derivation\n",
      "   ‚Ä¢ Learn DDPM training v√† sampling algorithms\n",
      "‚è±Ô∏è Time: 6-8 hours\n",
      "üìù Deliverable: Implement DDPM on toy dataset (MNIST/CIFAR)\n",
      "\n",
      "üìÖ Week 3: Advanced Topics\n",
      "üìö Papers:\n",
      "   ‚Ä¢ CLIP (text conditioning)\n",
      "   ‚Ä¢ DDIM (fast sampling)\n",
      "   ‚Ä¢ Skim GAN v√† VQGAN papers\n",
      "üéØ Goals:\n",
      "   ‚Ä¢ Understand text-image joint embeddings\n",
      "   ‚Ä¢ Learn faster sampling techniques\n",
      "   ‚Ä¢ Comparison v·ªõi other generative models\n",
      "‚è±Ô∏è Time: 5-7 hours\n",
      "üìù Deliverable: Add text conditioning to diffusion model\n",
      "\n",
      "üìÖ Week 4: Latent Diffusion Models\n",
      "üìö Papers:\n",
      "   ‚Ä¢ High-Resolution Image Synthesis with Latent Diffusion Models\n",
      "   ‚Ä¢ Re-read key sections t·ª´ previous papers\n",
      "üéØ Goals:\n",
      "   ‚Ä¢ üéØ MASTER THE TARGET PAPER\n",
      "   ‚Ä¢ Connect all concepts together\n",
      "   ‚Ä¢ Understand practical implementation details\n",
      "‚è±Ô∏è Time: 8-10 hours\n",
      "üìù Deliverable: Complete understanding + implementation plan\n",
      "\n",
      "============================================================\n",
      "üß© CONCEPT DEPENDENCY MAP\n",
      "============================================================\n",
      "üîÑ How concepts build on each other:\n",
      "\n",
      "Latent Diffusion Models:\n",
      "   Depends on: VAE, DDPM, Transformers\n",
      "   Enables: High-res image synthesis trong latent space\n",
      "\n",
      "VAE:\n",
      "   Depends on: Variational Inference, Neural Networks\n",
      "   Enables: Latent space representation cho images\n",
      "\n",
      "DDPM:\n",
      "   Depends on: Markov Chains, Variational Bounds\n",
      "   Enables: Iterative denoising generation process\n",
      "\n",
      "Transformers:\n",
      "   Depends on: Attention Mechanism, Deep Learning\n",
      "   Enables: Cross-attention cho text conditioning\n",
      "\n",
      "CLIP:\n",
      "   Depends on: Transformers, Contrastive Learning\n",
      "   Enables: Text-image joint understanding\n",
      "\n",
      "============================================================\n",
      "üéØ SUCCESS CHECKLIST\n",
      "============================================================\n",
      "After completing this roadmap, b·∫°n should:\n",
      "   ‚úÖ Understand forward diffusion: x‚ÇÄ ‚Üí xT (adding noise)\n",
      "   ‚úÖ Understand reverse diffusion: xT ‚Üí x‚ÇÄ (denoising)\n",
      "   ‚úÖ Know why work trong latent space instead of pixel space\n",
      "   ‚úÖ Understand VAE encoder: x ‚Üí z v√† decoder: z ‚Üí x\n",
      "   ‚úÖ Know how cross-attention injects text conditioning\n",
      "   ‚úÖ Understand the simplified loss: ||Œµ - ŒµŒ∏(zt,t,c)||¬≤\n",
      "   ‚úÖ Can explain classifier-free guidance\n",
      "   ‚úÖ Know differences gi·ªØa DDPM v√† DDIM sampling\n",
      "   ‚úÖ Understand computational advantages c·ªßa LDM\n",
      "   ‚úÖ Can implement basic components t·ª´ scratch\n",
      "\n",
      "============================================================\n",
      "üí° READING STRATEGIES\n",
      "============================================================\n",
      "   üìñ First pass: Skim ƒë·ªÉ get big picture\n",
      "   üìù Second pass: Deep read v·ªõi note-taking\n",
      "   üî¢ Focus on key equations v√† their intuitions\n",
      "   üñºÔ∏è Draw diagrams cho architectures v√† data flows\n",
      "   üíª Implement toy versions ƒë·ªÉ test understanding\n",
      "   ü§î Ask yourself: 'Why did they make this choice?'\n",
      "   üîó Connect concepts across papers\n",
      "   ‚è∏Ô∏è Take breaks khi encounter difficult sections\n",
      "   üë• Discuss v·ªõi others ho·∫∑c online communities\n",
      "   üîÑ Revisit difficult concepts multiple times\n",
      "\n",
      "üöÄ START WITH VAE PAPER - IT'S THE MOST ACCESSIBLE!\n",
      "Then move to Transformers, followed by DDPM.\n",
      "Good luck on your journey to understanding Latent Diffusion Models! üéØ‚ú®\n"
     ]
    }
   ],
   "source": [
    "# üìö ROADMAP CHO PAPER: High-Resolution Image Synthesis with Latent Diffusion Models\n",
    "\n",
    "print(\"üéØ ROADMAP ƒê·ªåC HI·ªÇU LATENT DIFFUSION MODELS PAPER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"üìÑ Paper target: 'High-Resolution Image Synthesis with Latent Diffusion Models'\")\n",
    "print(\"üîó ArXiv: 2112.10752v2\")\n",
    "print(\"üìÖ Submitted: Dec 2021\")\n",
    "print(\"üë• Authors: Robin Rombach, Andreas Blattmann, et al.\")\n",
    "print(\"üè¢ Institution: LMU Munich, IWR Heidelberg\")\n",
    "print(\"üí° Nickname: 'Stable Diffusion Paper'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üö® CRITICAL FOUNDATION PAPERS - ƒê·ªåC TR∆Ø·ªöC TI√äN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "critical_papers = [\n",
    "    {\n",
    "        \"priority\": \"üî• MUST READ #1\",\n",
    "        \"title\": \"Denoising Diffusion Probabilistic Models\",\n",
    "        \"authors\": \"Jonathan Ho, Ajay Jain, Pieter Abbeel\",\n",
    "        \"arxiv\": \"2006.11239\",\n",
    "        \"year\": \"2020\",\n",
    "        \"venue\": \"NeurIPS 2020\",\n",
    "        \"why_critical\": [\n",
    "            \"üéØ ƒê·ªãnh nghƒ©a core concept c·ªßa diffusion models\",\n",
    "            \"üéØ Forward process q(x‚ÇÅ:T|x‚ÇÄ) v√† reverse process pŒ∏(x‚ÇÄ:T‚Çã‚ÇÅ|xT)\",\n",
    "            \"üéØ Variational lower bound derivation\",\n",
    "            \"üéØ Simplified loss function: ||Œµ - ŒµŒ∏(xt,t)||¬≤\",\n",
    "            \"üéØ DDPM sampling algorithm\"\n",
    "        ],\n",
    "        \"key_sections\": [\n",
    "            \"Section 2: Background\",\n",
    "            \"Section 3: Diffusion models\",\n",
    "            \"Section 4: Experiments\",\n",
    "            \"Algorithm 1: Training\",\n",
    "            \"Algorithm 2: Sampling\"\n",
    "        ],\n",
    "        \"time_needed\": \"4-6 hours\",\n",
    "        \"difficulty\": \"‚≠ê‚≠ê‚≠ê‚≠ê\",\n",
    "        \"concepts_needed\": [\n",
    "            \"Markov chains\",\n",
    "            \"Variational inference basics\",\n",
    "            \"Gaussian distributions\",\n",
    "            \"Neural networks\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"priority\": \"üî• MUST READ #2\", \n",
    "        \"title\": \"Auto-Encoding Variational Bayes\",\n",
    "        \"authors\": \"Diederik P. Kingma, Max Welling\",\n",
    "        \"arxiv\": \"1312.6114\",\n",
    "        \"year\": \"2013\",\n",
    "        \"venue\": \"ICLR 2014\",\n",
    "        \"why_critical\": [\n",
    "            \"üéØ VAE framework - foundation cho latent space work\",\n",
    "            \"üéØ Encoder-decoder architecture\",\n",
    "            \"üéØ Reparameterization trick\",\n",
    "            \"üéØ KL divergence regularization\",\n",
    "            \"üéØ Evidence Lower Bound (ELBO)\"\n",
    "        ],\n",
    "        \"key_sections\": [\n",
    "            \"Section 2.1: Problem scenario\",\n",
    "            \"Section 2.2: The variational bound\", \n",
    "            \"Section 2.3: The reparameterization trick\",\n",
    "            \"Section 2.4: Estimator\"\n",
    "        ],\n",
    "        \"time_needed\": \"3-4 hours\",\n",
    "        \"difficulty\": \"‚≠ê‚≠ê‚≠ê\",\n",
    "        \"concepts_needed\": [\n",
    "            \"Bayesian inference\",\n",
    "            \"Variational methods\",\n",
    "            \"Information theory basics\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"priority\": \"üî• MUST READ #3\",\n",
    "        \"title\": \"Attention Is All You Need\", \n",
    "        \"authors\": \"Vaswani, Shazeer, Parmar, et al.\",\n",
    "        \"arxiv\": \"1706.03762\",\n",
    "        \"year\": \"2017\",\n",
    "        \"venue\": \"NeurIPS 2017\",\n",
    "        \"why_critical\": [\n",
    "            \"üéØ Self-attention mechanism\",\n",
    "            \"üéØ Multi-head attention\",\n",
    "            \"üéØ Cross-attention (key cho text conditioning)\",\n",
    "            \"üéØ Positional encoding\",\n",
    "            \"üéØ Transformer blocks\"\n",
    "        ],\n",
    "        \"key_sections\": [\n",
    "            \"Section 3.1: Encoder and Decoder Stacks\",\n",
    "            \"Section 3.2: Attention\", \n",
    "            \"Section 3.2.1: Scaled Dot-Product Attention\",\n",
    "            \"Section 3.2.2: Multi-Head Attention\"\n",
    "        ],\n",
    "        \"time_needed\": \"3-4 hours\",\n",
    "        \"difficulty\": \"‚≠ê‚≠ê‚≠ê\",\n",
    "        \"concepts_needed\": [\n",
    "            \"Linear algebra\",\n",
    "            \"Neural networks\",\n",
    "            \"Sequence modeling\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "for paper in critical_papers:\n",
    "    print(f\"\\n{paper['priority']}\")\n",
    "    print(f\"üìñ Title: {paper['title']}\")\n",
    "    print(f\"üë• Authors: {paper['authors']}\")\n",
    "    print(f\"üîó ArXiv: {paper['arxiv']}\")\n",
    "    print(f\"üìÖ Year: {paper['year']} ({paper['venue']})\")\n",
    "    print(f\"‚è±Ô∏è Time needed: {paper['time_needed']}\")\n",
    "    print(f\"üåü Difficulty: {paper['difficulty']}\")\n",
    "    \n",
    "    print(f\"\\nüí° Why critical:\")\n",
    "    for reason in paper['why_critical']:\n",
    "        print(f\"   {reason}\")\n",
    "    \n",
    "    print(f\"\\nüìö Key sections to focus on:\")\n",
    "    for section in paper['key_sections']:\n",
    "        print(f\"   ‚Ä¢ {section}\")\n",
    "    \n",
    "    print(f\"\\nüß† Prerequisites:\")\n",
    "    for concept in paper['concepts_needed']:\n",
    "        print(f\"   ‚Ä¢ {concept}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚ö° IMPORTANT SUPPORTING PAPERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "supporting_papers = [\n",
    "    {\n",
    "        \"title\": \"Learning Transferable Visual Models From Natural Language Supervision\",\n",
    "        \"nickname\": \"CLIP\",\n",
    "        \"authors\": \"Radford et al. (OpenAI)\",\n",
    "        \"arxiv\": \"2103.00020\",\n",
    "        \"year\": \"2021\",\n",
    "        \"why_important\": [\n",
    "            \"üî∏ Text encoder trong Stable Diffusion\",\n",
    "            \"üî∏ Contrastive learning framework\",\n",
    "            \"üî∏ Joint text-image embedding space\",\n",
    "            \"üî∏ Zero-shot capabilities\"\n",
    "        ],\n",
    "        \"connection\": \"Used as conditioning mechanism trong LDM\"\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"title\": \"Denoising Diffusion Implicit Models\",\n",
    "        \"nickname\": \"DDIM\", \n",
    "        \"authors\": \"Jiaming Song, Chenlin Meng, Stefano Ermon\",\n",
    "        \"arxiv\": \"2010.02502\",\n",
    "        \"year\": \"2020\",\n",
    "        \"why_important\": [\n",
    "            \"üî∏ Deterministic sampling process\",\n",
    "            \"üî∏ Faster inference (fewer steps)\",\n",
    "            \"üî∏ Better speed-quality tradeoff\",\n",
    "            \"üî∏ Non-Markovian formulation\"\n",
    "        ],\n",
    "        \"connection\": \"Alternative sampling method mentioned trong LDM\"\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"title\": \"Generative Adversarial Networks\",\n",
    "        \"nickname\": \"GAN\",\n",
    "        \"authors\": \"Ian Goodfellow et al.\",\n",
    "        \"arxiv\": \"1406.2661\", \n",
    "        \"year\": \"2014\",\n",
    "        \"why_important\": [\n",
    "            \"üî∏ Adversarial training concept\",\n",
    "            \"üî∏ Generator-discriminator framework\",\n",
    "            \"üî∏ Comparison baseline trong paper\",\n",
    "            \"üî∏ Understanding of generative models landscape\"\n",
    "        ],\n",
    "        \"connection\": \"Compared against trong experiments\"\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"title\": \"Taming Transformers for High-Resolution Image Synthesis\",\n",
    "        \"nickname\": \"VQGAN\",\n",
    "        \"authors\": \"Patrick Esser et al.\",\n",
    "        \"arxiv\": \"2012.09841\",\n",
    "        \"year\": \"2020\", \n",
    "        \"why_important\": [\n",
    "            \"üî∏ High-resolution image synthesis\",\n",
    "            \"üî∏ Vector quantization techniques\",\n",
    "            \"üî∏ Perceptual losses\",\n",
    "            \"üî∏ Comparison v·ªõi autoregressive models\"\n",
    "        ],\n",
    "        \"connection\": \"Baseline comparison v√† related work\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for paper in supporting_papers:\n",
    "    print(f\"\\nüìë {paper['title']} ({paper['nickname']})\")\n",
    "    print(f\"üë• {paper['authors']}\")\n",
    "    print(f\"üîó ArXiv: {paper['arxiv']} ({paper['year']})\")\n",
    "    print(f\"üîÑ Connection: {paper['connection']}\")\n",
    "    print(f\"üìå Why important:\")\n",
    "    for reason in paper['why_important']:\n",
    "        print(f\"   {reason}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìÖ SUGGESTED 4-WEEK READING SCHEDULE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "weekly_schedule = [\n",
    "    {\n",
    "        \"week\": \"Week 1: Foundation Concepts\",\n",
    "        \"papers\": [\n",
    "            \"Auto-Encoding Variational Bayes (VAE)\",\n",
    "            \"Attention Is All You Need (Transformers)\"\n",
    "        ],\n",
    "        \"goals\": [\n",
    "            \"Understand latent space representation\",\n",
    "            \"Master attention mechanisms\", \n",
    "            \"Learn encoder-decoder architectures\"\n",
    "        ],\n",
    "        \"time\": \"6-8 hours\",\n",
    "        \"deliverable\": \"Implement simple VAE v√† attention t·ª´ scratch\"\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"week\": \"Week 2: Diffusion Deep Dive\",\n",
    "        \"papers\": [\n",
    "            \"Denoising Diffusion Probabilistic Models (DDPM)\"\n",
    "        ],\n",
    "        \"goals\": [\n",
    "            \"Master forward v√† reverse diffusion process\",\n",
    "            \"Understand variational bound derivation\",\n",
    "            \"Learn DDPM training v√† sampling algorithms\"\n",
    "        ],\n",
    "        \"time\": \"6-8 hours\", \n",
    "        \"deliverable\": \"Implement DDPM on toy dataset (MNIST/CIFAR)\"\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"week\": \"Week 3: Advanced Topics\",\n",
    "        \"papers\": [\n",
    "            \"CLIP (text conditioning)\",\n",
    "            \"DDIM (fast sampling)\",\n",
    "            \"Skim GAN v√† VQGAN papers\"\n",
    "        ],\n",
    "        \"goals\": [\n",
    "            \"Understand text-image joint embeddings\",\n",
    "            \"Learn faster sampling techniques\",\n",
    "            \"Comparison v·ªõi other generative models\"\n",
    "        ],\n",
    "        \"time\": \"5-7 hours\",\n",
    "        \"deliverable\": \"Add text conditioning to diffusion model\"\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"week\": \"Week 4: Latent Diffusion Models\",\n",
    "        \"papers\": [\n",
    "            \"High-Resolution Image Synthesis with Latent Diffusion Models\",\n",
    "            \"Re-read key sections t·ª´ previous papers\"\n",
    "        ],\n",
    "        \"goals\": [\n",
    "            \"üéØ MASTER THE TARGET PAPER\",\n",
    "            \"Connect all concepts together\",\n",
    "            \"Understand practical implementation details\"\n",
    "        ],\n",
    "        \"time\": \"8-10 hours\",\n",
    "        \"deliverable\": \"Complete understanding + implementation plan\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for week in weekly_schedule:\n",
    "    print(f\"\\nüìÖ {week['week']}\")\n",
    "    print(f\"üìö Papers:\")\n",
    "    for paper in week['papers']:\n",
    "        print(f\"   ‚Ä¢ {paper}\")\n",
    "    \n",
    "    print(f\"üéØ Goals:\")\n",
    "    for goal in week['goals']:\n",
    "        print(f\"   ‚Ä¢ {goal}\")\n",
    "    \n",
    "    print(f\"‚è±Ô∏è Time: {week['time']}\")\n",
    "    print(f\"üìù Deliverable: {week['deliverable']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üß© CONCEPT DEPENDENCY MAP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "dependency_map = {\n",
    "    \"Latent Diffusion Models\": {\n",
    "        \"depends_on\": [\"VAE\", \"DDPM\", \"Transformers\"],\n",
    "        \"enables\": \"High-res image synthesis trong latent space\"\n",
    "    },\n",
    "    \"VAE\": {\n",
    "        \"depends_on\": [\"Variational Inference\", \"Neural Networks\"],\n",
    "        \"enables\": \"Latent space representation cho images\"\n",
    "    },\n",
    "    \"DDPM\": {\n",
    "        \"depends_on\": [\"Markov Chains\", \"Variational Bounds\"],\n",
    "        \"enables\": \"Iterative denoising generation process\"\n",
    "    },\n",
    "    \"Transformers\": {\n",
    "        \"depends_on\": [\"Attention Mechanism\", \"Deep Learning\"],\n",
    "        \"enables\": \"Cross-attention cho text conditioning\"\n",
    "    },\n",
    "    \"CLIP\": {\n",
    "        \"depends_on\": [\"Transformers\", \"Contrastive Learning\"],\n",
    "        \"enables\": \"Text-image joint understanding\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üîÑ How concepts build on each other:\")\n",
    "for concept, info in dependency_map.items():\n",
    "    print(f\"\\n{concept}:\")\n",
    "    print(f\"   Depends on: {', '.join(info['depends_on'])}\")\n",
    "    print(f\"   Enables: {info['enables']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ SUCCESS CHECKLIST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "success_checklist = [\n",
    "    \"‚úÖ Understand forward diffusion: x‚ÇÄ ‚Üí xT (adding noise)\",\n",
    "    \"‚úÖ Understand reverse diffusion: xT ‚Üí x‚ÇÄ (denoising)\",\n",
    "    \"‚úÖ Know why work trong latent space instead of pixel space\",\n",
    "    \"‚úÖ Understand VAE encoder: x ‚Üí z v√† decoder: z ‚Üí x\", \n",
    "    \"‚úÖ Know how cross-attention injects text conditioning\",\n",
    "    \"‚úÖ Understand the simplified loss: ||Œµ - ŒµŒ∏(zt,t,c)||¬≤\",\n",
    "    \"‚úÖ Can explain classifier-free guidance\",\n",
    "    \"‚úÖ Know differences gi·ªØa DDPM v√† DDIM sampling\",\n",
    "    \"‚úÖ Understand computational advantages c·ªßa LDM\",\n",
    "    \"‚úÖ Can implement basic components t·ª´ scratch\"\n",
    "]\n",
    "\n",
    "print(\"After completing this roadmap, b·∫°n should:\")\n",
    "for item in success_checklist:\n",
    "    print(f\"   {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üí° READING STRATEGIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "reading_tips = [\n",
    "    \"üìñ First pass: Skim ƒë·ªÉ get big picture\",\n",
    "    \"üìù Second pass: Deep read v·ªõi note-taking\",\n",
    "    \"üî¢ Focus on key equations v√† their intuitions\",\n",
    "    \"üñºÔ∏è Draw diagrams cho architectures v√† data flows\",\n",
    "    \"üíª Implement toy versions ƒë·ªÉ test understanding\",\n",
    "    \"ü§î Ask yourself: 'Why did they make this choice?'\",\n",
    "    \"üîó Connect concepts across papers\",\n",
    "    \"‚è∏Ô∏è Take breaks khi encounter difficult sections\",\n",
    "    \"üë• Discuss v·ªõi others ho·∫∑c online communities\",\n",
    "    \"üîÑ Revisit difficult concepts multiple times\"\n",
    "]\n",
    "\n",
    "for tip in reading_tips:\n",
    "    print(f\"   {tip}\")\n",
    "\n",
    "print(\"\\nüöÄ START WITH VAE PAPER - IT'S THE MOST ACCESSIBLE!\")\n",
    "print(\"Then move to Transformers, followed by DDPM.\")\n",
    "print(\"Good luck on your journey to understanding Latent Diffusion Models! üéØ‚ú®\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ec123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
