{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b058a296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 CÔNG THỨC TOÁN HỌC CẦN HIỂU TRƯỚC KHI ĐỌC STABLE DIFFUSION\n",
      "======================================================================\n",
      "🎯 Chia thành 4 mức độ: CƠ BẢN → TRUNG BÌNH → NÂNG CAO → CHUYÊN SÂU\n",
      "\n",
      "======================================================================\n",
      "📊 MỨC 1: TOÁN CƠ BẢN (BẮT BUỘC)\n",
      "======================================================================\n",
      "\n",
      "🔸 1. Vector và Matrix:\n",
      "   • Vector: [x₁, x₂, x₃, ...] - danh sách số\n",
      "   • Matrix multiplication: A × B\n",
      "   • Transpose: Aᵀ (lật ma trận)\n",
      "   • Dot product: a · b = a₁b₁ + a₂b₂ + ...\n",
      "   • Norm: ||x|| = √(x₁² + x₂² + ... + xₙ²)\n",
      "\n",
      "🔸 2. Đạo hàm (Derivatives):\n",
      "   • df/dx - tốc độ thay đổi của f theo x\n",
      "   • Chain rule: d/dx[f(g(x))] = f'(g(x)) · g'(x)\n",
      "   • Partial derivatives: ∂f/∂x (đạo hàm riêng)\n",
      "   • Gradient: ∇f = [∂f/∂x₁, ∂f/∂x₂, ...] (vector đạo hàm)\n",
      "\n",
      "🔸 3. Xác suất cơ bản:\n",
      "   • P(A) - xác suất sự kiện A xảy ra (0 ≤ P(A) ≤ 1)\n",
      "   • P(A|B) - xác suất có điều kiện (A xảy ra khi biết B)\n",
      "   • E[X] - kỳ vọng (expected value, giá trị trung bình)\n",
      "   • Var(X) - phương sai (variance, độ phân tán)\n",
      "\n",
      "🔸 4. Phân phối Gaussian:\n",
      "   • N(μ, σ²) - phân phối chuẩn với mean μ, variance σ²\n",
      "   • N(0, 1) - phân phối chuẩn tắc (mean=0, variance=1)\n",
      "   • Hình dạng: đường cong hình chuông\n",
      "   • 68% dữ liệu nằm trong ±1σ từ μ\n",
      "\n",
      "💡 KIỂM TRA MỨC 1:\n",
      "   ❓ Bạn có biết vector [1,2,3] · [4,5,6] = ?\n",
      "   ❓ N(0,1) có nghĩa là gì?\n",
      "   ❓ E[X] khác Var(X) như thế nào?\n",
      "\n",
      "======================================================================\n",
      "🧮 MỨC 2: MACHINE LEARNING CƠ BẢN (QUAN TRỌNG)\n",
      "======================================================================\n",
      "\n",
      "🔸 1. Loss Functions:\n",
      "   • L2 Loss: ||y - ŷ||² = (y₁-ŷ₁)² + (y₂-ŷ₂)² + ...\n",
      "   • L1 Loss: ||y - ŷ||₁ = |y₁-ŷ₁| + |y₂-ŷ₂| + ...\n",
      "   • Cross-entropy: -∑ y log(ŷ)\n",
      "   • MSE (Mean Squared Error): 1/n ∑(yᵢ - ŷᵢ)²\n",
      "\n",
      "🔸 2. Neural Networks:\n",
      "   • Forward pass: y = f(Wx + b)\n",
      "   • Activation functions: ReLU(x) = max(0,x), Sigmoid, etc\n",
      "   • Backpropagation: ∂Loss/∂W (gradient để update weights)\n",
      "   • Optimizer: W ← W - α∇W (gradient descent)\n",
      "\n",
      "🔸 3. Convolution:\n",
      "   • Conv2D: (I * K)[i,j] = ∑∑ I[i+m,j+n] × K[m,n]\n",
      "   • Kernel/Filter: ma trận nhỏ scan qua image\n",
      "   • Stride: bước nhảy của kernel\n",
      "   • Padding: thêm 0 vào biên để giữ kích thước\n",
      "\n",
      "🔸 4. Regularization:\n",
      "   • L1 Reg: Loss + λ∑|wᵢ| (khuyến khích sparse weights)\n",
      "   • L2 Reg: Loss + λ∑wᵢ² (khuyến khích small weights)\n",
      "   • Dropout: randomly set some neurons = 0\n",
      "   • Batch Norm: normalize inputs của mỗi layer\n",
      "\n",
      "💡 KIỂM TRA MỨC 2:\n",
      "   ❓ ReLU(x) với x = [-2, 0, 3] ra kết quả gì?\n",
      "   ❓ Convolution 3x3 kernel trên 5x5 image ra kích thước gì?\n",
      "   ❓ Backpropagation dùng để làm gì?\n",
      "\n",
      "======================================================================\n",
      "🎭 MỨC 3: DIFFUSION MODELS (TRỌNG TÂM)\n",
      "======================================================================\n",
      "\n",
      "🔸 1. VAE (Variational AutoEncoder):\n",
      "   • q(z|x) = N(μ(x), σ²(x)) - encoder tạo phân phối\n",
      "   • p(x|z) - decoder tái tạo từ latent z\n",
      "   • KL divergence: KL[q||p] = ∫ q(x)log(q(x)/p(x))dx\n",
      "   • Reparameterization: z = μ + ε·σ với ε ~ N(0,1)\n",
      "   • ELBO: E[log p(x|z)] - KL[q(z|x)||p(z)]\n",
      "\n",
      "🔸 2. Forward Process (Thêm nhiễu):\n",
      "   • q(xₜ|xₜ₋₁) = N(xₜ; √(1-βₜ)xₜ₋₁, βₜI)\n",
      "   • βₜ - noise schedule (tăng dần từ 0.0001 → 0.02)\n",
      "   • αₜ = 1 - βₜ\n",
      "   • ᾱₜ = ∏ᵢ₌₁ᵗ αᵢ (cumulative product)\n",
      "   • xₜ = √ᾱₜ x₀ + √(1-ᾱₜ) ε với ε ~ N(0,I)\n",
      "\n",
      "🔸 3. Reverse Process (Khử nhiễu):\n",
      "   • p(xₜ₋₁|xₜ) = N(xₜ₋₁; μθ(xₜ,t), Σθ(xₜ,t))\n",
      "   • Model học dự đoán μθ(xₜ,t)\n",
      "   • Thực tế: model dự đoán noise εθ(xₜ,t)\n",
      "   • μθ = 1/√αₜ (xₜ - βₜ/√(1-ᾱₜ) εθ(xₜ,t))\n",
      "\n",
      "🔸 4. Training Loss:\n",
      "   • LDM = E[||ε - εθ(xₜ,t)||²]\n",
      "   • ε ~ N(0,I) - true noise added\n",
      "   • εθ(xₜ,t) - predicted noise by model\n",
      "   • t ~ Uniform{1,...,T} - random timestep\n",
      "\n",
      "💡 KIỂM TRA MỨC 3:\n",
      "   ❓ Tại sao dự đoán noise ε thay vì dự đoán ảnh x₀?\n",
      "   ❓ Forward process có cần training không?\n",
      "   ❓ ᾱₜ giảm hay tăng theo t?\n",
      "\n",
      "======================================================================\n",
      "🚀 MỨC 4: STABLE DIFFUSION NÂNG CAO (TÙY CHỌN)\n",
      "======================================================================\n",
      "\n",
      "🔸 1. Attention Mechanism:\n",
      "   • Attention(Q,K,V) = softmax(QKᵀ/√d)V\n",
      "   • Q = query, K = key, V = value matrices\n",
      "   • Self-attention: Q,K,V cùng từ 1 source\n",
      "   • Cross-attention: Q từ image, K,V từ text\n",
      "   • Multi-head: concat nhiều attention heads\n",
      "\n",
      "🔸 2. CLIP Loss:\n",
      "   • Contrastive loss giữa text và image embeddings\n",
      "   • S = I @ Tᵀ (similarity matrix)\n",
      "   • L = CE(S, labels) + CE(Sᵀ, labels)\n",
      "   • CE = cross-entropy loss\n",
      "\n",
      "🔸 3. Classifier-Free Guidance:\n",
      "   • ε̃ = ε_uncond + s(ε_cond - ε_uncond)\n",
      "   • s = guidance scale (thường 7.5)\n",
      "   • ε_cond = εθ(xₜ, t, c) với conditioning c\n",
      "   • ε_uncond = εθ(xₜ, t, ∅) không conditioning\n",
      "\n",
      "🔸 4. Perceptual Loss:\n",
      "   • L_perceptual = ||φ(x) - φ(x̂)||²\n",
      "   • φ(·) = VGG features extractor\n",
      "   • So sánh ở feature space thay vì pixel space\n",
      "   • Multi-scale: L = Σλᵢ||φᵢ(x) - φᵢ(x̂)||²\n",
      "\n",
      "======================================================================\n",
      "✅ KIỂM TRA TỔNG THỂ\n",
      "======================================================================\n",
      "CHECKLIST - Bạn cần tick ít nhất 6/8 items:\n",
      "   □ Hiểu vector, matrix, đạo hàm cơ bản\n",
      "   □ Biết xác suất, phân phối Gaussian N(μ,σ²)\n",
      "   □ Quen với neural networks, convolution\n",
      "   □ Hiểu loss functions, backpropagation\n",
      "   □ Nắm được VAE: encoder q(z|x), decoder p(x|z)\n",
      "   □ Hiểu diffusion: forward process (add noise), reverse (denoise)\n",
      "   □ Biết attention mechanism cơ bản\n",
      "   □ Quen với CLIP và cross-attention\n",
      "\n",
      "======================================================================\n",
      "🎯 KHUYẾN NGHỊ HỌC TẬP\n",
      "======================================================================\n",
      "\n",
      "📚 NẾU BẠN CHƯA BIẾT GÌ:\n",
      "   1. Học Linear Algebra (Khan Academy)\n",
      "   2. Calculus cơ bản (derivatives)\n",
      "   3. Probability & Statistics\n",
      "   4. Neural Networks basics (3Blue1Brown)\n",
      "\n",
      "🧠 NẾU BẠN BIẾT CƠ BẢN:\n",
      "   1. Hiểu VAE trước (Understanding VAE paper)\n",
      "   2. Đọc DDPM paper (phần 2,3)\n",
      "   3. Xem video giải thích Diffusion Models\n",
      "   4. Thực hành code VAE đơn giản\n",
      "\n",
      "🚀 NẾU BẠN ĐÃ BIẾT ML:\n",
      "   1. Đi thẳng vào Diffusion Models\n",
      "   2. Focus vào Stable Diffusion paper\n",
      "   3. Implement từng component\n",
      "   4. Test với datasets nhỏ\n",
      "\n",
      "💪 NẾU BẠN PRO:\n",
      "   1. Đọc paper gốc\n",
      "   2. Reproduce results\n",
      "   3. Modify và experiment\n",
      "   4. Share knowledge với community\n",
      "\n",
      "\n",
      "💡 LỜI KHUYÊN CUỐI:\n",
      "• ĐỪNG SỢ CÔNG THỨC! Hầu hết chỉ là notation fancy\n",
      "• Hiểu intuition trước, công thức sau\n",
      "• Code helps understanding - implement để hiểu sâu\n",
      "• Ask questions - AI community rất helpful!\n",
      "\n",
      "🎯 Ready to conquer Stable Diffusion! 🚀\n"
     ]
    }
   ],
   "source": [
    "print(\"📚 CÔNG THỨC TOÁN HỌC CẦN HIỂU TRƯỚC KHI ĐỌC STABLE DIFFUSION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"🎯 Chia thành 4 mức độ: CƠ BẢN → TRUNG BÌNH → NÂNG CAO → CHUYÊN SÂU\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📊 MỨC 1: TOÁN CƠ BẢN (BẮT BUỘC)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "basic_math = {\n",
    "    \"1. Vector và Matrix\": [\n",
    "        \"• Vector: [x₁, x₂, x₃, ...] - danh sách số\",\n",
    "        \"• Matrix multiplication: A × B\",\n",
    "        \"• Transpose: Aᵀ (lật ma trận)\",\n",
    "        \"• Dot product: a · b = a₁b₁ + a₂b₂ + ...\",\n",
    "        \"• Norm: ||x|| = √(x₁² + x₂² + ... + xₙ²)\"\n",
    "    ],\n",
    "    \n",
    "    \"2. Đạo hàm (Derivatives)\": [\n",
    "        \"• df/dx - tốc độ thay đổi của f theo x\",\n",
    "        \"• Chain rule: d/dx[f(g(x))] = f'(g(x)) · g'(x)\",\n",
    "        \"• Partial derivatives: ∂f/∂x (đạo hàm riêng)\",\n",
    "        \"• Gradient: ∇f = [∂f/∂x₁, ∂f/∂x₂, ...] (vector đạo hàm)\"\n",
    "    ],\n",
    "    \n",
    "    \"3. Xác suất cơ bản\": [\n",
    "        \"• P(A) - xác suất sự kiện A xảy ra (0 ≤ P(A) ≤ 1)\",\n",
    "        \"• P(A|B) - xác suất có điều kiện (A xảy ra khi biết B)\",\n",
    "        \"• E[X] - kỳ vọng (expected value, giá trị trung bình)\",\n",
    "        \"• Var(X) - phương sai (variance, độ phân tán)\"\n",
    "    ],\n",
    "    \n",
    "    \"4. Phân phối Gaussian\": [\n",
    "        \"• N(μ, σ²) - phân phối chuẩn với mean μ, variance σ²\",\n",
    "        \"• N(0, 1) - phân phối chuẩn tắc (mean=0, variance=1)\",\n",
    "        \"• Hình dạng: đường cong hình chuông\",\n",
    "        \"• 68% dữ liệu nằm trong ±1σ từ μ\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, formulas in basic_math.items():\n",
    "    print(f\"\\n🔸 {category}:\")\n",
    "    for formula in formulas:\n",
    "        print(f\"   {formula}\")\n",
    "\n",
    "print(\"\\n💡 KIỂM TRA MỨC 1:\")\n",
    "print(\"   ❓ Bạn có biết vector [1,2,3] · [4,5,6] = ?\")\n",
    "print(\"   ❓ N(0,1) có nghĩa là gì?\")\n",
    "print(\"   ❓ E[X] khác Var(X) như thế nào?\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🧮 MỨC 2: MACHINE LEARNING CƠ BẢN (QUAN TRỌNG)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ml_math = {\n",
    "    \"1. Loss Functions\": [\n",
    "        \"• L2 Loss: ||y - ŷ||² = (y₁-ŷ₁)² + (y₂-ŷ₂)² + ...\",\n",
    "        \"• L1 Loss: ||y - ŷ||₁ = |y₁-ŷ₁| + |y₂-ŷ₂| + ...\",\n",
    "        \"• Cross-entropy: -∑ y log(ŷ)\",\n",
    "        \"• MSE (Mean Squared Error): 1/n ∑(yᵢ - ŷᵢ)²\"\n",
    "    ],\n",
    "    \n",
    "    \"2. Neural Networks\": [\n",
    "        \"• Forward pass: y = f(Wx + b)\",\n",
    "        \"• Activation functions: ReLU(x) = max(0,x), Sigmoid, etc\",\n",
    "        \"• Backpropagation: ∂Loss/∂W (gradient để update weights)\",\n",
    "        \"• Optimizer: W ← W - α∇W (gradient descent)\"\n",
    "    ],\n",
    "    \n",
    "    \"3. Convolution\": [\n",
    "        \"• Conv2D: (I * K)[i,j] = ∑∑ I[i+m,j+n] × K[m,n]\",\n",
    "        \"• Kernel/Filter: ma trận nhỏ scan qua image\",\n",
    "        \"• Stride: bước nhảy của kernel\",\n",
    "        \"• Padding: thêm 0 vào biên để giữ kích thước\"\n",
    "    ],\n",
    "    \n",
    "    \"4. Regularization\": [\n",
    "        \"• L1 Reg: Loss + λ∑|wᵢ| (khuyến khích sparse weights)\",\n",
    "        \"• L2 Reg: Loss + λ∑wᵢ² (khuyến khích small weights)\",\n",
    "        \"• Dropout: randomly set some neurons = 0\",\n",
    "        \"• Batch Norm: normalize inputs của mỗi layer\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, formulas in ml_math.items():\n",
    "    print(f\"\\n🔸 {category}:\")\n",
    "    for formula in formulas:\n",
    "        print(f\"   {formula}\")\n",
    "\n",
    "print(\"\\n💡 KIỂM TRA MỨC 2:\")\n",
    "print(\"   ❓ ReLU(x) với x = [-2, 0, 3] ra kết quả gì?\")\n",
    "print(\"   ❓ Convolution 3x3 kernel trên 5x5 image ra kích thước gì?\")\n",
    "print(\"   ❓ Backpropagation dùng để làm gì?\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🎭 MỨC 3: DIFFUSION MODELS (TRỌNG TÂM)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "diffusion_math = {\n",
    "    \"1. VAE (Variational AutoEncoder)\": [\n",
    "        \"• q(z|x) = N(μ(x), σ²(x)) - encoder tạo phân phối\",\n",
    "        \"• p(x|z) - decoder tái tạo từ latent z\",\n",
    "        \"• KL divergence: KL[q||p] = ∫ q(x)log(q(x)/p(x))dx\",\n",
    "        \"• Reparameterization: z = μ + ε·σ với ε ~ N(0,1)\",\n",
    "        \"• ELBO: E[log p(x|z)] - KL[q(z|x)||p(z)]\"\n",
    "    ],\n",
    "    \n",
    "    \"2. Forward Process (Thêm nhiễu)\": [\n",
    "        \"• q(xₜ|xₜ₋₁) = N(xₜ; √(1-βₜ)xₜ₋₁, βₜI)\",\n",
    "        \"• βₜ - noise schedule (tăng dần từ 0.0001 → 0.02)\",\n",
    "        \"• αₜ = 1 - βₜ\",\n",
    "        \"• ᾱₜ = ∏ᵢ₌₁ᵗ αᵢ (cumulative product)\",\n",
    "        \"• xₜ = √ᾱₜ x₀ + √(1-ᾱₜ) ε với ε ~ N(0,I)\"\n",
    "    ],\n",
    "    \n",
    "    \"3. Reverse Process (Khử nhiễu)\": [\n",
    "        \"• p(xₜ₋₁|xₜ) = N(xₜ₋₁; μθ(xₜ,t), Σθ(xₜ,t))\",\n",
    "        \"• Model học dự đoán μθ(xₜ,t)\",\n",
    "        \"• Thực tế: model dự đoán noise εθ(xₜ,t)\",\n",
    "        \"• μθ = 1/√αₜ (xₜ - βₜ/√(1-ᾱₜ) εθ(xₜ,t))\"\n",
    "    ],\n",
    "    \n",
    "    \"4. Training Loss\": [\n",
    "        \"• LDM = E[||ε - εθ(xₜ,t)||²]\",\n",
    "        \"• ε ~ N(0,I) - true noise added\",\n",
    "        \"• εθ(xₜ,t) - predicted noise by model\",\n",
    "        \"• t ~ Uniform{1,...,T} - random timestep\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, formulas in diffusion_math.items():\n",
    "    print(f\"\\n🔸 {category}:\")\n",
    "    for formula in formulas:\n",
    "        print(f\"   {formula}\")\n",
    "\n",
    "print(\"\\n💡 KIỂM TRA MỨC 3:\")\n",
    "print(\"   ❓ Tại sao dự đoán noise ε thay vì dự đoán ảnh x₀?\")\n",
    "print(\"   ❓ Forward process có cần training không?\")\n",
    "print(\"   ❓ ᾱₜ giảm hay tăng theo t?\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🚀 MỨC 4: STABLE DIFFUSION NÂNG CAO (TÙY CHỌN)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "advanced_math = {\n",
    "    \"1. Attention Mechanism\": [\n",
    "        \"• Attention(Q,K,V) = softmax(QKᵀ/√d)V\",\n",
    "        \"• Q = query, K = key, V = value matrices\",\n",
    "        \"• Self-attention: Q,K,V cùng từ 1 source\",\n",
    "        \"• Cross-attention: Q từ image, K,V từ text\",\n",
    "        \"• Multi-head: concat nhiều attention heads\"\n",
    "    ],\n",
    "    \n",
    "    \"2. CLIP Loss\": [\n",
    "        \"• Contrastive loss giữa text và image embeddings\",\n",
    "        \"• S = I @ Tᵀ (similarity matrix)\",\n",
    "        \"• L = CE(S, labels) + CE(Sᵀ, labels)\",\n",
    "        \"• CE = cross-entropy loss\"\n",
    "    ],\n",
    "    \n",
    "    \"3. Classifier-Free Guidance\": [\n",
    "        \"• ε̃ = ε_uncond + s(ε_cond - ε_uncond)\",\n",
    "        \"• s = guidance scale (thường 7.5)\",\n",
    "        \"• ε_cond = εθ(xₜ, t, c) với conditioning c\",\n",
    "        \"• ε_uncond = εθ(xₜ, t, ∅) không conditioning\"\n",
    "    ],\n",
    "    \n",
    "    \"4. Perceptual Loss\": [\n",
    "        \"• L_perceptual = ||φ(x) - φ(x̂)||²\",\n",
    "        \"• φ(·) = VGG features extractor\",\n",
    "        \"• So sánh ở feature space thay vì pixel space\",\n",
    "        \"• Multi-scale: L = Σλᵢ||φᵢ(x) - φᵢ(x̂)||²\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, formulas in advanced_math.items():\n",
    "    print(f\"\\n🔸 {category}:\")\n",
    "    for formula in formulas:\n",
    "        print(f\"   {formula}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ KIỂM TRA TỔNG THỂ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "checklist = [\n",
    "    \"□ Hiểu vector, matrix, đạo hàm cơ bản\",\n",
    "    \"□ Biết xác suất, phân phối Gaussian N(μ,σ²)\", \n",
    "    \"□ Quen với neural networks, convolution\",\n",
    "    \"□ Hiểu loss functions, backpropagation\",\n",
    "    \"□ Nắm được VAE: encoder q(z|x), decoder p(x|z)\",\n",
    "    \"□ Hiểu diffusion: forward process (add noise), reverse (denoise)\",\n",
    "    \"□ Biết attention mechanism cơ bản\",\n",
    "    \"□ Quen với CLIP và cross-attention\"\n",
    "]\n",
    "\n",
    "print(\"CHECKLIST - Bạn cần tick ít nhất 6/8 items:\")\n",
    "for item in checklist:\n",
    "    print(f\"   {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🎯 KHUYẾN NGHỊ HỌC TẬP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "recommendations = \"\"\"\n",
    "📚 NẾU BẠN CHƯA BIẾT GÌ:\n",
    "   1. Học Linear Algebra (Khan Academy)\n",
    "   2. Calculus cơ bản (derivatives)\n",
    "   3. Probability & Statistics\n",
    "   4. Neural Networks basics (3Blue1Brown)\n",
    "\n",
    "🧠 NẾU BẠN BIẾT CƠ BẢN:\n",
    "   1. Hiểu VAE trước (Understanding VAE paper)\n",
    "   2. Đọc DDPM paper (phần 2,3)\n",
    "   3. Xem video giải thích Diffusion Models\n",
    "   4. Thực hành code VAE đơn giản\n",
    "\n",
    "🚀 NẾU BẠN ĐÃ BIẾT ML:\n",
    "   1. Đi thẳng vào Diffusion Models\n",
    "   2. Focus vào Stable Diffusion paper\n",
    "   3. Implement từng component\n",
    "   4. Test với datasets nhỏ\n",
    "\n",
    "💪 NẾU BẠN PRO:\n",
    "   1. Đọc paper gốc\n",
    "   2. Reproduce results\n",
    "   3. Modify và experiment\n",
    "   4. Share knowledge với community\n",
    "\"\"\"\n",
    "\n",
    "print(recommendations)\n",
    "\n",
    "print(\"\\n💡 LỜI KHUYÊN CUỐI:\")\n",
    "print(\"• ĐỪNG SỢ CÔNG THỨC! Hầu hết chỉ là notation fancy\")\n",
    "print(\"• Hiểu intuition trước, công thức sau\") \n",
    "print(\"• Code helps understanding - implement để hiểu sâu\")\n",
    "print(\"• Ask questions - AI community rất helpful!\")\n",
    "print(\"\\n🎯 Ready to conquer Stable Diffusion! 🚀\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
