{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b058a296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š CÃ”NG THá»¨C TOÃN Há»ŒC Cáº¦N HIá»‚U TRÆ¯á»šC KHI Äá»ŒC STABLE DIFFUSION\n",
      "======================================================================\n",
      "ğŸ¯ Chia thÃ nh 4 má»©c Ä‘á»™: CÆ  Báº¢N â†’ TRUNG BÃŒNH â†’ NÃ‚NG CAO â†’ CHUYÃŠN SÃ‚U\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š Má»¨C 1: TOÃN CÆ  Báº¢N (Báº®T BUá»˜C)\n",
      "======================================================================\n",
      "\n",
      "ğŸ”¸ 1. Vector vÃ  Matrix:\n",
      "   â€¢ Vector: [xâ‚, xâ‚‚, xâ‚ƒ, ...] - danh sÃ¡ch sá»‘\n",
      "   â€¢ Matrix multiplication: A Ã— B\n",
      "   â€¢ Transpose: Aáµ€ (láº­t ma tráº­n)\n",
      "   â€¢ Dot product: a Â· b = aâ‚bâ‚ + aâ‚‚bâ‚‚ + ...\n",
      "   â€¢ Norm: ||x|| = âˆš(xâ‚Â² + xâ‚‚Â² + ... + xâ‚™Â²)\n",
      "\n",
      "ğŸ”¸ 2. Äáº¡o hÃ m (Derivatives):\n",
      "   â€¢ df/dx - tá»‘c Ä‘á»™ thay Ä‘á»•i cá»§a f theo x\n",
      "   â€¢ Chain rule: d/dx[f(g(x))] = f'(g(x)) Â· g'(x)\n",
      "   â€¢ Partial derivatives: âˆ‚f/âˆ‚x (Ä‘áº¡o hÃ m riÃªng)\n",
      "   â€¢ Gradient: âˆ‡f = [âˆ‚f/âˆ‚xâ‚, âˆ‚f/âˆ‚xâ‚‚, ...] (vector Ä‘áº¡o hÃ m)\n",
      "\n",
      "ğŸ”¸ 3. XÃ¡c suáº¥t cÆ¡ báº£n:\n",
      "   â€¢ P(A) - xÃ¡c suáº¥t sá»± kiá»‡n A xáº£y ra (0 â‰¤ P(A) â‰¤ 1)\n",
      "   â€¢ P(A|B) - xÃ¡c suáº¥t cÃ³ Ä‘iá»u kiá»‡n (A xáº£y ra khi biáº¿t B)\n",
      "   â€¢ E[X] - ká»³ vá»ng (expected value, giÃ¡ trá»‹ trung bÃ¬nh)\n",
      "   â€¢ Var(X) - phÆ°Æ¡ng sai (variance, Ä‘á»™ phÃ¢n tÃ¡n)\n",
      "\n",
      "ğŸ”¸ 4. PhÃ¢n phá»‘i Gaussian:\n",
      "   â€¢ N(Î¼, ÏƒÂ²) - phÃ¢n phá»‘i chuáº©n vá»›i mean Î¼, variance ÏƒÂ²\n",
      "   â€¢ N(0, 1) - phÃ¢n phá»‘i chuáº©n táº¯c (mean=0, variance=1)\n",
      "   â€¢ HÃ¬nh dáº¡ng: Ä‘Æ°á»ng cong hÃ¬nh chuÃ´ng\n",
      "   â€¢ 68% dá»¯ liá»‡u náº±m trong Â±1Ïƒ tá»« Î¼\n",
      "\n",
      "ğŸ’¡ KIá»‚M TRA Má»¨C 1:\n",
      "   â“ Báº¡n cÃ³ biáº¿t vector [1,2,3] Â· [4,5,6] = ?\n",
      "   â“ N(0,1) cÃ³ nghÄ©a lÃ  gÃ¬?\n",
      "   â“ E[X] khÃ¡c Var(X) nhÆ° tháº¿ nÃ o?\n",
      "\n",
      "======================================================================\n",
      "ğŸ§® Má»¨C 2: MACHINE LEARNING CÆ  Báº¢N (QUAN TRá»ŒNG)\n",
      "======================================================================\n",
      "\n",
      "ğŸ”¸ 1. Loss Functions:\n",
      "   â€¢ L2 Loss: ||y - Å·||Â² = (yâ‚-Å·â‚)Â² + (yâ‚‚-Å·â‚‚)Â² + ...\n",
      "   â€¢ L1 Loss: ||y - Å·||â‚ = |yâ‚-Å·â‚| + |yâ‚‚-Å·â‚‚| + ...\n",
      "   â€¢ Cross-entropy: -âˆ‘ y log(Å·)\n",
      "   â€¢ MSE (Mean Squared Error): 1/n âˆ‘(yáµ¢ - Å·áµ¢)Â²\n",
      "\n",
      "ğŸ”¸ 2. Neural Networks:\n",
      "   â€¢ Forward pass: y = f(Wx + b)\n",
      "   â€¢ Activation functions: ReLU(x) = max(0,x), Sigmoid, etc\n",
      "   â€¢ Backpropagation: âˆ‚Loss/âˆ‚W (gradient Ä‘á»ƒ update weights)\n",
      "   â€¢ Optimizer: W â† W - Î±âˆ‡W (gradient descent)\n",
      "\n",
      "ğŸ”¸ 3. Convolution:\n",
      "   â€¢ Conv2D: (I * K)[i,j] = âˆ‘âˆ‘ I[i+m,j+n] Ã— K[m,n]\n",
      "   â€¢ Kernel/Filter: ma tráº­n nhá» scan qua image\n",
      "   â€¢ Stride: bÆ°á»›c nháº£y cá»§a kernel\n",
      "   â€¢ Padding: thÃªm 0 vÃ o biÃªn Ä‘á»ƒ giá»¯ kÃ­ch thÆ°á»›c\n",
      "\n",
      "ğŸ”¸ 4. Regularization:\n",
      "   â€¢ L1 Reg: Loss + Î»âˆ‘|wáµ¢| (khuyáº¿n khÃ­ch sparse weights)\n",
      "   â€¢ L2 Reg: Loss + Î»âˆ‘wáµ¢Â² (khuyáº¿n khÃ­ch small weights)\n",
      "   â€¢ Dropout: randomly set some neurons = 0\n",
      "   â€¢ Batch Norm: normalize inputs cá»§a má»—i layer\n",
      "\n",
      "ğŸ’¡ KIá»‚M TRA Má»¨C 2:\n",
      "   â“ ReLU(x) vá»›i x = [-2, 0, 3] ra káº¿t quáº£ gÃ¬?\n",
      "   â“ Convolution 3x3 kernel trÃªn 5x5 image ra kÃ­ch thÆ°á»›c gÃ¬?\n",
      "   â“ Backpropagation dÃ¹ng Ä‘á»ƒ lÃ m gÃ¬?\n",
      "\n",
      "======================================================================\n",
      "ğŸ­ Má»¨C 3: DIFFUSION MODELS (TRá»ŒNG TÃ‚M)\n",
      "======================================================================\n",
      "\n",
      "ğŸ”¸ 1. VAE (Variational AutoEncoder):\n",
      "   â€¢ q(z|x) = N(Î¼(x), ÏƒÂ²(x)) - encoder táº¡o phÃ¢n phá»‘i\n",
      "   â€¢ p(x|z) - decoder tÃ¡i táº¡o tá»« latent z\n",
      "   â€¢ KL divergence: KL[q||p] = âˆ« q(x)log(q(x)/p(x))dx\n",
      "   â€¢ Reparameterization: z = Î¼ + ÎµÂ·Ïƒ vá»›i Îµ ~ N(0,1)\n",
      "   â€¢ ELBO: E[log p(x|z)] - KL[q(z|x)||p(z)]\n",
      "\n",
      "ğŸ”¸ 2. Forward Process (ThÃªm nhiá»…u):\n",
      "   â€¢ q(xâ‚œ|xâ‚œâ‚‹â‚) = N(xâ‚œ; âˆš(1-Î²â‚œ)xâ‚œâ‚‹â‚, Î²â‚œI)\n",
      "   â€¢ Î²â‚œ - noise schedule (tÄƒng dáº§n tá»« 0.0001 â†’ 0.02)\n",
      "   â€¢ Î±â‚œ = 1 - Î²â‚œ\n",
      "   â€¢ á¾±â‚œ = âˆáµ¢â‚Œâ‚áµ— Î±áµ¢ (cumulative product)\n",
      "   â€¢ xâ‚œ = âˆšá¾±â‚œ xâ‚€ + âˆš(1-á¾±â‚œ) Îµ vá»›i Îµ ~ N(0,I)\n",
      "\n",
      "ğŸ”¸ 3. Reverse Process (Khá»­ nhiá»…u):\n",
      "   â€¢ p(xâ‚œâ‚‹â‚|xâ‚œ) = N(xâ‚œâ‚‹â‚; Î¼Î¸(xâ‚œ,t), Î£Î¸(xâ‚œ,t))\n",
      "   â€¢ Model há»c dá»± Ä‘oÃ¡n Î¼Î¸(xâ‚œ,t)\n",
      "   â€¢ Thá»±c táº¿: model dá»± Ä‘oÃ¡n noise ÎµÎ¸(xâ‚œ,t)\n",
      "   â€¢ Î¼Î¸ = 1/âˆšÎ±â‚œ (xâ‚œ - Î²â‚œ/âˆš(1-á¾±â‚œ) ÎµÎ¸(xâ‚œ,t))\n",
      "\n",
      "ğŸ”¸ 4. Training Loss:\n",
      "   â€¢ LDM = E[||Îµ - ÎµÎ¸(xâ‚œ,t)||Â²]\n",
      "   â€¢ Îµ ~ N(0,I) - true noise added\n",
      "   â€¢ ÎµÎ¸(xâ‚œ,t) - predicted noise by model\n",
      "   â€¢ t ~ Uniform{1,...,T} - random timestep\n",
      "\n",
      "ğŸ’¡ KIá»‚M TRA Má»¨C 3:\n",
      "   â“ Táº¡i sao dá»± Ä‘oÃ¡n noise Îµ thay vÃ¬ dá»± Ä‘oÃ¡n áº£nh xâ‚€?\n",
      "   â“ Forward process cÃ³ cáº§n training khÃ´ng?\n",
      "   â“ á¾±â‚œ giáº£m hay tÄƒng theo t?\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ Má»¨C 4: STABLE DIFFUSION NÃ‚NG CAO (TÃ™Y CHá»ŒN)\n",
      "======================================================================\n",
      "\n",
      "ğŸ”¸ 1. Attention Mechanism:\n",
      "   â€¢ Attention(Q,K,V) = softmax(QKáµ€/âˆšd)V\n",
      "   â€¢ Q = query, K = key, V = value matrices\n",
      "   â€¢ Self-attention: Q,K,V cÃ¹ng tá»« 1 source\n",
      "   â€¢ Cross-attention: Q tá»« image, K,V tá»« text\n",
      "   â€¢ Multi-head: concat nhiá»u attention heads\n",
      "\n",
      "ğŸ”¸ 2. CLIP Loss:\n",
      "   â€¢ Contrastive loss giá»¯a text vÃ  image embeddings\n",
      "   â€¢ S = I @ Táµ€ (similarity matrix)\n",
      "   â€¢ L = CE(S, labels) + CE(Sáµ€, labels)\n",
      "   â€¢ CE = cross-entropy loss\n",
      "\n",
      "ğŸ”¸ 3. Classifier-Free Guidance:\n",
      "   â€¢ ÎµÌƒ = Îµ_uncond + s(Îµ_cond - Îµ_uncond)\n",
      "   â€¢ s = guidance scale (thÆ°á»ng 7.5)\n",
      "   â€¢ Îµ_cond = ÎµÎ¸(xâ‚œ, t, c) vá»›i conditioning c\n",
      "   â€¢ Îµ_uncond = ÎµÎ¸(xâ‚œ, t, âˆ…) khÃ´ng conditioning\n",
      "\n",
      "ğŸ”¸ 4. Perceptual Loss:\n",
      "   â€¢ L_perceptual = ||Ï†(x) - Ï†(xÌ‚)||Â²\n",
      "   â€¢ Ï†(Â·) = VGG features extractor\n",
      "   â€¢ So sÃ¡nh á»Ÿ feature space thay vÃ¬ pixel space\n",
      "   â€¢ Multi-scale: L = Î£Î»áµ¢||Ï†áµ¢(x) - Ï†áµ¢(xÌ‚)||Â²\n",
      "\n",
      "======================================================================\n",
      "âœ… KIá»‚M TRA Tá»”NG THá»‚\n",
      "======================================================================\n",
      "CHECKLIST - Báº¡n cáº§n tick Ã­t nháº¥t 6/8 items:\n",
      "   â–¡ Hiá»ƒu vector, matrix, Ä‘áº¡o hÃ m cÆ¡ báº£n\n",
      "   â–¡ Biáº¿t xÃ¡c suáº¥t, phÃ¢n phá»‘i Gaussian N(Î¼,ÏƒÂ²)\n",
      "   â–¡ Quen vá»›i neural networks, convolution\n",
      "   â–¡ Hiá»ƒu loss functions, backpropagation\n",
      "   â–¡ Náº¯m Ä‘Æ°á»£c VAE: encoder q(z|x), decoder p(x|z)\n",
      "   â–¡ Hiá»ƒu diffusion: forward process (add noise), reverse (denoise)\n",
      "   â–¡ Biáº¿t attention mechanism cÆ¡ báº£n\n",
      "   â–¡ Quen vá»›i CLIP vÃ  cross-attention\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ KHUYáº¾N NGHá»Š Há»ŒC Táº¬P\n",
      "======================================================================\n",
      "\n",
      "ğŸ“š Náº¾U Báº N CHÆ¯A BIáº¾T GÃŒ:\n",
      "   1. Há»c Linear Algebra (Khan Academy)\n",
      "   2. Calculus cÆ¡ báº£n (derivatives)\n",
      "   3. Probability & Statistics\n",
      "   4. Neural Networks basics (3Blue1Brown)\n",
      "\n",
      "ğŸ§  Náº¾U Báº N BIáº¾T CÆ  Báº¢N:\n",
      "   1. Hiá»ƒu VAE trÆ°á»›c (Understanding VAE paper)\n",
      "   2. Äá»c DDPM paper (pháº§n 2,3)\n",
      "   3. Xem video giáº£i thÃ­ch Diffusion Models\n",
      "   4. Thá»±c hÃ nh code VAE Ä‘Æ¡n giáº£n\n",
      "\n",
      "ğŸš€ Náº¾U Báº N ÄÃƒ BIáº¾T ML:\n",
      "   1. Äi tháº³ng vÃ o Diffusion Models\n",
      "   2. Focus vÃ o Stable Diffusion paper\n",
      "   3. Implement tá»«ng component\n",
      "   4. Test vá»›i datasets nhá»\n",
      "\n",
      "ğŸ’ª Náº¾U Báº N PRO:\n",
      "   1. Äá»c paper gá»‘c\n",
      "   2. Reproduce results\n",
      "   3. Modify vÃ  experiment\n",
      "   4. Share knowledge vá»›i community\n",
      "\n",
      "\n",
      "ğŸ’¡ Lá»œI KHUYÃŠN CUá»I:\n",
      "â€¢ Äá»ªNG Sá»¢ CÃ”NG THá»¨C! Háº§u háº¿t chá»‰ lÃ  notation fancy\n",
      "â€¢ Hiá»ƒu intuition trÆ°á»›c, cÃ´ng thá»©c sau\n",
      "â€¢ Code helps understanding - implement Ä‘á»ƒ hiá»ƒu sÃ¢u\n",
      "â€¢ Ask questions - AI community ráº¥t helpful!\n",
      "\n",
      "ğŸ¯ Ready to conquer Stable Diffusion! ğŸš€\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“š CÃ”NG THá»¨C TOÃN Há»ŒC Cáº¦N HIá»‚U TRÆ¯á»šC KHI Äá»ŒC STABLE DIFFUSION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"ğŸ¯ Chia thÃ nh 4 má»©c Ä‘á»™: CÆ  Báº¢N â†’ TRUNG BÃŒNH â†’ NÃ‚NG CAO â†’ CHUYÃŠN SÃ‚U\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š Má»¨C 1: TOÃN CÆ  Báº¢N (Báº®T BUá»˜C)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "basic_math = {\n",
    "    \"1. Vector vÃ  Matrix\": [\n",
    "        \"â€¢ Vector: [xâ‚, xâ‚‚, xâ‚ƒ, ...] - danh sÃ¡ch sá»‘\",\n",
    "        \"â€¢ Matrix multiplication: A Ã— B\",\n",
    "        \"â€¢ Transpose: Aáµ€ (láº­t ma tráº­n)\",\n",
    "        \"â€¢ Dot product: a Â· b = aâ‚bâ‚ + aâ‚‚bâ‚‚ + ...\",\n",
    "        \"â€¢ Norm: ||x|| = âˆš(xâ‚Â² + xâ‚‚Â² + ... + xâ‚™Â²)\"\n",
    "    ],\n",
    "    \n",
    "    \"2. Äáº¡o hÃ m (Derivatives)\": [\n",
    "        \"â€¢ df/dx - tá»‘c Ä‘á»™ thay Ä‘á»•i cá»§a f theo x\",\n",
    "        \"â€¢ Chain rule: d/dx[f(g(x))] = f'(g(x)) Â· g'(x)\",\n",
    "        \"â€¢ Partial derivatives: âˆ‚f/âˆ‚x (Ä‘áº¡o hÃ m riÃªng)\",\n",
    "        \"â€¢ Gradient: âˆ‡f = [âˆ‚f/âˆ‚xâ‚, âˆ‚f/âˆ‚xâ‚‚, ...] (vector Ä‘áº¡o hÃ m)\"\n",
    "    ],\n",
    "    \n",
    "    \"3. XÃ¡c suáº¥t cÆ¡ báº£n\": [\n",
    "        \"â€¢ P(A) - xÃ¡c suáº¥t sá»± kiá»‡n A xáº£y ra (0 â‰¤ P(A) â‰¤ 1)\",\n",
    "        \"â€¢ P(A|B) - xÃ¡c suáº¥t cÃ³ Ä‘iá»u kiá»‡n (A xáº£y ra khi biáº¿t B)\",\n",
    "        \"â€¢ E[X] - ká»³ vá»ng (expected value, giÃ¡ trá»‹ trung bÃ¬nh)\",\n",
    "        \"â€¢ Var(X) - phÆ°Æ¡ng sai (variance, Ä‘á»™ phÃ¢n tÃ¡n)\"\n",
    "    ],\n",
    "    \n",
    "    \"4. PhÃ¢n phá»‘i Gaussian\": [\n",
    "        \"â€¢ N(Î¼, ÏƒÂ²) - phÃ¢n phá»‘i chuáº©n vá»›i mean Î¼, variance ÏƒÂ²\",\n",
    "        \"â€¢ N(0, 1) - phÃ¢n phá»‘i chuáº©n táº¯c (mean=0, variance=1)\",\n",
    "        \"â€¢ HÃ¬nh dáº¡ng: Ä‘Æ°á»ng cong hÃ¬nh chuÃ´ng\",\n",
    "        \"â€¢ 68% dá»¯ liá»‡u náº±m trong Â±1Ïƒ tá»« Î¼\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, formulas in basic_math.items():\n",
    "    print(f\"\\nğŸ”¸ {category}:\")\n",
    "    for formula in formulas:\n",
    "        print(f\"   {formula}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ KIá»‚M TRA Má»¨C 1:\")\n",
    "print(\"   â“ Báº¡n cÃ³ biáº¿t vector [1,2,3] Â· [4,5,6] = ?\")\n",
    "print(\"   â“ N(0,1) cÃ³ nghÄ©a lÃ  gÃ¬?\")\n",
    "print(\"   â“ E[X] khÃ¡c Var(X) nhÆ° tháº¿ nÃ o?\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ§® Má»¨C 2: MACHINE LEARNING CÆ  Báº¢N (QUAN TRá»ŒNG)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ml_math = {\n",
    "    \"1. Loss Functions\": [\n",
    "        \"â€¢ L2 Loss: ||y - Å·||Â² = (yâ‚-Å·â‚)Â² + (yâ‚‚-Å·â‚‚)Â² + ...\",\n",
    "        \"â€¢ L1 Loss: ||y - Å·||â‚ = |yâ‚-Å·â‚| + |yâ‚‚-Å·â‚‚| + ...\",\n",
    "        \"â€¢ Cross-entropy: -âˆ‘ y log(Å·)\",\n",
    "        \"â€¢ MSE (Mean Squared Error): 1/n âˆ‘(yáµ¢ - Å·áµ¢)Â²\"\n",
    "    ],\n",
    "    \n",
    "    \"2. Neural Networks\": [\n",
    "        \"â€¢ Forward pass: y = f(Wx + b)\",\n",
    "        \"â€¢ Activation functions: ReLU(x) = max(0,x), Sigmoid, etc\",\n",
    "        \"â€¢ Backpropagation: âˆ‚Loss/âˆ‚W (gradient Ä‘á»ƒ update weights)\",\n",
    "        \"â€¢ Optimizer: W â† W - Î±âˆ‡W (gradient descent)\"\n",
    "    ],\n",
    "    \n",
    "    \"3. Convolution\": [\n",
    "        \"â€¢ Conv2D: (I * K)[i,j] = âˆ‘âˆ‘ I[i+m,j+n] Ã— K[m,n]\",\n",
    "        \"â€¢ Kernel/Filter: ma tráº­n nhá» scan qua image\",\n",
    "        \"â€¢ Stride: bÆ°á»›c nháº£y cá»§a kernel\",\n",
    "        \"â€¢ Padding: thÃªm 0 vÃ o biÃªn Ä‘á»ƒ giá»¯ kÃ­ch thÆ°á»›c\"\n",
    "    ],\n",
    "    \n",
    "    \"4. Regularization\": [\n",
    "        \"â€¢ L1 Reg: Loss + Î»âˆ‘|wáµ¢| (khuyáº¿n khÃ­ch sparse weights)\",\n",
    "        \"â€¢ L2 Reg: Loss + Î»âˆ‘wáµ¢Â² (khuyáº¿n khÃ­ch small weights)\",\n",
    "        \"â€¢ Dropout: randomly set some neurons = 0\",\n",
    "        \"â€¢ Batch Norm: normalize inputs cá»§a má»—i layer\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, formulas in ml_math.items():\n",
    "    print(f\"\\nğŸ”¸ {category}:\")\n",
    "    for formula in formulas:\n",
    "        print(f\"   {formula}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ KIá»‚M TRA Má»¨C 2:\")\n",
    "print(\"   â“ ReLU(x) vá»›i x = [-2, 0, 3] ra káº¿t quáº£ gÃ¬?\")\n",
    "print(\"   â“ Convolution 3x3 kernel trÃªn 5x5 image ra kÃ­ch thÆ°á»›c gÃ¬?\")\n",
    "print(\"   â“ Backpropagation dÃ¹ng Ä‘á»ƒ lÃ m gÃ¬?\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ­ Má»¨C 3: DIFFUSION MODELS (TRá»ŒNG TÃ‚M)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "diffusion_math = {\n",
    "    \"1. VAE (Variational AutoEncoder)\": [\n",
    "        \"â€¢ q(z|x) = N(Î¼(x), ÏƒÂ²(x)) - encoder táº¡o phÃ¢n phá»‘i\",\n",
    "        \"â€¢ p(x|z) - decoder tÃ¡i táº¡o tá»« latent z\",\n",
    "        \"â€¢ KL divergence: KL[q||p] = âˆ« q(x)log(q(x)/p(x))dx\",\n",
    "        \"â€¢ Reparameterization: z = Î¼ + ÎµÂ·Ïƒ vá»›i Îµ ~ N(0,1)\",\n",
    "        \"â€¢ ELBO: E[log p(x|z)] - KL[q(z|x)||p(z)]\"\n",
    "    ],\n",
    "    \n",
    "    \"2. Forward Process (ThÃªm nhiá»…u)\": [\n",
    "        \"â€¢ q(xâ‚œ|xâ‚œâ‚‹â‚) = N(xâ‚œ; âˆš(1-Î²â‚œ)xâ‚œâ‚‹â‚, Î²â‚œI)\",\n",
    "        \"â€¢ Î²â‚œ - noise schedule (tÄƒng dáº§n tá»« 0.0001 â†’ 0.02)\",\n",
    "        \"â€¢ Î±â‚œ = 1 - Î²â‚œ\",\n",
    "        \"â€¢ á¾±â‚œ = âˆáµ¢â‚Œâ‚áµ— Î±áµ¢ (cumulative product)\",\n",
    "        \"â€¢ xâ‚œ = âˆšá¾±â‚œ xâ‚€ + âˆš(1-á¾±â‚œ) Îµ vá»›i Îµ ~ N(0,I)\"\n",
    "    ],\n",
    "    \n",
    "    \"3. Reverse Process (Khá»­ nhiá»…u)\": [\n",
    "        \"â€¢ p(xâ‚œâ‚‹â‚|xâ‚œ) = N(xâ‚œâ‚‹â‚; Î¼Î¸(xâ‚œ,t), Î£Î¸(xâ‚œ,t))\",\n",
    "        \"â€¢ Model há»c dá»± Ä‘oÃ¡n Î¼Î¸(xâ‚œ,t)\",\n",
    "        \"â€¢ Thá»±c táº¿: model dá»± Ä‘oÃ¡n noise ÎµÎ¸(xâ‚œ,t)\",\n",
    "        \"â€¢ Î¼Î¸ = 1/âˆšÎ±â‚œ (xâ‚œ - Î²â‚œ/âˆš(1-á¾±â‚œ) ÎµÎ¸(xâ‚œ,t))\"\n",
    "    ],\n",
    "    \n",
    "    \"4. Training Loss\": [\n",
    "        \"â€¢ LDM = E[||Îµ - ÎµÎ¸(xâ‚œ,t)||Â²]\",\n",
    "        \"â€¢ Îµ ~ N(0,I) - true noise added\",\n",
    "        \"â€¢ ÎµÎ¸(xâ‚œ,t) - predicted noise by model\",\n",
    "        \"â€¢ t ~ Uniform{1,...,T} - random timestep\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, formulas in diffusion_math.items():\n",
    "    print(f\"\\nğŸ”¸ {category}:\")\n",
    "    for formula in formulas:\n",
    "        print(f\"   {formula}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ KIá»‚M TRA Má»¨C 3:\")\n",
    "print(\"   â“ Táº¡i sao dá»± Ä‘oÃ¡n noise Îµ thay vÃ¬ dá»± Ä‘oÃ¡n áº£nh xâ‚€?\")\n",
    "print(\"   â“ Forward process cÃ³ cáº§n training khÃ´ng?\")\n",
    "print(\"   â“ á¾±â‚œ giáº£m hay tÄƒng theo t?\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸš€ Má»¨C 4: STABLE DIFFUSION NÃ‚NG CAO (TÃ™Y CHá»ŒN)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "advanced_math = {\n",
    "    \"1. Attention Mechanism\": [\n",
    "        \"â€¢ Attention(Q,K,V) = softmax(QKáµ€/âˆšd)V\",\n",
    "        \"â€¢ Q = query, K = key, V = value matrices\",\n",
    "        \"â€¢ Self-attention: Q,K,V cÃ¹ng tá»« 1 source\",\n",
    "        \"â€¢ Cross-attention: Q tá»« image, K,V tá»« text\",\n",
    "        \"â€¢ Multi-head: concat nhiá»u attention heads\"\n",
    "    ],\n",
    "    \n",
    "    \"2. CLIP Loss\": [\n",
    "        \"â€¢ Contrastive loss giá»¯a text vÃ  image embeddings\",\n",
    "        \"â€¢ S = I @ Táµ€ (similarity matrix)\",\n",
    "        \"â€¢ L = CE(S, labels) + CE(Sáµ€, labels)\",\n",
    "        \"â€¢ CE = cross-entropy loss\"\n",
    "    ],\n",
    "    \n",
    "    \"3. Classifier-Free Guidance\": [\n",
    "        \"â€¢ ÎµÌƒ = Îµ_uncond + s(Îµ_cond - Îµ_uncond)\",\n",
    "        \"â€¢ s = guidance scale (thÆ°á»ng 7.5)\",\n",
    "        \"â€¢ Îµ_cond = ÎµÎ¸(xâ‚œ, t, c) vá»›i conditioning c\",\n",
    "        \"â€¢ Îµ_uncond = ÎµÎ¸(xâ‚œ, t, âˆ…) khÃ´ng conditioning\"\n",
    "    ],\n",
    "    \n",
    "    \"4. Perceptual Loss\": [\n",
    "        \"â€¢ L_perceptual = ||Ï†(x) - Ï†(xÌ‚)||Â²\",\n",
    "        \"â€¢ Ï†(Â·) = VGG features extractor\",\n",
    "        \"â€¢ So sÃ¡nh á»Ÿ feature space thay vÃ¬ pixel space\",\n",
    "        \"â€¢ Multi-scale: L = Î£Î»áµ¢||Ï†áµ¢(x) - Ï†áµ¢(xÌ‚)||Â²\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, formulas in advanced_math.items():\n",
    "    print(f\"\\nğŸ”¸ {category}:\")\n",
    "    for formula in formulas:\n",
    "        print(f\"   {formula}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… KIá»‚M TRA Tá»”NG THá»‚\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "checklist = [\n",
    "    \"â–¡ Hiá»ƒu vector, matrix, Ä‘áº¡o hÃ m cÆ¡ báº£n\",\n",
    "    \"â–¡ Biáº¿t xÃ¡c suáº¥t, phÃ¢n phá»‘i Gaussian N(Î¼,ÏƒÂ²)\", \n",
    "    \"â–¡ Quen vá»›i neural networks, convolution\",\n",
    "    \"â–¡ Hiá»ƒu loss functions, backpropagation\",\n",
    "    \"â–¡ Náº¯m Ä‘Æ°á»£c VAE: encoder q(z|x), decoder p(x|z)\",\n",
    "    \"â–¡ Hiá»ƒu diffusion: forward process (add noise), reverse (denoise)\",\n",
    "    \"â–¡ Biáº¿t attention mechanism cÆ¡ báº£n\",\n",
    "    \"â–¡ Quen vá»›i CLIP vÃ  cross-attention\"\n",
    "]\n",
    "\n",
    "print(\"CHECKLIST - Báº¡n cáº§n tick Ã­t nháº¥t 6/8 items:\")\n",
    "for item in checklist:\n",
    "    print(f\"   {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¯ KHUYáº¾N NGHá»Š Há»ŒC Táº¬P\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "recommendations = \"\"\"\n",
    "ğŸ“š Náº¾U Báº N CHÆ¯A BIáº¾T GÃŒ:\n",
    "   1. Há»c Linear Algebra (Khan Academy)\n",
    "   2. Calculus cÆ¡ báº£n (derivatives)\n",
    "   3. Probability & Statistics\n",
    "   4. Neural Networks basics (3Blue1Brown)\n",
    "\n",
    "ğŸ§  Náº¾U Báº N BIáº¾T CÆ  Báº¢N:\n",
    "   1. Hiá»ƒu VAE trÆ°á»›c (Understanding VAE paper)\n",
    "   2. Äá»c DDPM paper (pháº§n 2,3)\n",
    "   3. Xem video giáº£i thÃ­ch Diffusion Models\n",
    "   4. Thá»±c hÃ nh code VAE Ä‘Æ¡n giáº£n\n",
    "\n",
    "ğŸš€ Náº¾U Báº N ÄÃƒ BIáº¾T ML:\n",
    "   1. Äi tháº³ng vÃ o Diffusion Models\n",
    "   2. Focus vÃ o Stable Diffusion paper\n",
    "   3. Implement tá»«ng component\n",
    "   4. Test vá»›i datasets nhá»\n",
    "\n",
    "ğŸ’ª Náº¾U Báº N PRO:\n",
    "   1. Äá»c paper gá»‘c\n",
    "   2. Reproduce results\n",
    "   3. Modify vÃ  experiment\n",
    "   4. Share knowledge vá»›i community\n",
    "\"\"\"\n",
    "\n",
    "print(recommendations)\n",
    "\n",
    "print(\"\\nğŸ’¡ Lá»œI KHUYÃŠN CUá»I:\")\n",
    "print(\"â€¢ Äá»ªNG Sá»¢ CÃ”NG THá»¨C! Háº§u háº¿t chá»‰ lÃ  notation fancy\")\n",
    "print(\"â€¢ Hiá»ƒu intuition trÆ°á»›c, cÃ´ng thá»©c sau\") \n",
    "print(\"â€¢ Code helps understanding - implement Ä‘á»ƒ hiá»ƒu sÃ¢u\")\n",
    "print(\"â€¢ Ask questions - AI community ráº¥t helpful!\")\n",
    "print(\"\\nğŸ¯ Ready to conquer Stable Diffusion! ğŸš€\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
