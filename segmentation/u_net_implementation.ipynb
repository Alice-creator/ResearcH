{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1479387a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycocotools in ./.venv/lib/python3.12/site-packages (2.0.10)\n",
      "Requirement already satisfied: safetensors in ./.venv/lib/python3.12/site-packages (0.5.3)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from pycocotools) (2.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pycocotools safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cfd3cbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations: ['captions_val2017.json', 'instances_val2017.json', 'captions_train2017.json', 'person_keypoints_val2017.json', 'instances_train2017.json', 'person_keypoints_train2017.json']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 118287\n",
      "Validation images: 5000\n",
      "Test images: 40670\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "# Use local dataset path from kagglehub download\n",
    "base_path = \"/home/loc-dang/.cache/kagglehub/datasets/awsaf49/coco-2017-dataset/versions/2/coco2017\"\n",
    "print(\"Annotations:\", os.listdir(os.path.join(base_path, \"annotations\")))\n",
    "print(\"Training images:\", len(os.listdir(os.path.join(base_path, \"train2017\"))))\n",
    "print(\"Validation images:\", len(os.listdir(os.path.join(base_path, \"val2017\"))))\n",
    "print(\"Test images:\", len(os.listdir(os.path.join(base_path, \"test2017\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ee81f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu126\n",
      "CUDA available: True\n",
      "CUDA version: 12.6\n",
      "Number of GPUs: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"Current device:\", torch.cuda.current_device())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3afce394",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=0),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab9195c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing DoubleConv:\n",
      "Input shape: torch.Size([1, 3, 100, 100])\n",
      "Output shape: torch.Size([1, 64, 96, 96])\n",
      "Size change: 100 -> 96\n",
      "Pixels lost: 4 (due to unpadded conv)\n"
     ]
    }
   ],
   "source": [
    "# Test DoubleConv block\n",
    "print(\"ðŸ§ª Testing DoubleConv:\")\n",
    "test_input = torch.randn(1, 3, 100, 100)  # [batch, channels, height, width]\n",
    "double_conv = DoubleConv(3, 64)\n",
    "output = double_conv(test_input)\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Size change: {test_input.shape[-1]} -> {output.shape[-1]}\")\n",
    "print(f\"Pixels lost: {test_input.shape[-1] - output.shape[-1]} (due to unpadded conv)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "408fd7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.double_conv = DoubleConv(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.max_pool(x)\n",
    "        x = self.double_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1904ed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.up_conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.double_conv = DoubleConv(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        batch, channels, x2_height,x2_width = x2.shape\n",
    "        up_sample = self.up_conv(x1)\n",
    "        each_size = (x2_height - up_sample.shape[2]) // 2  \n",
    "\n",
    "        x2_crop = x2[:,:,each_size:x2_height - each_size, each_size:x2_width - each_size]\n",
    "        concatenate = torch.cat([up_sample, x2_crop], dim=1)\n",
    "\n",
    "        return self.double_conv(concatenate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d2b3663",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.encoder_0 = DoubleConv(in_channels=in_channels, out_channels=64)\n",
    "        self.encoder_1 = Encoder(64, 128)\n",
    "        self.encoder_2 = Encoder(128, 256)\n",
    "        self.encoder_3 = Encoder(256, 512)\n",
    "        self.encoder_4 = Encoder(512, 1024)\n",
    "\n",
    "        self.decoder_0 = Decoder(1024, 512)\n",
    "        self.decoder_1 = Decoder(512, 256)\n",
    "        self.decoder_2 = Decoder(256, 128)\n",
    "        self.decoder_3 = Decoder(128, 64)\n",
    "\n",
    "        self.out = nn.Conv2d(64, num_classes, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.encoder_0(x)\n",
    "        x2 = self.encoder_1(x1)\n",
    "        x3 = self.encoder_2(x2)\n",
    "        x4 = self.encoder_3(x3)\n",
    "        x5 = self.encoder_4(x4)\n",
    "\n",
    "        x = self.decoder_0(x5, x4)\n",
    "        x = self.decoder_1(x, x3)\n",
    "        x = self.decoder_2(x, x2)\n",
    "        x = self.decoder_3(x, x1)\n",
    "        return self.out(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9e4d8da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Testing Complete U-Net:\n",
      "Model created: UNet\n",
      "Input shape: torch.Size([1, 3, 572, 572])\n",
      "Output shape: torch.Size([1, 2, 388, 388])\n",
      "Input size: 572 -> Output size: 388\n",
      "Size reduction: 184 pixels\n",
      "\n",
      "Model Parameters:\n",
      "Total: 31,031,810\n",
      "Trainable: 31,031,810\n"
     ]
    }
   ],
   "source": [
    "# Test UNet Architecture\n",
    "print(\"ðŸš€ Testing Complete U-Net:\")\n",
    "\n",
    "# Create model\n",
    "model = UNet(in_channels=3, num_classes=2)  # RGB input, binary segmentation\n",
    "print(f\"Model created: {model.__class__.__name__}\")\n",
    "\n",
    "# Test with dummy input\n",
    "test_input = torch.randn(1, 3, 572, 572)  # Original paper input size\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(test_input)\n",
    "    \n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Input size: {test_input.shape[-1]} -> Output size: {output.shape[-1]}\")\n",
    "print(f\"Size reduction: {test_input.shape[-1] - output.shape[-1]} pixels\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel Parameters:\")\n",
    "print(f\"Total: {total_params:,}\")\n",
    "print(f\"Trainable: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76cf3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
